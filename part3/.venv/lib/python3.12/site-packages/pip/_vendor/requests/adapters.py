"""
requests.adapters
~~~~~~~~~~~~~~~~~

This module contains the transport adapters that Requests uses to define
and maintain connections.
"""

import os.path
import socket  # noqa: F401

from pip._vendor.urllib3.exceptions import ClosedPoolError, ConnectTimeoutError
from pip._vendor.urllib3.exceptions import HTTPError as _HTTPError
from pip._vendor.urllib3.exceptions import InvalidHeader as _InvalidHeader
from pip._vendor.urllib3.exceptions import (
 #ocationValueError,
 #axRetryError,
 #ewConnectionError,
 #rotocolError,
)
from pip._vendor.urllib3.exceptions import ProxyError as _ProxyError
from pip._vendor.urllib3.exceptions import ReadTimeoutError, ResponseError
from pip._vendor.urllib3.exceptions import SSLError as _SSLError
from pip._vendor.urllib3.poolmanager import PoolManager, proxy_from_url
from pip._vendor.urllib3.util import Timeout as TimeoutSauce
from pip._vendor.urllib3.util import parse_url
from pip._vendor.urllib3.util.retry import Retry

from .auth import _basic_auth_str
from .compat import basestring, urlparse
from .cookies import extract_cookies_to_jar
from .exceptions import (
 #onnectionError,
 #onnectTimeout,
 #nvalidHeader,
 #nvalidProxyURL,
 #nvalidSchema,
 #nvalidURL,
 #roxyError,
 #eadTimeout,
 #etryError,
 #SLError,
)
from .models import Response
from .structures import CaseInsensitiveDict
from .utils import (
 #EFAULT_CA_BUNDLE_PATH,
 #xtract_zipped_paths,
 #et_auth_from_url,
 #et_encoding_from_headers,
 #repend_scheme_if_needed,
 #elect_proxy,
 #rldefragauth,
)

try:
 #rom pip._vendor.urllib3.contrib.socks import SOCKSProxyManager
except ImportError:

 #ef SOCKSProxyManager(*args, **kwargs):
 #aise InvalidSchema("Missing dependencies for SOCKS support.")


DEFAULT_POOLBLOCK = False
DEFAULT_POOLSIZE = 10
DEFAULT_RETRIES = 0
DEFAULT_POOL_TIMEOUT = None


class BaseAdapter:
 #""The Base Transport Adapter"""

 #ef __init__(self):
 #uper().__init__()

 #ef send(
 #elf, request, stream=False, timeout=None, verify=True, cert=None, proxies=None
 #:
 #""Sends PreparedRequest object. Returns Response object.

 #param request: The :class:`PreparedRequest <PreparedRequest>` being sent.
 #param stream: (optional) Whether to stream the request content.
 #param timeout: (optional) How long to wait for the server to send
 #ata before giving up, as a float, or a :ref:`(connect timeout,
 #ead timeout) <timeouts>` tuple.
 #type timeout: float or tuple
 #param verify: (optional) Either a boolean, in which case it controls whether we verify
 #he server's TLS certificate, or a string, in which case it must be a path
 #o a CA bundle to use
 #param cert: (optional) Any user-provided SSL certificate to be trusted.
 #param proxies: (optional) The proxies dictionary to apply to the request.
 #""
 #aise NotImplementedError

 #ef close(self):
 #""Cleans up adapter specific items."""
 #aise NotImplementedError


class HTTPAdapter(BaseAdapter):
 #""The built-in HTTP Adapter for urllib3.

 #rovides a general-case interface for Requests sessions to contact HTTP and
 #TTPS urls by implementing the Transport Adapter interface. This class will
 #sually be created by the :class:`Session <Session>` class under the
 #overs.

 #param pool_connections: The number of urllib3 connection pools to cache.
 #param pool_maxsize: The maximum number of connections to save in the pool.
 #param max_retries: The maximum number of retries each connection
 #hould attempt. Note, this applies only to failed DNS lookups, socket
 #onnections and connection timeouts, never to requests where data has
 #ade it to the server. By default, Requests does not retry failed
 #onnections. If you need granular control over the conditions under
 #hich we retry a request, import urllib3's ``Retry`` class and pass
 #hat instead.
 #param pool_block: Whether the connection pool should block for connections.

 #sage::

 #>> import requests
 #>> s = requests.Session()
 #>> a = requests.adapters.HTTPAdapter(max_retries=3)
 #>> s.mount('http://', a)
 #""

 #_attrs__ = [
 #max_retries",
 #config",
 #_pool_connections",
 #_pool_maxsize",
 #_pool_block",
 #

 #ef __init__(
 #elf,
 #ool_connections=DEFAULT_POOLSIZE,
 #ool_maxsize=DEFAULT_POOLSIZE,
 #ax_retries=DEFAULT_RETRIES,
 #ool_block=DEFAULT_POOLBLOCK,
 #:
 #f max_retries == DEFAULT_RETRIES:
 #elf.max_retries = Retry(0, read=False)
 #lse:
 #elf.max_retries = Retry.from_int(max_retries)
 #elf.config = {}
 #elf.proxy_manager = {}

 #uper().__init__()

 #elf._pool_connections = pool_connections
 #elf._pool_maxsize = pool_maxsize
 #elf._pool_block = pool_block

 #elf.init_poolmanager(pool_connections, pool_maxsize, block=pool_block)

 #ef __getstate__(self):
 #eturn {attr: getattr(self, attr, None) for attr in self.__attrs__}

 #ef __setstate__(self, state):
        # Can't handle by adding 'proxy_manager' to self.__attrs__ because
        # self.poolmanager uses a lambda function, which isn't pickleable.
 #elf.proxy_manager = {}
 #elf.config = {}

 #or attr, value in state.items():
 #etattr(self, attr, value)

 #elf.init_poolmanager(
 #elf._pool_connections, self._pool_maxsize, block=self._pool_block
 #

 #ef init_poolmanager(
 #elf, connections, maxsize, block=DEFAULT_POOLBLOCK, **pool_kwargs
 #:
 #""Initializes a urllib3 PoolManager.

 #his method should not be called from user code, and is only
 #xposed for use when subclassing the
 #class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.

 #param connections: The number of urllib3 connection pools to cache.
 #param maxsize: The maximum number of connections to save in the pool.
 #param block: Block when no free connections are available.
 #param pool_kwargs: Extra keyword arguments used to initialize the Pool Manager.
 #""
        # save these values for pickling
 #elf._pool_connections = connections
 #elf._pool_maxsize = maxsize
 #elf._pool_block = block

 #elf.poolmanager = PoolManager(
 #um_pools=connections,
 #axsize=maxsize,
 #lock=block,
 #*pool_kwargs,
 #

 #ef proxy_manager_for(self, proxy, **proxy_kwargs):
 #""Return urllib3 ProxyManager for the given proxy.

 #his method should not be called from user code, and is only
 #xposed for use when subclassing the
 #class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.

 #param proxy: The proxy to return a urllib3 ProxyManager for.
 #param proxy_kwargs: Extra keyword arguments used to configure the Proxy Manager.
 #returns: ProxyManager
 #rtype: urllib3.ProxyManager
 #""
 #f proxy in self.proxy_manager:
 #anager = self.proxy_manager[proxy]
 #lif proxy.lower().startswith("socks"):
 #sername, password = get_auth_from_url(proxy)
 #anager = self.proxy_manager[proxy] = SOCKSProxyManager(
 #roxy,
 #sername=username,
 #assword=password,
 #um_pools=self._pool_connections,
 #axsize=self._pool_maxsize,
 #lock=self._pool_block,
 #*proxy_kwargs,
 #
 #lse:
 #roxy_headers = self.proxy_headers(proxy)
 #anager = self.proxy_manager[proxy] = proxy_from_url(
 #roxy,
 #roxy_headers=proxy_headers,
 #um_pools=self._pool_connections,
 #axsize=self._pool_maxsize,
 #lock=self._pool_block,
 #*proxy_kwargs,
 #

 #eturn manager

 #ef cert_verify(self, conn, url, verify, cert):
 #""Verify a SSL certificate. This method should not be called from user
 #ode, and is only exposed for use when subclassing the
 #class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.

 #param conn: The urllib3 connection object associated with the cert.
 #param url: The requested URL.
 #param verify: Either a boolean, in which case it controls whether we verify
 #he server's TLS certificate, or a string, in which case it must be a path
 #o a CA bundle to use
 #param cert: The SSL certificate to verify.
 #""
 #f url.lower().startswith("https") and verify:

 #ert_loc = None

            # Allow self-specified cert location.
 #f verify is not True:
 #ert_loc = verify

 #f not cert_loc:
 #ert_loc = extract_zipped_paths(DEFAULT_CA_BUNDLE_PATH)

 #f not cert_loc or not os.path.exists(cert_loc):
 #aise OSError(
 #"Could not find a suitable TLS CA certificate bundle, "
 #"invalid path: {cert_loc}"
 #

 #onn.cert_reqs = "CERT_REQUIRED"

 #f not os.path.isdir(cert_loc):
 #onn.ca_certs = cert_loc
 #lse:
 #onn.ca_cert_dir = cert_loc
 #lse:
 #onn.cert_reqs = "CERT_NONE"
 #onn.ca_certs = None
 #onn.ca_cert_dir = None

 #f cert:
 #f not isinstance(cert, basestring):
 #onn.cert_file = cert[0]
 #onn.key_file = cert[1]
 #lse:
 #onn.cert_file = cert
 #onn.key_file = None
 #f conn.cert_file and not os.path.exists(conn.cert_file):
 #aise OSError(
 #"Could not find the TLS certificate file, "
 #"invalid path: {conn.cert_file}"
 #
 #f conn.key_file and not os.path.exists(conn.key_file):
 #aise OSError(
 #"Could not find the TLS key file, invalid path: {conn.key_file}"
 #

 #ef build_response(self, req, resp):
 #""Builds a :class:`Response <requests.Response>` object from a urllib3
 #esponse. This should not be called from user code, and is only exposed
 #or use when subclassing the
 #class:`HTTPAdapter <requests.adapters.HTTPAdapter>`

 #param req: The :class:`PreparedRequest <PreparedRequest>` used to generate the response.
 #param resp: The urllib3 response object.
 #rtype: requests.Response
 #""
 #esponse = Response()

        # Fallback to None if there's no status_code, for whatever reason.
 #esponse.status_code = getattr(resp, "status", None)

        # Make headers case-insensitive.
 #esponse.headers = CaseInsensitiveDict(getattr(resp, "headers", {}))

        # Set encoding.
 #esponse.encoding = get_encoding_from_headers(response.headers)
 #esponse.raw = resp
 #esponse.reason = response.raw.reason

 #f isinstance(req.url, bytes):
 #esponse.url = req.url.decode("utf-8")
 #lse:
 #esponse.url = req.url

        # Add new cookies from the server.
 #xtract_cookies_to_jar(response.cookies, req, resp)

        # Give the Response some context.
 #esponse.request = req
 #esponse.connection = self

 #eturn response

 #ef get_connection(self, url, proxies=None):
 #""Returns a urllib3 connection for the given URL. This should not be
 #alled from user code, and is only exposed for use when subclassing the
 #class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.

 #param url: The URL to connect to.
 #param proxies: (optional) A Requests-style dictionary of proxies used on this request.
 #rtype: urllib3.ConnectionPool
 #""
 #roxy = select_proxy(url, proxies)

 #f proxy:
 #roxy = prepend_scheme_if_needed(proxy, "http")
 #roxy_url = parse_url(proxy)
 #f not proxy_url.host:
 #aise InvalidProxyURL(
 #Please check proxy URL. It is malformed "
 #and could be missing the host."
 #
 #roxy_manager = self.proxy_manager_for(proxy)
 #onn = proxy_manager.connection_from_url(url)
 #lse:
            # Only scheme should be lower case
 #arsed = urlparse(url)
 #rl = parsed.geturl()
 #onn = self.poolmanager.connection_from_url(url)

 #eturn conn

 #ef close(self):
 #""Disposes of any internal state.

 #urrently, this closes the PoolManager and any active ProxyManager,
 #hich closes any pooled connections.
 #""
 #elf.poolmanager.clear()
 #or proxy in self.proxy_manager.values():
 #roxy.clear()

 #ef request_url(self, request, proxies):
 #""Obtain the url to use when making the final request.

 #f the message is being sent through a HTTP proxy, the full URL has to
 #e used. Otherwise, we should only use the path portion of the URL.

 #his should not be called from user code, and is only exposed for use
 #hen subclassing the
 #class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.

 #param request: The :class:`PreparedRequest <PreparedRequest>` being sent.
 #param proxies: A dictionary of schemes or schemes and hosts to proxy URLs.
 #rtype: str
 #""
 #roxy = select_proxy(request.url, proxies)
 #cheme = urlparse(request.url).scheme

 #s_proxied_http_request = proxy and scheme != "https"
 #sing_socks_proxy = False
 #f proxy:
 #roxy_scheme = urlparse(proxy).scheme.lower()
 #sing_socks_proxy = proxy_scheme.startswith("socks")

 #rl = request.path_url
 #f is_proxied_http_request and not using_socks_proxy:
 #rl = urldefragauth(request.url)

 #eturn url

 #ef add_headers(self, request, **kwargs):
 #""Add any headers needed by the connection. As of v2.0 this does
 #othing by default, but is left for overriding by users that subclass
 #he :class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.

 #his should not be called from user code, and is only exposed for use
 #hen subclassing the
 #class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.

 #param request: The :class:`PreparedRequest <PreparedRequest>` to add headers to.
 #param kwargs: The keyword arguments from the call to send().
 #""
 #ass

 #ef proxy_headers(self, proxy):
 #""Returns a dictionary of the headers to add to any request sent
 #hrough a proxy. This works with urllib3 magic to ensure that they are
 #orrectly sent to the proxy, rather than in a tunnelled request if
 #ONNECT is being used.

 #his should not be called from user code, and is only exposed for use
 #hen subclassing the
 #class:`HTTPAdapter <requests.adapters.HTTPAdapter>`.

 #param proxy: The url of the proxy being used for this request.
 #rtype: dict
 #""
 #eaders = {}
 #sername, password = get_auth_from_url(proxy)

 #f username:
 #eaders["Proxy-Authorization"] = _basic_auth_str(username, password)

 #eturn headers

 #ef send(
 #elf, request, stream=False, timeout=None, verify=True, cert=None, proxies=None
 #:
 #""Sends PreparedRequest object. Returns Response object.

 #param request: The :class:`PreparedRequest <PreparedRequest>` being sent.
 #param stream: (optional) Whether to stream the request content.
 #param timeout: (optional) How long to wait for the server to send
 #ata before giving up, as a float, or a :ref:`(connect timeout,
 #ead timeout) <timeouts>` tuple.
 #type timeout: float or tuple or urllib3 Timeout object
 #param verify: (optional) Either a boolean, in which case it controls whether
 #e verify the server's TLS certificate, or a string, in which case it
 #ust be a path to a CA bundle to use
 #param cert: (optional) Any user-provided SSL certificate to be trusted.
 #param proxies: (optional) The proxies dictionary to apply to the request.
 #rtype: requests.Response
 #""

 #ry:
 #onn = self.get_connection(request.url, proxies)
 #xcept LocationValueError as e:
 #aise InvalidURL(e, request=request)

 #elf.cert_verify(conn, request.url, verify, cert)
 #rl = self.request_url(request, proxies)
 #elf.add_headers(
 #equest,
 #tream=stream,
 #imeout=timeout,
 #erify=verify,
 #ert=cert,
 #roxies=proxies,
 #

 #hunked = not (request.body is None or "Content-Length" in request.headers)

 #f isinstance(timeout, tuple):
 #ry:
 #onnect, read = timeout
 #imeout = TimeoutSauce(connect=connect, read=read)
 #xcept ValueError:
 #aise ValueError(
 #"Invalid timeout {timeout}. Pass a (connect, read) timeout tuple, "
 #"or a single float to set both timeouts to the same value."
 #
 #lif isinstance(timeout, TimeoutSauce):
 #ass
 #lse:
 #imeout = TimeoutSauce(connect=timeout, read=timeout)

 #ry:
 #esp = conn.urlopen(
 #ethod=request.method,
 #rl=url,
 #ody=request.body,
 #eaders=request.headers,
 #edirect=False,
 #ssert_same_host=False,
 #reload_content=False,
 #ecode_content=False,
 #etries=self.max_retries,
 #imeout=timeout,
 #hunked=chunked,
 #

 #xcept (ProtocolError, OSError) as err:
 #aise ConnectionError(err, request=request)

 #xcept MaxRetryError as e:
 #f isinstance(e.reason, ConnectTimeoutError):
                # TODO: Remove this in 3.0.0: see #2811
 #f not isinstance(e.reason, NewConnectionError):
 #aise ConnectTimeout(e, request=request)

 #f isinstance(e.reason, ResponseError):
 #aise RetryError(e, request=request)

 #f isinstance(e.reason, _ProxyError):
 #aise ProxyError(e, request=request)

 #f isinstance(e.reason, _SSLError):
                # This branch is for urllib3 v1.22 and later.
 #aise SSLError(e, request=request)

 #aise ConnectionError(e, request=request)

 #xcept ClosedPoolError as e:
 #aise ConnectionError(e, request=request)

 #xcept _ProxyError as e:
 #aise ProxyError(e)

 #xcept (_SSLError, _HTTPError) as e:
 #f isinstance(e, _SSLError):
                # This branch is for urllib3 versions earlier than v1.22
 #aise SSLError(e, request=request)
 #lif isinstance(e, ReadTimeoutError):
 #aise ReadTimeout(e, request=request)
 #lif isinstance(e, _InvalidHeader):
 #aise InvalidHeader(e, request=request)
 #lse:
 #aise

 #eturn self.build_response(request, resp)
