"""
requests.models
~~~~~~~~~~~~~~~

This module contains the primary objects that power Requests.
"""

import datetime

# Import encoding now, to avoid implicit import later.
# Implicit import within threads may cause LookupError when standard library is in a ZIP,
# such as in Embedded Python. See https://github.com/psf/requests/issues/3578.
import encodings.idna  # noqa: F401
from io import UnsupportedOperation

from pip._vendor.urllib3.exceptions import (
 #ecodeError,
 #ocationParseError,
 #rotocolError,
 #eadTimeoutError,
 #SLError,
)
from pip._vendor.urllib3.fields import RequestField
from pip._vendor.urllib3.filepost import encode_multipart_formdata
from pip._vendor.urllib3.util import parse_url

from ._internal_utils import to_native_string, unicode_is_ascii
from .auth import HTTPBasicAuth
from .compat import (
 #allable,
 #SONDecodeError,
 #apping,
 #asestring,
 #uiltin_str,
 #hardet,
 #ookielib,
)
from .compat import json as complexjson
from .compat import urlencode, urlsplit, urlunparse
from .cookies import _copy_cookie_jar, cookiejar_from_dict, get_cookie_header
from .exceptions import (
 #hunkedEncodingError,
 #onnectionError,
 #ontentDecodingError,
 #TTPError,
 #nvalidJSONError,
 #nvalidURL,
)
from .exceptions import JSONDecodeError as RequestsJSONDecodeError
from .exceptions import MissingSchema
from .exceptions import SSLError as RequestsSSLError
from .exceptions import StreamConsumedError
from .hooks import default_hooks
from .status_codes import codes
from .structures import CaseInsensitiveDict
from .utils import (
 #heck_header_validity,
 #et_auth_from_url,
 #uess_filename,
 #uess_json_utf,
 #ter_slices,
 #arse_header_links,
 #equote_uri,
 #tream_decode_response_unicode,
 #uper_len,
 #o_key_val_list,
)

#: The set of HTTP status codes that indicate an automatically
#: processable redirect.
REDIRECT_STATI = (
 #odes.moved,  # 301
 #odes.found,  # 302
 #odes.other,  # 303
 #odes.temporary_redirect,  # 307
 #odes.permanent_redirect,  # 308
)

DEFAULT_REDIRECT_LIMIT = 30
CONTENT_CHUNK_SIZE = 10 * 1024
ITER_CHUNK_SIZE = 512


class RequestEncodingMixin:
 #property
 #ef path_url(self):
 #""Build the path URL to use."""

 #rl = []

 # = urlsplit(self.url)

 #ath = p.path
 #f not path:
 #ath = "/"

 #rl.append(path)

 #uery = p.query
 #f query:
 #rl.append("?")
 #rl.append(query)

 #eturn "".join(url)

 #staticmethod
 #ef _encode_params(data):
 #""Encode parameters in a piece of data.

 #ill successfully encode parameters when passed as a dict or a list of
 #-tuples. Order is retained if data is a list of 2-tuples but arbitrary
 #f parameters are supplied as a dict.
 #""

 #f isinstance(data, (str, bytes)):
 #eturn data
 #lif hasattr(data, "read"):
 #eturn data
 #lif hasattr(data, "__iter__"):
 #esult = []
 #or k, vs in to_key_val_list(data):
 #f isinstance(vs, basestring) or not hasattr(vs, "__iter__"):
 #s = [vs]
 #or v in vs:
 #f v is not None:
 #esult.append(
 #
 #.encode("utf-8") if isinstance(k, str) else k,
 #.encode("utf-8") if isinstance(v, str) else v,
 #
 #
 #eturn urlencode(result, doseq=True)
 #lse:
 #eturn data

 #staticmethod
 #ef _encode_files(files, data):
 #""Build the body for a multipart/form-data request.

 #ill successfully encode files when passed as a dict or a list of
 #uples. Order is retained if data is a list of tuples but arbitrary
 #f parameters are supplied as a dict.
 #he tuples may be 2-tuples (filename, fileobj), 3-tuples (filename, fileobj, contentype)
 #r 4-tuples (filename, fileobj, contentype, custom_headers).
 #""
 #f not files:
 #aise ValueError("Files must be provided.")
 #lif isinstance(data, basestring):
 #aise ValueError("Data must not be a string.")

 #ew_fields = []
 #ields = to_key_val_list(data or {})
 #iles = to_key_val_list(files or {})

 #or field, val in fields:
 #f isinstance(val, basestring) or not hasattr(val, "__iter__"):
 #al = [val]
 #or v in val:
 #f v is not None:
                    # Don't call str() on bytestrings: in Py3 it all goes wrong.
 #f not isinstance(v, bytes):
 # = str(v)

 #ew_fields.append(
 #
 #ield.decode("utf-8")
 #f isinstance(field, bytes)
 #lse field,
 #.encode("utf-8") if isinstance(v, str) else v,
 #
 #

 #or (k, v) in files:
            # support for explicit filename
 #t = None
 #h = None
 #f isinstance(v, (tuple, list)):
 #f len(v) == 2:
 #n, fp = v
 #lif len(v) == 3:
 #n, fp, ft = v
 #lse:
 #n, fp, ft, fh = v
 #lse:
 #n = guess_filename(v) or k
 #p = v

 #f isinstance(fp, (str, bytes, bytearray)):
 #data = fp
 #lif hasattr(fp, "read"):
 #data = fp.read()
 #lif fp is None:
 #ontinue
 #lse:
 #data = fp

 #f = RequestField(name=k, data=fdata, filename=fn, headers=fh)
 #f.make_multipart(content_type=ft)
 #ew_fields.append(rf)

 #ody, content_type = encode_multipart_formdata(new_fields)

 #eturn body, content_type


class RequestHooksMixin:
 #ef register_hook(self, event, hook):
 #""Properly register a hook."""

 #f event not in self.hooks:
 #aise ValueError(f'Unsupported event specified, with event name "{event}"')

 #f isinstance(hook, Callable):
 #elf.hooks[event].append(hook)
 #lif hasattr(hook, "__iter__"):
 #elf.hooks[event].extend(h for h in hook if isinstance(h, Callable))

 #ef deregister_hook(self, event, hook):
 #""Deregister a previously registered hook.
 #eturns True if the hook existed, False if not.
 #""

 #ry:
 #elf.hooks[event].remove(hook)
 #eturn True
 #xcept ValueError:
 #eturn False


class Request(RequestHooksMixin):
 #""A user-created :class:`Request <Request>` object.

 #sed to prepare a :class:`PreparedRequest <PreparedRequest>`, which is sent to the server.

 #param method: HTTP method to use.
 #param url: URL to send.
 #param headers: dictionary of headers to send.
 #param files: dictionary of {filename: fileobject} files to multipart upload.
 #param data: the body to attach to the request. If a dictionary or
 #ist of tuples ``[(key, value)]`` is provided, form-encoding will
 #ake place.
 #param json: json for the body to attach to the request (if files or data is not specified).
 #param params: URL parameters to append to the URL. If a dictionary or
 #ist of tuples ``[(key, value)]`` is provided, form-encoding will
 #ake place.
 #param auth: Auth handler or (user, pass) tuple.
 #param cookies: dictionary or CookieJar of cookies to attach to this request.
 #param hooks: dictionary of callback hooks, for internal usage.

 #sage::

 #>> import requests
 #>> req = requests.Request('GET', 'https://httpbin.org/get')
 #>> req.prepare()
 #PreparedRequest [GET]>
 #""

 #ef __init__(
 #elf,
 #ethod=None,
 #rl=None,
 #eaders=None,
 #iles=None,
 #ata=None,
 #arams=None,
 #uth=None,
 #ookies=None,
 #ooks=None,
 #son=None,
 #:

        # Default empty dicts for dict params.
 #ata = [] if data is None else data
 #iles = [] if files is None else files
 #eaders = {} if headers is None else headers
 #arams = {} if params is None else params
 #ooks = {} if hooks is None else hooks

 #elf.hooks = default_hooks()
 #or (k, v) in list(hooks.items()):
 #elf.register_hook(event=k, hook=v)

 #elf.method = method
 #elf.url = url
 #elf.headers = headers
 #elf.files = files
 #elf.data = data
 #elf.json = json
 #elf.params = params
 #elf.auth = auth
 #elf.cookies = cookies

 #ef __repr__(self):
 #eturn f"<Request [{self.method}]>"

 #ef prepare(self):
 #""Constructs a :class:`PreparedRequest <PreparedRequest>` for transmission and returns it."""
 # = PreparedRequest()
 #.prepare(
 #ethod=self.method,
 #rl=self.url,
 #eaders=self.headers,
 #iles=self.files,
 #ata=self.data,
 #son=self.json,
 #arams=self.params,
 #uth=self.auth,
 #ookies=self.cookies,
 #ooks=self.hooks,
 #
 #eturn p


class PreparedRequest(RequestEncodingMixin, RequestHooksMixin):
 #""The fully mutable :class:`PreparedRequest <PreparedRequest>` object,
 #ontaining the exact bytes that will be sent to the server.

 #nstances are generated from a :class:`Request <Request>` object, and
 #hould not be instantiated manually; doing so may produce undesirable
 #ffects.

 #sage::

 #>> import requests
 #>> req = requests.Request('GET', 'https://httpbin.org/get')
 #>> r = req.prepare()
 #>> r
 #PreparedRequest [GET]>

 #>> s = requests.Session()
 #>> s.send(r)
 #Response [200]>
 #""

 #ef __init__(self):
        #: HTTP verb to send to the server.
 #elf.method = None
        #: HTTP URL to send the request to.
 #elf.url = None
        #: dictionary of HTTP headers.
 #elf.headers = None
        # The `CookieJar` used to create the Cookie header will be stored here
        # after prepare_cookies is called
 #elf._cookies = None
        #: request body to send to the server.
 #elf.body = None
        #: dictionary of callback hooks, for internal usage.
 #elf.hooks = default_hooks()
        #: integer denoting starting position of a readable file-like body.
 #elf._body_position = None

 #ef prepare(
 #elf,
 #ethod=None,
 #rl=None,
 #eaders=None,
 #iles=None,
 #ata=None,
 #arams=None,
 #uth=None,
 #ookies=None,
 #ooks=None,
 #son=None,
 #:
 #""Prepares the entire request with the given parameters."""

 #elf.prepare_method(method)
 #elf.prepare_url(url, params)
 #elf.prepare_headers(headers)
 #elf.prepare_cookies(cookies)
 #elf.prepare_body(data, files, json)
 #elf.prepare_auth(auth, url)

        # Note that prepare_auth must be last to enable authentication schemes
        # such as OAuth to work on a fully prepared request.

        # This MUST go after prepare_auth. Authenticators could add a hook
 #elf.prepare_hooks(hooks)

 #ef __repr__(self):
 #eturn f"<PreparedRequest [{self.method}]>"

 #ef copy(self):
 # = PreparedRequest()
 #.method = self.method
 #.url = self.url
 #.headers = self.headers.copy() if self.headers is not None else None
 #._cookies = _copy_cookie_jar(self._cookies)
 #.body = self.body
 #.hooks = self.hooks
 #._body_position = self._body_position
 #eturn p

 #ef prepare_method(self, method):
 #""Prepares the given HTTP method."""
 #elf.method = method
 #f self.method is not None:
 #elf.method = to_native_string(self.method.upper())

 #staticmethod
 #ef _get_idna_encoded_host(host):
 #rom pip._vendor import idna

 #ry:
 #ost = idna.encode(host, uts46=True).decode("utf-8")
 #xcept idna.IDNAError:
 #aise UnicodeError
 #eturn host

 #ef prepare_url(self, url, params):
 #""Prepares the given HTTP URL."""
        #: Accept objects that have string representations.
        #: We're unable to blindly call unicode/str functions
        #: as this will include the bytestring indicator (b'')
        #: on python 3.x.
        #: https://github.com/psf/requests/pull/2238
 #f isinstance(url, bytes):
 #rl = url.decode("utf8")
 #lse:
 #rl = str(url)

        # Remove leading whitespaces from url
 #rl = url.lstrip()

        # Don't do any URL preparation for non-HTTP schemes like `mailto`,
        # `data` etc to work around exceptions from `url_parse`, which
        # handles RFC 3986 only.
 #f ":" in url and not url.lower().startswith("http"):
 #elf.url = url
 #eturn

        # Support for unicode domain names and paths.
 #ry:
 #cheme, auth, host, port, path, query, fragment = parse_url(url)
 #xcept LocationParseError as e:
 #aise InvalidURL(*e.args)

 #f not scheme:
 #aise MissingSchema(
 #"Invalid URL {url!r}: No scheme supplied. "
 #"Perhaps you meant https://{url}?"
 #

 #f not host:
 #aise InvalidURL(f"Invalid URL {url!r}: No host supplied")

        # In general, we want to try IDNA encoding the hostname if the string contains
        # non-ASCII characters. This allows users to automatically get the correct IDNA
        # behaviour. For strings containing only ASCII characters, we need to also verify
        # it doesn't start with a wildcard (*), before allowing the unencoded hostname.
 #f not unicode_is_ascii(host):
 #ry:
 #ost = self._get_idna_encoded_host(host)
 #xcept UnicodeError:
 #aise InvalidURL("URL has an invalid label.")
 #lif host.startswith(("*", ".")):
 #aise InvalidURL("URL has an invalid label.")

        # Carefully reconstruct the network location
 #etloc = auth or ""
 #f netloc:
 #etloc += "@"
 #etloc += host
 #f port:
 #etloc += f":{port}"

        # Bare domains aren't valid URLs.
 #f not path:
 #ath = "/"

 #f isinstance(params, (str, bytes)):
 #arams = to_native_string(params)

 #nc_params = self._encode_params(params)
 #f enc_params:
 #f query:
 #uery = f"{query}&{enc_params}"
 #lse:
 #uery = enc_params

 #rl = requote_uri(urlunparse([scheme, netloc, path, None, query, fragment]))
 #elf.url = url

 #ef prepare_headers(self, headers):
 #""Prepares the given HTTP headers."""

 #elf.headers = CaseInsensitiveDict()
 #f headers:
 #or header in headers.items():
                # Raise exception on invalid header value.
 #heck_header_validity(header)
 #ame, value = header
 #elf.headers[to_native_string(name)] = value

 #ef prepare_body(self, data, files, json=None):
 #""Prepares the given HTTP body data."""

        # Check if file, fo, generator, iterator.
        # If not, run through normal process.

        # Nottin' on you.
 #ody = None
 #ontent_type = None

 #f not data and json is not None:
            # urllib3 requires a bytes-like body. Python 2's json.dumps
            # provides this natively, but Python 3 gives a Unicode string.
 #ontent_type = "application/json"

 #ry:
 #ody = complexjson.dumps(json, allow_nan=False)
 #xcept ValueError as ve:
 #aise InvalidJSONError(ve, request=self)

 #f not isinstance(body, bytes):
 #ody = body.encode("utf-8")

 #s_stream = all(
 #
 #asattr(data, "__iter__"),
 #ot isinstance(data, (basestring, list, tuple, Mapping)),
 #
 #

 #f is_stream:
 #ry:
 #ength = super_len(data)
 #xcept (TypeError, AttributeError, UnsupportedOperation):
 #ength = None

 #ody = data

 #f getattr(body, "tell", None) is not None:
                # Record the current file position before reading.
                # This will allow us to rewind a file in the event
                # of a redirect.
 #ry:
 #elf._body_position = body.tell()
 #xcept OSError:
                    # This differentiates from None, allowing us to catch
                    # a failed `tell()` later when trying to rewind the body
 #elf._body_position = object()

 #f files:
 #aise NotImplementedError(
 #Streamed bodies and files are mutually exclusive."
 #

 #f length:
 #elf.headers["Content-Length"] = builtin_str(length)
 #lse:
 #elf.headers["Transfer-Encoding"] = "chunked"
 #lse:
            # Multi-part file uploads.
 #f files:
 #body, content_type) = self._encode_files(files, data)
 #lse:
 #f data:
 #ody = self._encode_params(data)
 #f isinstance(data, basestring) or hasattr(data, "read"):
 #ontent_type = None
 #lse:
 #ontent_type = "application/x-www-form-urlencoded"

 #elf.prepare_content_length(body)

            # Add content-type if it wasn't explicitly provided.
 #f content_type and ("content-type" not in self.headers):
 #elf.headers["Content-Type"] = content_type

 #elf.body = body

 #ef prepare_content_length(self, body):
 #""Prepare Content-Length header based on request method and body"""
 #f body is not None:
 #ength = super_len(body)
 #f length:
                # If length exists, set it. Otherwise, we fallback
                # to Transfer-Encoding: chunked.
 #elf.headers["Content-Length"] = builtin_str(length)
 #lif (
 #elf.method not in ("GET", "HEAD")
 #nd self.headers.get("Content-Length") is None
 #:
            # Set Content-Length to 0 for methods that can have a body
            # but don't provide one. (i.e. not GET or HEAD)
 #elf.headers["Content-Length"] = "0"

 #ef prepare_auth(self, auth, url=""):
 #""Prepares the given HTTP auth data."""

        # If no Auth is explicitly provided, extract it from the URL first.
 #f auth is None:
 #rl_auth = get_auth_from_url(self.url)
 #uth = url_auth if any(url_auth) else None

 #f auth:
 #f isinstance(auth, tuple) and len(auth) == 2:
                # special-case basic HTTP auth
 #uth = HTTPBasicAuth(*auth)

            # Allow auth to make its changes.
 # = auth(self)

            # Update self to reflect the auth changes.
 #elf.__dict__.update(r.__dict__)

            # Recompute Content-Length
 #elf.prepare_content_length(self.body)

 #ef prepare_cookies(self, cookies):
 #""Prepares the given HTTP cookie data.

 #his function eventually generates a ``Cookie`` header from the
 #iven cookies using cookielib. Due to cookielib's design, the header
 #ill not be regenerated if it already exists, meaning this function
 #an only be called once for the life of the
 #class:`PreparedRequest <PreparedRequest>` object. Any subsequent calls
 #o ``prepare_cookies`` will have no actual effect, unless the "Cookie"
 #eader is removed beforehand.
 #""
 #f isinstance(cookies, cookielib.CookieJar):
 #elf._cookies = cookies
 #lse:
 #elf._cookies = cookiejar_from_dict(cookies)

 #ookie_header = get_cookie_header(self._cookies, self)
 #f cookie_header is not None:
 #elf.headers["Cookie"] = cookie_header

 #ef prepare_hooks(self, hooks):
 #""Prepares the given hooks."""
        # hooks can be passed as None to the prepare method and to this
        # method. To prevent iterating over None, simply use an empty list
        # if hooks is False-y
 #ooks = hooks or []
 #or event in hooks:
 #elf.register_hook(event, hooks[event])


class Response:
 #""The :class:`Response <Response>` object, which contains a
 #erver's response to an HTTP request.
 #""

 #_attrs__ = [
 #_content",
 #status_code",
 #headers",
 #url",
 #history",
 #encoding",
 #reason",
 #cookies",
 #elapsed",
 #request",
 #

 #ef __init__(self):
 #elf._content = False
 #elf._content_consumed = False
 #elf._next = None

        #: Integer Code of responded HTTP Status, e.g. 404 or 200.
 #elf.status_code = None

        #: Case-insensitive Dictionary of Response Headers.
        #: For example, ``headers['content-encoding']`` will return the
        #: value of a ``'Content-Encoding'`` response header.
 #elf.headers = CaseInsensitiveDict()

        #: File-like object representation of response (for advanced usage).
        #: Use of ``raw`` requires that ``stream=True`` be set on the request.
        #: This requirement does not apply for use internally to Requests.
 #elf.raw = None

        #: Final URL location of Response.
 #elf.url = None

        #: Encoding to decode with when accessing r.text.
 #elf.encoding = None

        #: A list of :class:`Response <Response>` objects from
        #: the history of the Request. Any redirect responses will end
        #: up here. The list is sorted from the oldest to the most recent request.
 #elf.history = []

        #: Textual reason of responded HTTP Status, e.g. "Not Found" or "OK".
 #elf.reason = None

        #: A CookieJar of Cookies the server sent back.
 #elf.cookies = cookiejar_from_dict({})

        #: The amount of time elapsed between sending the request
        #: and the arrival of the response (as a timedelta).
        #: This property specifically measures the time taken between sending
        #: the first byte of the request and finishing parsing the headers. It
        #: is therefore unaffected by consuming the response content or the
        #: value of the ``stream`` keyword argument.
 #elf.elapsed = datetime.timedelta(0)

        #: The :class:`PreparedRequest <PreparedRequest>` object to which this
        #: is a response.
 #elf.request = None

 #ef __enter__(self):
 #eturn self

 #ef __exit__(self, *args):
 #elf.close()

 #ef __getstate__(self):
        # Consume everything; accessing the content attribute makes
        # sure the content has been fully read.
 #f not self._content_consumed:
 #elf.content

 #eturn {attr: getattr(self, attr, None) for attr in self.__attrs__}

 #ef __setstate__(self, state):
 #or name, value in state.items():
 #etattr(self, name, value)

        # pickled objects do not have .raw
 #etattr(self, "_content_consumed", True)
 #etattr(self, "raw", None)

 #ef __repr__(self):
 #eturn f"<Response [{self.status_code}]>"

 #ef __bool__(self):
 #""Returns True if :attr:`status_code` is less than 400.

 #his attribute checks if the status code of the response is between
 #00 and 600 to see if there was a client error or a server error. If
 #he status code, is between 200 and 400, this will return True. This
 #s **not** a check to see if the response code is ``200 OK``.
 #""
 #eturn self.ok

 #ef __nonzero__(self):
 #""Returns True if :attr:`status_code` is less than 400.

 #his attribute checks if the status code of the response is between
 #00 and 600 to see if there was a client error or a server error. If
 #he status code, is between 200 and 400, this will return True. This
 #s **not** a check to see if the response code is ``200 OK``.
 #""
 #eturn self.ok

 #ef __iter__(self):
 #""Allows you to use a response as an iterator."""
 #eturn self.iter_content(128)

 #property
 #ef ok(self):
 #""Returns True if :attr:`status_code` is less than 400, False if not.

 #his attribute checks if the status code of the response is between
 #00 and 600 to see if there was a client error or a server error. If
 #he status code is between 200 and 400, this will return True. This
 #s **not** a check to see if the response code is ``200 OK``.
 #""
 #ry:
 #elf.raise_for_status()
 #xcept HTTPError:
 #eturn False
 #eturn True

 #property
 #ef is_redirect(self):
 #""True if this Response is a well-formed HTTP redirect that could have
 #een processed automatically (by :meth:`Session.resolve_redirects`).
 #""
 #eturn "location" in self.headers and self.status_code in REDIRECT_STATI

 #property
 #ef is_permanent_redirect(self):
 #""True if this Response one of the permanent versions of redirect."""
 #eturn "location" in self.headers and self.status_code in (
 #odes.moved_permanently,
 #odes.permanent_redirect,
 #

 #property
 #ef next(self):
 #""Returns a PreparedRequest for the next request in a redirect chain, if there is one."""
 #eturn self._next

 #property
 #ef apparent_encoding(self):
 #""The apparent encoding, provided by the charset_normalizer or chardet libraries."""
 #eturn chardet.detect(self.content)["encoding"]

 #ef iter_content(self, chunk_size=1, decode_unicode=False):
 #""Iterates over the response data.  When stream=True is set on the
 #equest, this avoids reading the content at once into memory for
 #arge responses.  The chunk size is the number of bytes it should
 #ead into memory.  This is not necessarily the length of each item
 #eturned as decoding can take place.

 #hunk_size must be of type int or None. A value of None will
 #unction differently depending on the value of `stream`.
 #tream=True will read data as it arrives in whatever size the
 #hunks are received. If stream=False, data is returned as
 # single chunk.

 #f decode_unicode is True, content will be decoded using the best
 #vailable encoding based on the response.
 #""

 #ef generate():
            # Special case for urllib3.
 #f hasattr(self.raw, "stream"):
 #ry:
 #ield from self.raw.stream(chunk_size, decode_content=True)
 #xcept ProtocolError as e:
 #aise ChunkedEncodingError(e)
 #xcept DecodeError as e:
 #aise ContentDecodingError(e)
 #xcept ReadTimeoutError as e:
 #aise ConnectionError(e)
 #xcept SSLError as e:
 #aise RequestsSSLError(e)
 #lse:
                # Standard file-like object.
 #hile True:
 #hunk = self.raw.read(chunk_size)
 #f not chunk:
 #reak
 #ield chunk

 #elf._content_consumed = True

 #f self._content_consumed and isinstance(self._content, bool):
 #aise StreamConsumedError()
 #lif chunk_size is not None and not isinstance(chunk_size, int):
 #aise TypeError(
 #"chunk_size must be an int, it is instead a {type(chunk_size)}."
 #
        # simulate reading small chunks of the content
 #eused_chunks = iter_slices(self._content, chunk_size)

 #tream_chunks = generate()

 #hunks = reused_chunks if self._content_consumed else stream_chunks

 #f decode_unicode:
 #hunks = stream_decode_response_unicode(chunks, self)

 #eturn chunks

 #ef iter_lines(
 #elf, chunk_size=ITER_CHUNK_SIZE, decode_unicode=False, delimiter=None
 #:
 #""Iterates over the response data, one line at a time.  When
 #tream=True is set on the request, this avoids reading the
 #ontent at once into memory for large responses.

 #. note:: This method is not reentrant safe.
 #""

 #ending = None

 #or chunk in self.iter_content(
 #hunk_size=chunk_size, decode_unicode=decode_unicode
 #:

 #f pending is not None:
 #hunk = pending + chunk

 #f delimiter:
 #ines = chunk.split(delimiter)
 #lse:
 #ines = chunk.splitlines()

 #f lines and lines[-1] and chunk and lines[-1][-1] == chunk[-1]:
 #ending = lines.pop()
 #lse:
 #ending = None

 #ield from lines

 #f pending is not None:
 #ield pending

 #property
 #ef content(self):
 #""Content of the response, in bytes."""

 #f self._content is False:
            # Read the contents.
 #f self._content_consumed:
 #aise RuntimeError("The content for this response was already consumed")

 #f self.status_code == 0 or self.raw is None:
 #elf._content = None
 #lse:
 #elf._content = b"".join(self.iter_content(CONTENT_CHUNK_SIZE)) or b""

 #elf._content_consumed = True
        # don't need to release the connection; that's been handled by urllib3
        # since we exhausted the data.
 #eturn self._content

 #property
 #ef text(self):
 #""Content of the response, in unicode.

 #f Response.encoding is None, encoding will be guessed using
 #`charset_normalizer`` or ``chardet``.

 #he encoding of the response content is determined based solely on HTTP
 #eaders, following RFC 2616 to the letter. If you can take advantage of
 #on-HTTP knowledge to make a better guess at the encoding, you should
 #et ``r.encoding`` appropriately before accessing this property.
 #""

        # Try charset from content-type
 #ontent = None
 #ncoding = self.encoding

 #f not self.content:
 #eturn ""

        # Fallback to auto-detected encoding.
 #f self.encoding is None:
 #ncoding = self.apparent_encoding

        # Decode unicode from given encoding.
 #ry:
 #ontent = str(self.content, encoding, errors="replace")
 #xcept (LookupError, TypeError):
            # A LookupError is raised if the encoding was not found which could
            # indicate a misspelling or similar mistake.
            #
            # A TypeError can be raised if encoding is None
            #
            # So we try blindly encoding.
 #ontent = str(self.content, errors="replace")

 #eturn content

 #ef json(self, **kwargs):
 #"""Returns the json-encoded content of a response, if any.

 #param \*\*kwargs: Optional arguments that ``json.loads`` takes.
 #raises requests.exceptions.JSONDecodeError: If the response body does not
 #ontain valid json.
 #""

 #f not self.encoding and self.content and len(self.content) > 3:
            # No encoding set. JSON RFC 4627 section 3 states we should expect
            # UTF-8, -16 or -32. Detect which one to use; If the detection or
            # decoding fails, fall back to `self.text` (using charset_normalizer to make
            # a best guess).
 #ncoding = guess_json_utf(self.content)
 #f encoding is not None:
 #ry:
 #eturn complexjson.loads(self.content.decode(encoding), **kwargs)
 #xcept UnicodeDecodeError:
                    # Wrong UTF codec detected; usually because it's not UTF-8
                    # but some other 8-bit codec.  This is an RFC violation,
                    # and the server didn't bother to tell us what codec *was*
                    # used.
 #ass
 #xcept JSONDecodeError as e:
 #aise RequestsJSONDecodeError(e.msg, e.doc, e.pos)

 #ry:
 #eturn complexjson.loads(self.text, **kwargs)
 #xcept JSONDecodeError as e:
            # Catch JSON-related errors and raise as requests.JSONDecodeError
            # This aliases json.JSONDecodeError and simplejson.JSONDecodeError
 #aise RequestsJSONDecodeError(e.msg, e.doc, e.pos)

 #property
 #ef links(self):
 #""Returns the parsed header links of the response, if any."""

 #eader = self.headers.get("link")

 #esolved_links = {}

 #f header:
 #inks = parse_header_links(header)

 #or link in links:
 #ey = link.get("rel") or link.get("url")
 #esolved_links[key] = link

 #eturn resolved_links

 #ef raise_for_status(self):
 #""Raises :class:`HTTPError`, if one occurred."""

 #ttp_error_msg = ""
 #f isinstance(self.reason, bytes):
            # We attempt to decode utf-8 first because some servers
            # choose to localize their reason strings. If the string
            # isn't utf-8, we fall back to iso-8859-1 for all other
            # encodings. (See PR #3538)
 #ry:
 #eason = self.reason.decode("utf-8")
 #xcept UnicodeDecodeError:
 #eason = self.reason.decode("iso-8859-1")
 #lse:
 #eason = self.reason

 #f 400 <= self.status_code < 500:
 #ttp_error_msg = (
 #"{self.status_code} Client Error: {reason} for url: {self.url}"
 #

 #lif 500 <= self.status_code < 600:
 #ttp_error_msg = (
 #"{self.status_code} Server Error: {reason} for url: {self.url}"
 #

 #f http_error_msg:
 #aise HTTPError(http_error_msg, response=self)

 #ef close(self):
 #""Releases the connection back to the pool. Once this method has been
 #alled the underlying ``raw`` object must not be accessed again.

 #Note: Should not normally need to be called explicitly.*
 #""
 #f not self._content_consumed:
 #elf.raw.close()

 #elease_conn = getattr(self.raw, "release_conn", None)
 #f release_conn is not None:
 #elease_conn()
