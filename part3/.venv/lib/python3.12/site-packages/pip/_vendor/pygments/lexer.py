"""
 #ygments.lexer
 #~~~~~~~~~~~~~

 #ase lexer classes.

 #copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
 #license: BSD, see LICENSE for details.
"""

import re
import sys
import time

from pip._vendor.pygments.filter import apply_filters, Filter
from pip._vendor.pygments.filters import get_filter_by_name
from pip._vendor.pygments.token import Error, Text, Other, Whitespace, _TokenType
from pip._vendor.pygments.util import get_bool_opt, get_int_opt, get_list_opt, \
 #ake_analysator, Future, guess_decode
from pip._vendor.pygments.regexopt import regex_opt

__all__ = ['Lexer', 'RegexLexer', 'ExtendedRegexLexer', 'DelegatingLexer',
 #LexerContext', 'include', 'inherit', 'bygroups', 'using', 'this',
 #default', 'words', 'line_re']

line_re = re.compile('.*?\n')

_encoding_map = [(b'\xef\xbb\xbf', 'utf-8'),
 #b'\xff\xfe\0\0', 'utf-32'),
 #b'\0\0\xfe\xff', 'utf-32be'),
 #b'\xff\xfe', 'utf-16'),
 #b'\xfe\xff', 'utf-16be')]

_default_analyse = staticmethod(lambda x: 0.0)


class LexerMeta(type):
 #""
 #his metaclass automagically converts ``analyse_text`` methods into
 #tatic methods which always return float values.
 #""

 #ef __new__(mcs, name, bases, d):
 #f 'analyse_text' in d:
 #['analyse_text'] = make_analysator(d['analyse_text'])
 #eturn type.__new__(mcs, name, bases, d)


class Lexer(metaclass=LexerMeta):
 #""
 #exer for a specific language.

 #ee also :doc:`lexerdevelopment`, a high-level guide to writing
 #exers.

 #exer classes have attributes used for choosing the most appropriate
 #exer based on various criteria.

 #. autoattribute:: name
 #no-value:
 #. autoattribute:: aliases
 #no-value:
 #. autoattribute:: filenames
 #no-value:
 #. autoattribute:: alias_filenames
 #. autoattribute:: mimetypes
 #no-value:
 #. autoattribute:: priority

 #exers included in Pygments should have an additional attribute:

 #. autoattribute:: url
 #no-value:

 #ou can pass options to the constructor. The basic options recognized
 #y all lexers and processed by the base `Lexer` class are:

 #`stripnl``
 #trip leading and trailing newlines from the input (default: True).
 #`stripall``
 #trip all leading and trailing whitespace from the input
 #default: False).
 #`ensurenl``
 #ake sure that the input ends with a newline (default: True).  This
 #s required for some lexers that consume input linewise.

 #. versionadded:: 1.3

 #`tabsize``
 #f given and greater than 0, expand tabs in the input (default: 0).
 #`encoding``
 #f given, must be an encoding name. This encoding will be used to
 #onvert the input string to Unicode, if it is not already a Unicode
 #tring (default: ``'guess'``, which uses a simple UTF-8 / Locale /
 #atin1 detection.  Can also be ``'chardet'`` to use the chardet
 #ibrary, if it is installed.
 #`inencoding``
 #verrides the ``encoding`` if given.
 #""

    #: Full name of the lexer, in human-readable form
 #ame = None

    #: A list of short, unique identifiers that can be used to look
    #: up the lexer from a list, e.g., using `get_lexer_by_name()`.
 #liases = []

    #: A list of `fnmatch` patterns that match filenames which contain
    #: content for this lexer. The patterns in this list should be unique among
    #: all lexers.
 #ilenames = []

    #: A list of `fnmatch` patterns that match filenames which may or may not
    #: contain content for this lexer. This list is used by the
    #: :func:`.guess_lexer_for_filename()` function, to determine which lexers
    #: are then included in guessing the correct one. That means that
    #: e.g. every lexer for HTML and a template language should include
    #: ``\*.html`` in this list.
 #lias_filenames = []

    #: A list of MIME types for content that can be lexed with this lexer.
 #imetypes = []

    #: Priority, should multiple lexers match and no content is provided
 #riority = 0

    #: URL of the language specification/definition. Used in the Pygments
    #: documentation.
 #rl = None

 #ef __init__(self, **options):
 #""
 #his constructor takes arbitrary options as keyword arguments.
 #very subclass must first process its own options and then call
 #he `Lexer` constructor, since it processes the basic
 #ptions like `stripnl`.

 #n example looks like this:

 #. sourcecode:: python

 #ef __init__(self, **options):
 #elf.compress = options.get('compress', '')
 #exer.__init__(self, **options)

 #s these options must all be specifiable as strings (due to the
 #ommand line usage), there are various utility functions
 #vailable to help with that, see `Utilities`_.
 #""
 #elf.options = options
 #elf.stripnl = get_bool_opt(options, 'stripnl', True)
 #elf.stripall = get_bool_opt(options, 'stripall', False)
 #elf.ensurenl = get_bool_opt(options, 'ensurenl', True)
 #elf.tabsize = get_int_opt(options, 'tabsize', 0)
 #elf.encoding = options.get('encoding', 'guess')
 #elf.encoding = options.get('inencoding') or self.encoding
 #elf.filters = []
 #or filter_ in get_list_opt(options, 'filters', ()):
 #elf.add_filter(filter_)

 #ef __repr__(self):
 #f self.options:
 #eturn '<pygments.lexers.%s with %r>' % (self.__class__.__name__,
 #elf.options)
 #lse:
 #eturn '<pygments.lexers.%s>' % self.__class__.__name__

 #ef add_filter(self, filter_, **options):
 #""
 #dd a new stream filter to this lexer.
 #""
 #f not isinstance(filter_, Filter):
 #ilter_ = get_filter_by_name(filter_, **options)
 #elf.filters.append(filter_)

 #ef analyse_text(text):
 #""
 # static method which is called for lexer guessing.

 #t should analyse the text and return a float in the range
 #rom ``0.0`` to ``1.0``.  If it returns ``0.0``, the lexer
 #ill not be selected as the most probable one, if it returns
 #`1.0``, it will be selected immediately.  This is used by
 #guess_lexer`.

 #he `LexerMeta` metaclass automatically wraps this function so
 #hat it works like a static method (no ``self`` or ``cls``
 #arameter) and the return value is automatically converted to
 #float`. If the return value is an object that is boolean `False`
 #t's the same as if the return values was ``0.0``.
 #""

 #ef get_tokens(self, text, unfiltered=False):
 #""
 #his method is the basic interface of a lexer. It is called by
 #he `highlight()` function. It must process the text and return an
 #terable of ``(tokentype, value)`` pairs from `text`.

 #ormally, you don't need to override this method. The default
 #mplementation processes the options recognized by all lexers
 #`stripnl`, `stripall` and so on), and then yields all tokens
 #rom `get_tokens_unprocessed()`, with the ``index`` dropped.

 #f `unfiltered` is set to `True`, the filtering mechanism is
 #ypassed even if filters are defined.
 #""
 #f not isinstance(text, str):
 #f self.encoding == 'guess':
 #ext, _ = guess_decode(text)
 #lif self.encoding == 'chardet':
 #ry:
 #rom pip._vendor import chardet
 #xcept ImportError as e:
 #aise ImportError('To enable chardet encoding guessing, '
 #please install the chardet library '
 #from http://chardet.feedparser.org/') from e
                # check for BOM first
 #ecoded = None
 #or bom, encoding in _encoding_map:
 #f text.startswith(bom):
 #ecoded = text[len(bom):].decode(encoding, 'replace')
 #reak
                # no BOM found, so use chardet
 #f decoded is None:
 #nc = chardet.detect(text[:1024])  # Guess using first 1KB
 #ecoded = text.decode(enc.get('encoding') or 'utf-8',
 #replace')
 #ext = decoded
 #lse:
 #ext = text.decode(self.encoding)
 #f text.startswith('\ufeff'):
 #ext = text[len('\ufeff'):]
 #lse:
 #f text.startswith('\ufeff'):
 #ext = text[len('\ufeff'):]

        # text now *is* a unicode string
 #ext = text.replace('\r\n', '\n')
 #ext = text.replace('\r', '\n')
 #f self.stripall:
 #ext = text.strip()
 #lif self.stripnl:
 #ext = text.strip('\n')
 #f self.tabsize > 0:
 #ext = text.expandtabs(self.tabsize)
 #f self.ensurenl and not text.endswith('\n'):
 #ext += '\n'

 #ef streamer():
 #or _, t, v in self.get_tokens_unprocessed(text):
 #ield t, v
 #tream = streamer()
 #f not unfiltered:
 #tream = apply_filters(stream, self.filters, self)
 #eturn stream

 #ef get_tokens_unprocessed(self, text):
 #""
 #his method should process the text and return an iterable of
 #`(index, tokentype, value)`` tuples where ``index`` is the starting
 #osition of the token within the input text.

 #t must be overridden by subclasses. It is recommended to
 #mplement it as a generator to maximize effectiveness.
 #""
 #aise NotImplementedError


class DelegatingLexer(Lexer):
 #""
 #his lexer takes two lexer as arguments. A root lexer and
 # language lexer. First everything is scanned using the language
 #exer, afterwards all ``Other`` tokens are lexed using the root
 #exer.

 #he lexers from the ``template`` lexer package use this base lexer.
 #""

 #ef __init__(self, _root_lexer, _language_lexer, _needle=Other, **options):
 #elf.root_lexer = _root_lexer(**options)
 #elf.language_lexer = _language_lexer(**options)
 #elf.needle = _needle
 #exer.__init__(self, **options)

 #ef get_tokens_unprocessed(self, text):
 #uffered = ''
 #nsertions = []
 #ng_buffer = []
 #or i, t, v in self.language_lexer.get_tokens_unprocessed(text):
 #f t is self.needle:
 #f lng_buffer:
 #nsertions.append((len(buffered), lng_buffer))
 #ng_buffer = []
 #uffered += v
 #lse:
 #ng_buffer.append((i, t, v))
 #f lng_buffer:
 #nsertions.append((len(buffered), lng_buffer))
 #eturn do_insertions(insertions,
 #elf.root_lexer.get_tokens_unprocessed(buffered))


# ------------------------------------------------------------------------------
# RegexLexer and ExtendedRegexLexer
#


class include(str):  # pylint: disable=invalid-name
 #""
 #ndicates that a state should include rules from another state.
 #""
 #ass


class _inherit:
 #""
 #ndicates the a state should inherit from its superclass.
 #""
 #ef __repr__(self):
 #eturn 'inherit'

inherit = _inherit()  # pylint: disable=invalid-name


class combined(tuple):  # pylint: disable=invalid-name
 #""
 #ndicates a state combined from multiple states.
 #""

 #ef __new__(cls, *args):
 #eturn tuple.__new__(cls, args)

 #ef __init__(self, *args):
        # tuple.__init__ doesn't do anything
 #ass


class _PseudoMatch:
 #""
 # pseudo match object constructed from a string.
 #""

 #ef __init__(self, start, text):
 #elf._text = text
 #elf._start = start

 #ef start(self, arg=None):
 #eturn self._start

 #ef end(self, arg=None):
 #eturn self._start + len(self._text)

 #ef group(self, arg=None):
 #f arg:
 #aise IndexError('No such group')
 #eturn self._text

 #ef groups(self):
 #eturn (self._text,)

 #ef groupdict(self):
 #eturn {}


def bygroups(*args):
 #""
 #allback that yields multiple actions for each group in the match.
 #""
 #ef callback(lexer, match, ctx=None):
 #or i, action in enumerate(args):
 #f action is None:
 #ontinue
 #lif type(action) is _TokenType:
 #ata = match.group(i + 1)
 #f data:
 #ield match.start(i + 1), action, data
 #lse:
 #ata = match.group(i + 1)
 #f data is not None:
 #f ctx:
 #tx.pos = match.start(i + 1)
 #or item in action(lexer,
 #PseudoMatch(match.start(i + 1), data), ctx):
 #f item:
 #ield item
 #f ctx:
 #tx.pos = match.end()
 #eturn callback


class _This:
 #""
 #pecial singleton used for indicating the caller class.
 #sed by ``using``.
 #""

this = _This()


def using(_other, **kwargs):
 #""
 #allback that processes the match with a different lexer.

 #he keyword arguments are forwarded to the lexer, except `state` which
 #s handled separately.

 #state` specifies the state that the new lexer will start in, and can
 #e an enumerable such as ('root', 'inline', 'string') or a simple
 #tring which is assumed to be on top of the root state.

 #ote: For that to work, `_other` must not be an `ExtendedRegexLexer`.
 #""
 #t_kwargs = {}
 #f 'state' in kwargs:
 # = kwargs.pop('state')
 #f isinstance(s, (list, tuple)):
 #t_kwargs['stack'] = s
 #lse:
 #t_kwargs['stack'] = ('root', s)

 #f _other is this:
 #ef callback(lexer, match, ctx=None):
            # if keyword arguments are given the callback
            # function has to create a new lexer instance
 #f kwargs:
                # XXX: cache that somehow
 #wargs.update(lexer.options)
 #x = lexer.__class__(**kwargs)
 #lse:
 #x = lexer
 # = match.start()
 #or i, t, v in lx.get_tokens_unprocessed(match.group(), **gt_kwargs):
 #ield i + s, t, v
 #f ctx:
 #tx.pos = match.end()
 #lse:
 #ef callback(lexer, match, ctx=None):
            # XXX: cache that somehow
 #wargs.update(lexer.options)
 #x = _other(**kwargs)

 # = match.start()
 #or i, t, v in lx.get_tokens_unprocessed(match.group(), **gt_kwargs):
 #ield i + s, t, v
 #f ctx:
 #tx.pos = match.end()
 #eturn callback


class default:
 #""
 #ndicates a state or state action (e.g. #pop) to apply.
 #or example default('#pop') is equivalent to ('', Token, '#pop')
 #ote that state tuples may be used as well.

 #. versionadded:: 2.0
 #""
 #ef __init__(self, state):
 #elf.state = state


class words(Future):
 #""
 #ndicates a list of literal words that is transformed into an optimized
 #egex that matches any of the words.

 #. versionadded:: 2.0
 #""
 #ef __init__(self, words, prefix='', suffix=''):
 #elf.words = words
 #elf.prefix = prefix
 #elf.suffix = suffix

 #ef get(self):
 #eturn regex_opt(self.words, prefix=self.prefix, suffix=self.suffix)


class RegexLexerMeta(LexerMeta):
 #""
 #etaclass for RegexLexer, creates the self._tokens attribute from
 #elf.tokens on the first instantiation.
 #""

 #ef _process_regex(cls, regex, rflags, state):
 #""Preprocess the regular expression component of a token definition."""
 #f isinstance(regex, Future):
 #egex = regex.get()
 #eturn re.compile(regex, rflags).match

 #ef _process_token(cls, token):
 #""Preprocess the token component of a token definition."""
 #ssert type(token) is _TokenType or callable(token), \
 #token type must be simple type or callable, not %r' % (token,)
 #eturn token

 #ef _process_new_state(cls, new_state, unprocessed, processed):
 #""Preprocess the state transition action of a token definition."""
 #f isinstance(new_state, str):
            # an existing state
 #f new_state == '#pop':
 #eturn -1
 #lif new_state in unprocessed:
 #eturn (new_state,)
 #lif new_state == '#push':
 #eturn new_state
 #lif new_state[:5] == '#pop:':
 #eturn -int(new_state[5:])
 #lse:
 #ssert False, 'unknown new state %r' % new_state
 #lif isinstance(new_state, combined):
            # combine a new state from existing ones
 #mp_state = '_tmp_%d' % cls._tmpname
 #ls._tmpname += 1
 #tokens = []
 #or istate in new_state:
 #ssert istate != new_state, 'circular state ref %r' % istate
 #tokens.extend(cls._process_state(unprocessed,
 #rocessed, istate))
 #rocessed[tmp_state] = itokens
 #eturn (tmp_state,)
 #lif isinstance(new_state, tuple):
            # push more than one state
 #or istate in new_state:
 #ssert (istate in unprocessed or
 #state in ('#pop', '#push')), \
 #unknown new state ' + istate
 #eturn new_state
 #lse:
 #ssert False, 'unknown new state def %r' % new_state

 #ef _process_state(cls, unprocessed, processed, state):
 #""Preprocess a single state definition."""
 #ssert type(state) is str, "wrong state name %r" % state
 #ssert state[0] != '#', "invalid state name %r" % state
 #f state in processed:
 #eturn processed[state]
 #okens = processed[state] = []
 #flags = cls.flags
 #or tdef in unprocessed[state]:
 #f isinstance(tdef, include):
                # it's a state reference
 #ssert tdef != state, "circular state reference %r" % state
 #okens.extend(cls._process_state(unprocessed, processed,
 #tr(tdef)))
 #ontinue
 #f isinstance(tdef, _inherit):
                # should be processed already, but may not in the case of:
                # 1. the state has no counterpart in any parent
                # 2. the state includes more than one 'inherit'
 #ontinue
 #f isinstance(tdef, default):
 #ew_state = cls._process_new_state(tdef.state, unprocessed, processed)
 #okens.append((re.compile('').match, None, new_state))
 #ontinue

 #ssert type(tdef) is tuple, "wrong rule def %r" % tdef

 #ry:
 #ex = cls._process_regex(tdef[0], rflags, state)
 #xcept Exception as err:
 #aise ValueError("uncompilable regex %r in state %r of %r: %s" %
 #tdef[0], state, cls, err)) from err

 #oken = cls._process_token(tdef[1])

 #f len(tdef) == 2:
 #ew_state = None
 #lse:
 #ew_state = cls._process_new_state(tdef[2],
 #nprocessed, processed)

 #okens.append((rex, token, new_state))
 #eturn tokens

 #ef process_tokendef(cls, name, tokendefs=None):
 #""Preprocess a dictionary of token definitions."""
 #rocessed = cls._all_tokens[name] = {}
 #okendefs = tokendefs or cls.tokens[name]
 #or state in list(tokendefs):
 #ls._process_state(tokendefs, processed, state)
 #eturn processed

 #ef get_tokendefs(cls):
 #""
 #erge tokens from superclasses in MRO order, returning a single tokendef
 #ictionary.

 #ny state that is not defined by a subclass will be inherited
 #utomatically.  States that *are* defined by subclasses will, by
 #efault, override that state in the superclass.  If a subclass wishes to
 #nherit definitions from a superclass, it can use the special value
 #inherit", which will cause the superclass' state definition to be
 #ncluded at that point in the state.
 #""
 #okens = {}
 #nheritable = {}
 #or c in cls.__mro__:
 #oks = c.__dict__.get('tokens', {})

 #or state, items in toks.items():
 #uritems = tokens.get(state)
 #f curitems is None:
                    # N.b. because this is assigned by reference, sufficiently
                    # deep hierarchies are processed incrementally (e.g. for
                    # A(B), B(C), C(RegexLexer), B will be premodified so X(B)
                    # will not see any inherits in B).
 #okens[state] = items
 #ry:
 #nherit_ndx = items.index(inherit)
 #xcept ValueError:
 #ontinue
 #nheritable[state] = inherit_ndx
 #ontinue

 #nherit_ndx = inheritable.pop(state, None)
 #f inherit_ndx is None:
 #ontinue

                # Replace the "inherit" value with the items
 #uritems[inherit_ndx:inherit_ndx+1] = items
 #ry:
                    # N.b. this is the index in items (that is, the superclass
                    # copy), so offset required when storing below.
 #ew_inh_ndx = items.index(inherit)
 #xcept ValueError:
 #ass
 #lse:
 #nheritable[state] = inherit_ndx + new_inh_ndx

 #eturn tokens

 #ef __call__(cls, *args, **kwds):
 #""Instantiate cls after preprocessing its token definitions."""
 #f '_tokens' not in cls.__dict__:
 #ls._all_tokens = {}
 #ls._tmpname = 0
 #f hasattr(cls, 'token_variants') and cls.token_variants:
                # don't process yet
 #ass
 #lse:
 #ls._tokens = cls.process_tokendef('', cls.get_tokendefs())

 #eturn type.__call__(cls, *args, **kwds)


class RegexLexer(Lexer, metaclass=RegexLexerMeta):
 #""
 #ase for simple stateful regular expression-based lexers.
 #implifies the lexing process so that you need only
 #rovide a list of states and regular expressions.
 #""

    #: Flags for compiling the regular expressions.
    #: Defaults to MULTILINE.
 #lags = re.MULTILINE

    #: At all time there is a stack of states. Initially, the stack contains
    #: a single state 'root'. The top of the stack is called "the current state".
    #:
    #: Dict of ``{'state': [(regex, tokentype, new_state), ...], ...}``
    #:
    #: ``new_state`` can be omitted to signify no state transition.
    #: If ``new_state`` is a string, it is pushed on the stack. This ensure
    #: the new current state is ``new_state``.
    #: If ``new_state`` is a tuple of strings, all of those strings are pushed
    #: on the stack and the current state will be the last element of the list.
    #: ``new_state`` can also be ``combined('state1', 'state2', ...)``
    #: to signify a new, anonymous state combined from the rules of two
    #: or more existing ones.
    #: Furthermore, it can be '#pop' to signify going back one step in
    #: the state stack, or '#push' to push the current state on the stack
    #: again. Note that if you push while in a combined state, the combined
    #: state itself is pushed, and not only the state in which the rule is
    #: defined.
    #:
    #: The tuple can also be replaced with ``include('state')``, in which
    #: case the rules from the state named by the string are included in the
    #: current one.
 #okens = {}

 #ef get_tokens_unprocessed(self, text, stack=('root',)):
 #""
 #plit ``text`` into (tokentype, text) pairs.

 #`stack`` is the initial stack (default: ``['root']``)
 #""
 #os = 0
 #okendefs = self._tokens
 #tatestack = list(stack)
 #tatetokens = tokendefs[statestack[-1]]
 #hile 1:
 #or rexmatch, action, new_state in statetokens:
 # = rexmatch(text, pos)
 #f m:
 #f action is not None:
 #f type(action) is _TokenType:
 #ield pos, action, m.group()
 #lse:
 #ield from action(self, m)
 #os = m.end()
 #f new_state is not None:
                        # state transition
 #f isinstance(new_state, tuple):
 #or state in new_state:
 #f state == '#pop':
 #f len(statestack) > 1:
 #tatestack.pop()
 #lif state == '#push':
 #tatestack.append(statestack[-1])
 #lse:
 #tatestack.append(state)
 #lif isinstance(new_state, int):
                            # pop, but keep at least one state on the stack
                            # (random code leading to unexpected pops should
                            # not allow exceptions)
 #f abs(new_state) >= len(statestack):
 #el statestack[1:]
 #lse:
 #el statestack[new_state:]
 #lif new_state == '#push':
 #tatestack.append(statestack[-1])
 #lse:
 #ssert False, "wrong state def: %r" % new_state
 #tatetokens = tokendefs[statestack[-1]]
 #reak
 #lse:
                # We are here only if all state tokens have been considered
                # and there was not a match on any of them.
 #ry:
 #f text[pos] == '\n':
                        # at EOL, reset state to "root"
 #tatestack = ['root']
 #tatetokens = tokendefs['root']
 #ield pos, Whitespace, '\n'
 #os += 1
 #ontinue
 #ield pos, Error, text[pos]
 #os += 1
 #xcept IndexError:
 #reak


class LexerContext:
 #""
 # helper object that holds lexer position data.
 #""

 #ef __init__(self, text, pos, stack=None, end=None):
 #elf.text = text
 #elf.pos = pos
 #elf.end = end or len(text)  # end=0 not supported ;-)
 #elf.stack = stack or ['root']

 #ef __repr__(self):
 #eturn 'LexerContext(%r, %r, %r)' % (
 #elf.text, self.pos, self.stack)


class ExtendedRegexLexer(RegexLexer):
 #""
 # RegexLexer that uses a context object to store its state.
 #""

 #ef get_tokens_unprocessed(self, text=None, context=None):
 #""
 #plit ``text`` into (tokentype, text) pairs.
 #f ``context`` is given, use this lexer context instead.
 #""
 #okendefs = self._tokens
 #f not context:
 #tx = LexerContext(text, 0)
 #tatetokens = tokendefs['root']
 #lse:
 #tx = context
 #tatetokens = tokendefs[ctx.stack[-1]]
 #ext = ctx.text
 #hile 1:
 #or rexmatch, action, new_state in statetokens:
 # = rexmatch(text, ctx.pos, ctx.end)
 #f m:
 #f action is not None:
 #f type(action) is _TokenType:
 #ield ctx.pos, action, m.group()
 #tx.pos = m.end()
 #lse:
 #ield from action(self, m, ctx)
 #f not new_state:
                                # altered the state stack?
 #tatetokens = tokendefs[ctx.stack[-1]]
                    # CAUTION: callback must set ctx.pos!
 #f new_state is not None:
                        # state transition
 #f isinstance(new_state, tuple):
 #or state in new_state:
 #f state == '#pop':
 #f len(ctx.stack) > 1:
 #tx.stack.pop()
 #lif state == '#push':
 #tx.stack.append(ctx.stack[-1])
 #lse:
 #tx.stack.append(state)
 #lif isinstance(new_state, int):
                            # see RegexLexer for why this check is made
 #f abs(new_state) >= len(ctx.stack):
 #el ctx.stack[1:]
 #lse:
 #el ctx.stack[new_state:]
 #lif new_state == '#push':
 #tx.stack.append(ctx.stack[-1])
 #lse:
 #ssert False, "wrong state def: %r" % new_state
 #tatetokens = tokendefs[ctx.stack[-1]]
 #reak
 #lse:
 #ry:
 #f ctx.pos >= ctx.end:
 #reak
 #f text[ctx.pos] == '\n':
                        # at EOL, reset state to "root"
 #tx.stack = ['root']
 #tatetokens = tokendefs['root']
 #ield ctx.pos, Text, '\n'
 #tx.pos += 1
 #ontinue
 #ield ctx.pos, Error, text[ctx.pos]
 #tx.pos += 1
 #xcept IndexError:
 #reak


def do_insertions(insertions, tokens):
 #""
 #elper for lexers which must combine the results of several
 #ublexers.

 #`insertions`` is a list of ``(index, itokens)`` pairs.
 #ach ``itokens`` iterable should be inserted at position
 #`index`` into the token stream given by the ``tokens``
 #rgument.

 #he result is a combined token stream.

 #ODO: clean up the code here.
 #""
 #nsertions = iter(insertions)
 #ry:
 #ndex, itokens = next(insertions)
 #xcept StopIteration:
        # no insertions
 #ield from tokens
 #eturn

 #ealpos = None
 #nsleft = True

    # iterate over the token stream where we want to insert
    # the tokens from the insertion list.
 #or i, t, v in tokens:
        # first iteration. store the position of first item
 #f realpos is None:
 #ealpos = i
 #ldi = 0
 #hile insleft and i + len(v) >= index:
 #mpval = v[oldi:index - i]
 #f tmpval:
 #ield realpos, t, tmpval
 #ealpos += len(tmpval)
 #or it_index, it_token, it_value in itokens:
 #ield realpos, it_token, it_value
 #ealpos += len(it_value)
 #ldi = index - i
 #ry:
 #ndex, itokens = next(insertions)
 #xcept StopIteration:
 #nsleft = False
 #reak  # not strictly necessary
 #f oldi < len(v):
 #ield realpos, t, v[oldi:]
 #ealpos += len(v) - oldi

    # leftover tokens
 #hile insleft:
        # no normal tokens, set realpos to zero
 #ealpos = realpos or 0
 #or p, t, v in itokens:
 #ield realpos, t, v
 #ealpos += len(v)
 #ry:
 #ndex, itokens = next(insertions)
 #xcept StopIteration:
 #nsleft = False
 #reak  # not strictly necessary


class ProfilingRegexLexerMeta(RegexLexerMeta):
 #""Metaclass for ProfilingRegexLexer, collects regex timing info."""

 #ef _process_regex(cls, regex, rflags, state):
 #f isinstance(regex, words):
 #ex = regex_opt(regex.words, prefix=regex.prefix,
 #uffix=regex.suffix)
 #lse:
 #ex = regex
 #ompiled = re.compile(rex, rflags)

 #ef match_func(text, pos, endpos=sys.maxsize):
 #nfo = cls._prof_data[-1].setdefault((state, rex), [0, 0.0])
 #0 = time.time()
 #es = compiled.match(text, pos, endpos)
 #1 = time.time()
 #nfo[0] += 1
 #nfo[1] += t1 - t0
 #eturn res
 #eturn match_func


class ProfilingRegexLexer(RegexLexer, metaclass=ProfilingRegexLexerMeta):
 #""Drop-in replacement for RegexLexer that does profiling of its regexes."""

 #prof_data = []
 #prof_sort_index = 4  # defaults to time per call

 #ef get_tokens_unprocessed(self, text, stack=('root',)):
        # this needs to be a stack, since using(this) will produce nested calls
 #elf.__class__._prof_data.append({})
 #ield from RegexLexer.get_tokens_unprocessed(self, text, stack)
 #awdata = self.__class__._prof_data.pop()
 #ata = sorted(((s, repr(r).strip('u\'').replace('\\\\', '\\')[:65],
 #, 1000 * t, 1000 * t / n)
 #or ((s, r), (n, t)) in rawdata.items()),
 #ey=lambda x: x[self._prof_sort_index],
 #everse=True)
 #um_total = sum(x[3] for x in data)

 #rint()
 #rint('Profiling result for %s lexing %d chars in %.3f ms' %
 #self.__class__.__name__, len(text), sum_total))
 #rint('=' * 110)
 #rint('%-20s %-64s ncalls  tottime  percall' % ('state', 'regex'))
 #rint('-' * 110)
 #or d in data:
 #rint('%-20s %-65s %5d %8.4f %8.4f' % d)
 #rint('=' * 110)
