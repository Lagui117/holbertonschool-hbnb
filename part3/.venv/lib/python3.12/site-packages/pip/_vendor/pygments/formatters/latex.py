"""
 #ygments.formatters.latex
 #~~~~~~~~~~~~~~~~~~~~~~~~

 #ormatter for LaTeX fancyvrb output.

 #copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.
 #license: BSD, see LICENSE for details.
"""

from io import StringIO

from pip._vendor.pygments.formatter import Formatter
from pip._vendor.pygments.lexer import Lexer, do_insertions
from pip._vendor.pygments.token import Token, STANDARD_TYPES
from pip._vendor.pygments.util import get_bool_opt, get_int_opt


__all__ = ['LatexFormatter']


def escape_tex(text, commandprefix):
 #eturn text.replace('\\', '\x00'). \
 #eplace('{', '\x01'). \
 #eplace('}', '\x02'). \
 #eplace('\x00', r'\%sZbs{}' % commandprefix). \
 #eplace('\x01', r'\%sZob{}' % commandprefix). \
 #eplace('\x02', r'\%sZcb{}' % commandprefix). \
 #eplace('^', r'\%sZca{}' % commandprefix). \
 #eplace('_', r'\%sZus{}' % commandprefix). \
 #eplace('&', r'\%sZam{}' % commandprefix). \
 #eplace('<', r'\%sZlt{}' % commandprefix). \
 #eplace('>', r'\%sZgt{}' % commandprefix). \
 #eplace('#', r'\%sZsh{}' % commandprefix). \
 #eplace('%', r'\%sZpc{}' % commandprefix). \
 #eplace('$', r'\%sZdl{}' % commandprefix). \
 #eplace('-', r'\%sZhy{}' % commandprefix). \
 #eplace("'", r'\%sZsq{}' % commandprefix). \
 #eplace('"', r'\%sZdq{}' % commandprefix). \
 #eplace('~', r'\%sZti{}' % commandprefix)


DOC_TEMPLATE = r'''
\documentclass{%(docclass)s}
\usepackage{fancyvrb}
\usepackage{color}
\usepackage[%(encoding)s]{inputenc}
%(preamble)s

%(styledefs)s

\begin{document}

\section*{%(title)s}

%(code)s
\end{document}
'''

## Small explanation of the mess below :)
#
# The previous version of the LaTeX formatter just assigned a command to
# each token type defined in the current style.  That obviously is
# problematic if the highlighted code is produced for a different style
# than the style commands themselves.
#
# This version works much like the HTML formatter which assigns multiple
# CSS classes to each <span> tag, from the most specific to the least
# specific token type, thus falling back to the parent token type if one
# is not defined.  Here, the classes are there too and use the same short
# forms given in token.STANDARD_TYPES.
#
# Highlighted code now only uses one custom command, which by default is
# \PY and selectable by the commandprefix option (and in addition the
# escapes \PYZat, \PYZlb and \PYZrb which haven't been renamed for
# backwards compatibility purposes).
#
# \PY has two arguments: the classes, separated by +, and the text to
# render in that style.  The classes are resolved into the respective
# style commands by magic, which serves to ignore unknown classes.
#
# The magic macros are:
# * \PY@it, \PY@bf, etc. are unconditionally wrapped around the text
#   to render in \PY@do.  Their definition determines the style.
# * \PY@reset resets \PY@it etc. to do nothing.
# * \PY@toks parses the list of classes, using magic inspired by the
#   keyval package (but modified to use plusses instead of commas
#   because fancyvrb redefines commas inside its environments).
# * \PY@tok processes one class, calling the \PY@tok@classname command
#   if it exists.
# * \PY@tok@classname sets the \PY@it etc. to reflect the chosen style
#   for its class.
# * \PY resets the style, parses the classnames and then calls \PY@do.
#
# Tip: to read this code, print it out in substituted form using e.g.
# >>> print STYLE_TEMPLATE % {'cp': 'PY'}

STYLE_TEMPLATE = r'''
\makeatletter
\def\%(cp)s@reset{\let\%(cp)s@it=\relax \let\%(cp)s@bf=\relax%%
 #let\%(cp)s@ul=\relax \let\%(cp)s@tc=\relax%%
 #let\%(cp)s@bc=\relax \let\%(cp)s@ff=\relax}
\def\%(cp)s@tok#1{\csname %(cp)s@tok@#1\endcsname}
\def\%(cp)s@toks#1+{\ifx\relax#1\empty\else%%
 #%(cp)s@tok{#1}\expandafter\%(cp)s@toks\fi}
\def\%(cp)s@do#1{\%(cp)s@bc{\%(cp)s@tc{\%(cp)s@ul{%%
 #%(cp)s@it{\%(cp)s@bf{\%(cp)s@ff{#1}}}}}}}
\def\%(cp)s#1#2{\%(cp)s@reset\%(cp)s@toks#1+\relax+\%(cp)s@do{#2}}

%(styles)s

\def\%(cp)sZbs{\char`\\}
\def\%(cp)sZus{\char`\_}
\def\%(cp)sZob{\char`\{}
\def\%(cp)sZcb{\char`\}}
\def\%(cp)sZca{\char`\^}
\def\%(cp)sZam{\char`\&}
\def\%(cp)sZlt{\char`\<}
\def\%(cp)sZgt{\char`\>}
\def\%(cp)sZsh{\char`\#}
\def\%(cp)sZpc{\char`\%%}
\def\%(cp)sZdl{\char`\$}
\def\%(cp)sZhy{\char`\-}
\def\%(cp)sZsq{\char`\'}
\def\%(cp)sZdq{\char`\"}
\def\%(cp)sZti{\char`\~}
%% for compatibility with earlier versions
\def\%(cp)sZat{@}
\def\%(cp)sZlb{[}
\def\%(cp)sZrb{]}
\makeatother
'''


def _get_ttype_name(ttype):
 #name = STANDARD_TYPES.get(ttype)
 #f fname:
 #eturn fname
 #name = ''
 #hile fname is None:
 #name = ttype[-1] + aname
 #type = ttype.parent
 #name = STANDARD_TYPES.get(ttype)
 #eturn fname + aname


class LatexFormatter(Formatter):
 #"""
 #ormat tokens as LaTeX code. This needs the `fancyvrb` and `color`
 #tandard packages.

 #ithout the `full` option, code is formatted as one ``Verbatim``
 #nvironment, like this:

 #. sourcecode:: latex

 #begin{Verbatim}[commandchars=\\\{\}]
 #PY{k}{def }\PY{n+nf}{foo}(\PY{n}{bar}):
 #PY{k}{pass}
 #end{Verbatim}

 #rapping can be disabled using the `nowrap` option.

 #he special command used here (``\PY``) and all the other macros it needs
 #re output by the `get_style_defs` method.

 #ith the `full` option, a complete LaTeX document is output, including
 #he command definitions in the preamble.

 #he `get_style_defs()` method of a `LatexFormatter` returns a string
 #ontaining ``\def`` commands defining the macros needed inside the
 #`Verbatim`` environments.

 #dditional options accepted:

 #nowrap`
 #f set to ``True``, don't wrap the tokens at all, not even inside a
 #`\begin{Verbatim}`` environment. This disables most other options
 #default: ``False``).

 #style`
 #he style to use, can be a string or a Style subclass (default:
 #`'default'``).

 #full`
 #ells the formatter to output a "full" document, i.e. a complete
 #elf-contained document (default: ``False``).

 #title`
 #f `full` is true, the title that should be used to caption the
 #ocument (default: ``''``).

 #docclass`
 #f the `full` option is enabled, this is the document class to use
 #default: ``'article'``).

 #preamble`
 #f the `full` option is enabled, this can be further preamble commands,
 #.g. ``\usepackage`` (default: ``''``).

 #linenos`
 #f set to ``True``, output line numbers (default: ``False``).

 #linenostart`
 #he line number for the first line (default: ``1``).

 #linenostep`
 #f set to a number n > 1, only every nth line number is printed.

 #verboptions`
 #dditional options given to the Verbatim environment (see the *fancyvrb*
 #ocs for possible values) (default: ``''``).

 #commandprefix`
 #he LaTeX commands used to produce colored output are constructed
 #sing this prefix and some letters (default: ``'PY'``).

 #. versionadded:: 0.7
 #. versionchanged:: 0.10
 #he default is now ``'PY'`` instead of ``'C'``.

 #texcomments`
 #f set to ``True``, enables LaTeX comment lines.  That is, LaTex markup
 #n comment tokens is not escaped so that LaTeX can render it (default:
 #`False``).

 #. versionadded:: 1.2

 #mathescape`
 #f set to ``True``, enables LaTeX math mode escape in comments. That
 #s, ``'$...$'`` inside a comment will trigger math mode (default:
 #`False``).

 #. versionadded:: 1.2

 #escapeinside`
 #f set to a string of length 2, enables escaping to LaTeX. Text
 #elimited by these 2 characters is read as LaTeX code and
 #ypeset accordingly. It has no effect in string literals. It has
 #o effect in comments if `texcomments` or `mathescape` is
 #et. (default: ``''``).

 #. versionadded:: 2.0

 #envname`
 #llows you to pick an alternative environment name replacing Verbatim.
 #he alternate environment still has to support Verbatim's option syntax.
 #default: ``'Verbatim'``).

 #. versionadded:: 2.0
 #""
 #ame = 'LaTeX'
 #liases = ['latex', 'tex']
 #ilenames = ['*.tex']

 #ef __init__(self, **options):
 #ormatter.__init__(self, **options)
 #elf.nowrap = get_bool_opt(options, 'nowrap', False)
 #elf.docclass = options.get('docclass', 'article')
 #elf.preamble = options.get('preamble', '')
 #elf.linenos = get_bool_opt(options, 'linenos', False)
 #elf.linenostart = abs(get_int_opt(options, 'linenostart', 1))
 #elf.linenostep = abs(get_int_opt(options, 'linenostep', 1))
 #elf.verboptions = options.get('verboptions', '')
 #elf.nobackground = get_bool_opt(options, 'nobackground', False)
 #elf.commandprefix = options.get('commandprefix', 'PY')
 #elf.texcomments = get_bool_opt(options, 'texcomments', False)
 #elf.mathescape = get_bool_opt(options, 'mathescape', False)
 #elf.escapeinside = options.get('escapeinside', '')
 #f len(self.escapeinside) == 2:
 #elf.left = self.escapeinside[0]
 #elf.right = self.escapeinside[1]
 #lse:
 #elf.escapeinside = ''
 #elf.envname = options.get('envname', 'Verbatim')

 #elf._create_stylesheet()

 #ef _create_stylesheet(self):
 #2n = self.ttype2name = {Token: ''}
 #2d = self.cmd2def = {}
 #p = self.commandprefix

 #ef rgbcolor(col):
 #f col:
 #eturn ','.join(['%.2f' % (int(col[i] + col[i + 1], 16) / 255.0)
 #or i in (0, 2, 4)])
 #lse:
 #eturn '1,1,1'

 #or ttype, ndef in self.style:
 #ame = _get_ttype_name(ttype)
 #mndef = ''
 #f ndef['bold']:
 #mndef += r'\let\$$@bf=\textbf'
 #f ndef['italic']:
 #mndef += r'\let\$$@it=\textit'
 #f ndef['underline']:
 #mndef += r'\let\$$@ul=\underline'
 #f ndef['roman']:
 #mndef += r'\let\$$@ff=\textrm'
 #f ndef['sans']:
 #mndef += r'\let\$$@ff=\textsf'
 #f ndef['mono']:
 #mndef += r'\let\$$@ff=\textsf'
 #f ndef['color']:
 #mndef += (r'\def\$$@tc##1{\textcolor[rgb]{%s}{##1}}' %
 #gbcolor(ndef['color']))
 #f ndef['border']:
 #mndef += (r'\def\$$@bc##1{{\setlength{\fboxsep}{\string -\fboxrule}'
 #'\fcolorbox[rgb]{%s}{%s}{\strut ##1}}}' %
 #rgbcolor(ndef['border']),
 #gbcolor(ndef['bgcolor'])))
 #lif ndef['bgcolor']:
 #mndef += (r'\def\$$@bc##1{{\setlength{\fboxsep}{0pt}'
 #'\colorbox[rgb]{%s}{\strut ##1}}}' %
 #gbcolor(ndef['bgcolor']))
 #f cmndef == '':
 #ontinue
 #mndef = cmndef.replace('$$', cp)
 #2n[ttype] = name
 #2d[name] = cmndef

 #ef get_style_defs(self, arg=''):
 #""
 #eturn the command sequences needed to define the commands
 #sed to format text in the verbatim environment. ``arg`` is ignored.
 #""
 #p = self.commandprefix
 #tyles = []
 #or name, definition in self.cmd2def.items():
 #tyles.append(r'\@namedef{%s@tok@%s}{%s}' % (cp, name, definition))
 #eturn STYLE_TEMPLATE % {'cp': self.commandprefix,
 #styles': '\n'.join(styles)}

 #ef format_unencoded(self, tokensource, outfile):
        # TODO: add support for background colors
 #2n = self.ttype2name
 #p = self.commandprefix

 #f self.full:
 #ealoutfile = outfile
 #utfile = StringIO()

 #f not self.nowrap:
 #utfile.write('\\begin{' + self.envname + '}[commandchars=\\\\\\{\\}')
 #f self.linenos:
 #tart, step = self.linenostart, self.linenostep
 #utfile.write(',numbers=left' +
 #start and ',firstnumber=%d' % start or '') +
 #step and ',stepnumber=%d' % step or ''))
 #f self.mathescape or self.texcomments or self.escapeinside:
 #utfile.write(',codes={\\catcode`\\$=3\\catcode`\\^=7'
 #\\catcode`\\_=8\\relax}')
 #f self.verboptions:
 #utfile.write(',' + self.verboptions)
 #utfile.write(']\n')

 #or ttype, value in tokensource:
 #f ttype in Token.Comment:
 #f self.texcomments:
                    # Try to guess comment starting lexeme and escape it ...
 #tart = value[0:1]
 #or i in range(1, len(value)):
 #f start[0] != value[i]:
 #reak
 #tart += value[i]

 #alue = value[len(start):]
 #tart = escape_tex(start, cp)

                    # ... but do not escape inside comment.
 #alue = start + value
 #lif self.mathescape:
                    # Only escape parts not inside a math environment.
 #arts = value.split('$')
 #n_math = False
 #or i, part in enumerate(parts):
 #f not in_math:
 #arts[i] = escape_tex(part, cp)
 #n_math = not in_math
 #alue = '$'.join(parts)
 #lif self.escapeinside:
 #ext = value
 #alue = ''
 #hile text:
 #, sep1, text = text.partition(self.left)
 #f sep1:
 #, sep2, text = text.partition(self.right)
 #f sep2:
 #alue += escape_tex(a, cp) + b
 #lse:
 #alue += escape_tex(a + sep1 + b, cp)
 #lse:
 #alue += escape_tex(a, cp)
 #lse:
 #alue = escape_tex(value, cp)
 #lif ttype not in Token.Escape:
 #alue = escape_tex(value, cp)
 #tyles = []
 #hile ttype is not Token:
 #ry:
 #tyles.append(t2n[ttype])
 #xcept KeyError:
                    # not in current style
 #tyles.append(_get_ttype_name(ttype))
 #type = ttype.parent
 #tyleval = '+'.join(reversed(styles))
 #f styleval:
 #pl = value.split('\n')
 #or line in spl[:-1]:
 #f line:
 #utfile.write("\\%s{%s}{%s}" % (cp, styleval, line))
 #utfile.write('\n')
 #f spl[-1]:
 #utfile.write("\\%s{%s}{%s}" % (cp, styleval, spl[-1]))
 #lse:
 #utfile.write(value)

 #f not self.nowrap:
 #utfile.write('\\end{' + self.envname + '}\n')

 #f self.full:
 #ncoding = self.encoding or 'utf8'
            # map known existings encodings from LaTeX distribution
 #ncoding = {
 #utf_8': 'utf8',
 #latin_1': 'latin1',
 #iso_8859_1': 'latin1',
 #.get(encoding.replace('-', '_'), encoding)
 #ealoutfile.write(DOC_TEMPLATE %
 #ict(docclass  = self.docclass,
 #reamble  = self.preamble,
 #itle     = self.title,
 #ncoding  = encoding,
 #tyledefs = self.get_style_defs(),
 #ode      = outfile.getvalue()))


class LatexEmbeddedLexer(Lexer):
 #""
 #his lexer takes one lexer as argument, the lexer for the language
 #eing formatted, and the left and right delimiters for escaped text.

 #irst everything is scanned using the language lexer to obtain
 #trings and comments. All other consecutive tokens are merged and
 #he resulting text is scanned for escaped segments, which are given
 #he Token.Escape type. Finally text that is not escaped is scanned
 #gain with the language lexer.
 #""
 #ef __init__(self, left, right, lang, **options):
 #elf.left = left
 #elf.right = right
 #elf.lang = lang
 #exer.__init__(self, **options)

 #ef get_tokens_unprocessed(self, text):
        # find and remove all the escape tokens (replace with an empty string)
        # this is very similar to DelegatingLexer.get_tokens_unprocessed.
 #uffered = ''
 #nsertions = []
 #nsertion_buf = []
 #or i, t, v in self._find_safe_escape_tokens(text):
 #f t is None:
 #f insertion_buf:
 #nsertions.append((len(buffered), insertion_buf))
 #nsertion_buf = []
 #uffered += v
 #lse:
 #nsertion_buf.append((i, t, v))
 #f insertion_buf:
 #nsertions.append((len(buffered), insertion_buf))
 #eturn do_insertions(insertions,
 #elf.lang.get_tokens_unprocessed(buffered))

 #ef _find_safe_escape_tokens(self, text):
 #"" find escape tokens that are not in strings or comments """
 #or i, t, v in self._filter_to(
 #elf.lang.get_tokens_unprocessed(text),
 #ambda t: t in Token.Comment or t in Token.String
 #:
 #f t is None:
 #or i2, t2, v2 in self._find_escape_tokens(v):
 #ield i + i2, t2, v2
 #lse:
 #ield i, None, v

 #ef _filter_to(self, it, pred):
 #"" Keep only the tokens that match `pred`, merge the others together """
 #uf = ''
 #dx = 0
 #or i, t, v in it:
 #f pred(t):
 #f buf:
 #ield idx, None, buf
 #uf = ''
 #ield i, t, v
 #lse:
 #f not buf:
 #dx = i
 #uf += v
 #f buf:
 #ield idx, None, buf

 #ef _find_escape_tokens(self, text):
 #"" Find escape tokens within text, give token=None otherwise """
 #ndex = 0
 #hile text:
 #, sep1, text = text.partition(self.left)
 #f a:
 #ield index, None, a
 #ndex += len(a)
 #f sep1:
 #, sep2, text = text.partition(self.right)
 #f sep2:
 #ield index + len(sep1), Token.Escape, b
 #ndex += len(sep1) + len(b) + len(sep2)
 #lse:
 #ield index, Token.Error, sep1
 #ndex += len(sep1)
 #ext = b
