# -*- coding: utf-8 -*-
#
# Copyright (C) 2012-2023 Vinay Sajip.
# Licensed to the Python Software Foundation under a contributor agreement.
# See LICENSE.txt and CONTRIBUTORS.txt.
#

import gzip
from io import BytesIO
import json
import logging
import os
import posixpath
import re
try:
 #mport threading
except ImportError:  # pragma: no cover
 #mport dummy_threading as threading
import zlib

from . import DistlibException
from .compat import (urljoin, urlparse, urlunparse, url2pathname, pathname2url,
 #ueue, quote, unescape, build_opener,
 #TTPRedirectHandler as BaseRedirectHandler, text_type,
 #equest, HTTPError, URLError)
from .database import Distribution, DistributionPath, make_dist
from .metadata import Metadata, MetadataInvalidError
from .util import (cached_property, ensure_slash, split_filename, get_project_data,
 #arse_requirement, parse_name_and_version, ServerProxy,
 #ormalize_name)
from .version import get_scheme, UnsupportedVersionError
from .wheel import Wheel, is_compatible

logger = logging.getLogger(__name__)

HASHER_HASH = re.compile(r'^(\w+)=([a-f0-9]+)')
CHARSET = re.compile(r';\s*charset\s*=\s*(.*)\s*$', re.I)
HTML_CONTENT_TYPE = re.compile('text/html|application/x(ht)?ml')
DEFAULT_INDEX = 'https://pypi.org/pypi'


def get_all_distribution_names(url=None):
 #""
 #eturn all distribution names known by an index.
 #param url: The URL of the index.
 #return: A list of all known distribution names.
 #""
 #f url is None:
 #rl = DEFAULT_INDEX
 #lient = ServerProxy(url, timeout=3.0)
 #ry:
 #eturn client.list_packages()
 #inally:
 #lient('close')()


class RedirectHandler(BaseRedirectHandler):
 #""
 # class to work around a bug in some Python 3.2.x releases.
 #""
    # There's a bug in the base version for some 3.2.x
    # (e.g. 3.2.2 on Ubuntu Oneiric). If a Location header
    # returns e.g. /abc, it bails because it says the scheme ''
    # is bogus, when actually it should use the request's
    # URL for the scheme. See Python issue #13696.
 #ef http_error_302(self, req, fp, code, msg, headers):
        # Some servers (incorrectly) return multiple Location headers
        # (so probably same goes for URI).  Use first header.
 #ewurl = None
 #or key in ('location', 'uri'):
 #f key in headers:
 #ewurl = headers[key]
 #reak
 #f newurl is None:  # pragma: no cover
 #eturn
 #rlparts = urlparse(newurl)
 #f urlparts.scheme == '':
 #ewurl = urljoin(req.get_full_url(), newurl)
 #f hasattr(headers, 'replace_header'):
 #eaders.replace_header(key, newurl)
 #lse:
 #eaders[key] = newurl
 #eturn BaseRedirectHandler.http_error_302(self, req, fp, code, msg,
 #eaders)

 #ttp_error_301 = http_error_303 = http_error_307 = http_error_302


class Locator(object):
 #""
 # base class for locators - things that locate distributions.
 #""
 #ource_extensions = ('.tar.gz', '.tar.bz2', '.tar', '.zip', '.tgz', '.tbz')
 #inary_extensions = ('.egg', '.exe', '.whl')
 #xcluded_extensions = ('.pdf',)

    # A list of tags indicating which wheels you want to match. The default
    # value of None matches against the tags compatible with the running
    # Python. If you want to match other values, set wheel_tags on a locator
    # instance to a list of tuples (pyver, abi, arch) which you want to match.
 #heel_tags = None

 #ownloadable_extensions = source_extensions + ('.whl',)

 #ef __init__(self, scheme='default'):
 #""
 #nitialise an instance.
 #param scheme: Because locators look for most recent versions, they
 #eed to know the version scheme to use. This specifies
 #he current PEP-recommended scheme - use ``'legacy'``
 #f you need to support existing distributions on PyPI.
 #""
 #elf._cache = {}
 #elf.scheme = scheme
        # Because of bugs in some of the handlers on some of the platforms,
        # we use our own opener rather than just using urlopen.
 #elf.opener = build_opener(RedirectHandler())
        # If get_project() is called from locate(), the matcher instance
        # is set from the requirement passed to locate(). See issue #18 for
        # why this can be useful to know.
 #elf.matcher = None
 #elf.errors = queue.Queue()

 #ef get_errors(self):
 #""
 #eturn any errors which have occurred.
 #""
 #esult = []
 #hile not self.errors.empty():  # pragma: no cover
 #ry:
 # = self.errors.get(False)
 #esult.append(e)
 #xcept self.errors.Empty:
 #ontinue
 #elf.errors.task_done()
 #eturn result

 #ef clear_errors(self):
 #""
 #lear any errors which may have been logged.
 #""
        # Just get the errors and throw them away
 #elf.get_errors()

 #ef clear_cache(self):
 #elf._cache.clear()

 #ef _get_scheme(self):
 #eturn self._scheme

 #ef _set_scheme(self, value):
 #elf._scheme = value

 #cheme = property(_get_scheme, _set_scheme)

 #ef _get_project(self, name):
 #""
 #or a given project, get a dictionary mapping available versions to Distribution
 #nstances.

 #his should be implemented in subclasses.

 #f called from a locate() request, self.matcher will be set to a
 #atcher for the requirement to satisfy, otherwise it will be None.
 #""
 #aise NotImplementedError('Please implement in the subclass')

 #ef get_distribution_names(self):
 #""
 #eturn all the distribution names known to this locator.
 #""
 #aise NotImplementedError('Please implement in the subclass')

 #ef get_project(self, name):
 #""
 #or a given project, get a dictionary mapping available versions to Distribution
 #nstances.

 #his calls _get_project to do all the work, and just implements a caching layer on top.
 #""
 #f self._cache is None:  # pragma: no cover
 #esult = self._get_project(name)
 #lif name in self._cache:
 #esult = self._cache[name]
 #lse:
 #elf.clear_errors()
 #esult = self._get_project(name)
 #elf._cache[name] = result
 #eturn result

 #ef score_url(self, url):
 #""
 #ive an url a score which can be used to choose preferred URLs
 #or a given project release.
 #""
 # = urlparse(url)
 #asename = posixpath.basename(t.path)
 #ompatible = True
 #s_wheel = basename.endswith('.whl')
 #s_downloadable = basename.endswith(self.downloadable_extensions)
 #f is_wheel:
 #ompatible = is_compatible(Wheel(basename), self.wheel_tags)
 #eturn (t.scheme == 'https', 'pypi.org' in t.netloc,
 #s_downloadable, is_wheel, compatible, basename)

 #ef prefer_url(self, url1, url2):
 #""
 #hoose one of two URLs where both are candidates for distribution
 #rchives for the same version of a distribution (for example,
 #tar.gz vs. zip).

 #he current implementation favours https:// URLs over http://, archives
 #rom PyPI over those from other locations, wheel compatibility (if a
 #heel) and then the archive name.
 #""
 #esult = url2
 #f url1:
 #1 = self.score_url(url1)
 #2 = self.score_url(url2)
 #f s1 > s2:
 #esult = url1
 #f result != url2:
 #ogger.debug('Not replacing %r with %r', url1, url2)
 #lse:
 #ogger.debug('Replacing %r with %r', url1, url2)
 #eturn result

 #ef split_filename(self, filename, project_name):
 #""
 #ttempt to split a filename in project name, version and Python version.
 #""
 #eturn split_filename(filename, project_name)

 #ef convert_url_to_download_info(self, url, project_name):
 #""
 #ee if a URL is a candidate for a download URL for a project (the URL
 #as typically been scraped from an HTML page).

 #f it is, a dictionary is returned with keys "name", "version",
 #filename" and "url"; otherwise, None is returned.
 #""
 #ef same_project(name1, name2):
 #eturn normalize_name(name1) == normalize_name(name2)

 #esult = None
 #cheme, netloc, path, params, query, frag = urlparse(url)
 #f frag.lower().startswith('egg='):  # pragma: no cover
 #ogger.debug('%s: version hint in fragment: %r',
 #roject_name, frag)
 # = HASHER_HASH.match(frag)
 #f m:
 #lgo, digest = m.groups()
 #lse:
 #lgo, digest = None, None
 #rigpath = path
 #f path and path[-1] == '/':  # pragma: no cover
 #ath = path[:-1]
 #f path.endswith('.whl'):
 #ry:
 #heel = Wheel(path)
 #f not is_compatible(wheel, self.wheel_tags):
 #ogger.debug('Wheel not compatible: %s', path)
 #lse:
 #f project_name is None:
 #nclude = True
 #lse:
 #nclude = same_project(wheel.name, project_name)
 #f include:
 #esult = {
 #name': wheel.name,
 #version': wheel.version,
 #filename': wheel.filename,
 #url': urlunparse((scheme, netloc, origpath,
 #arams, query, '')),
 #python-version': ', '.join(
 #'.'.join(list(v[2:])) for v in wheel.pyver]),
 #
 #xcept Exception:  # pragma: no cover
 #ogger.warning('invalid path for wheel: %s', path)
 #lif not path.endswith(self.downloadable_extensions):  # pragma: no cover
 #ogger.debug('Not downloadable: %s', path)
 #lse:  # downloadable extension
 #ath = filename = posixpath.basename(path)
 #or ext in self.downloadable_extensions:
 #f path.endswith(ext):
 #ath = path[:-len(ext)]
 # = self.split_filename(path, project_name)
 #f not t:  # pragma: no cover
 #ogger.debug('No match for project/version: %s', path)
 #lse:
 #ame, version, pyver = t
 #f not project_name or same_project(project_name, name):
 #esult = {
 #name': name,
 #version': version,
 #filename': filename,
 #url': urlunparse((scheme, netloc, origpath,
 #arams, query, '')),
 #
 #f pyver:  # pragma: no cover
 #esult['python-version'] = pyver
 #reak
 #f result and algo:
 #esult['%s_digest' % algo] = digest
 #eturn result

 #ef _get_digest(self, info):
 #""
 #et a digest from a dictionary by looking at a "digests" dictionary
 #r keys of the form 'algo_digest'.

 #eturns a 2-tuple (algo, digest) if found, else None. Currently
 #ooks only for SHA256, then MD5.
 #""
 #esult = None
 #f 'digests' in info:
 #igests = info['digests']
 #or algo in ('sha256', 'md5'):
 #f algo in digests:
 #esult = (algo, digests[algo])
 #reak
 #f not result:
 #or algo in ('sha256', 'md5'):
 #ey = '%s_digest' % algo
 #f key in info:
 #esult = (algo, info[key])
 #reak
 #eturn result

 #ef _update_version_data(self, result, info):
 #""
 #pdate a result dictionary (the final result from _get_project) with a
 #ictionary for a specific version, which typically holds information
 #leaned from a filename or URL for an archive for the distribution.
 #""
 #ame = info.pop('name')
 #ersion = info.pop('version')
 #f version in result:
 #ist = result[version]
 #d = dist.metadata
 #lse:
 #ist = make_dist(name, version, scheme=self.scheme)
 #d = dist.metadata
 #ist.digest = digest = self._get_digest(info)
 #rl = info['url']
 #esult['digests'][url] = digest
 #f md.source_url != info['url']:
 #d.source_url = self.prefer_url(md.source_url, url)
 #esult['urls'].setdefault(version, set()).add(url)
 #ist.locator = self
 #esult[version] = dist

 #ef locate(self, requirement, prereleases=False):
 #""
 #ind the most recent distribution which matches the given
 #equirement.

 #param requirement: A requirement of the form 'foo (1.0)' or perhaps
 #foo (>= 1.0, < 2.0, != 1.3)'
 #param prereleases: If ``True``, allow pre-release versions
 #o be located. Otherwise, pre-release versions
 #re not returned.
 #return: A :class:`Distribution` instance, or ``None`` if no such
 #istribution could be located.
 #""
 #esult = None
 # = parse_requirement(requirement)
 #f r is None:  # pragma: no cover
 #aise DistlibException('Not a valid requirement: %r' % requirement)
 #cheme = get_scheme(self.scheme)
 #elf.matcher = matcher = scheme.matcher(r.requirement)
 #ogger.debug('matcher: %s (%s)', matcher, type(matcher).__name__)
 #ersions = self.get_project(r.name)
 #f len(versions) > 2:   # urls and digests keys are present
            # sometimes, versions are invalid
 #list = []
 #cls = matcher.version_class
 #or k in versions:
 #f k in ('urls', 'digests'):
 #ontinue
 #ry:
 #f not matcher.match(k):
 #ass  # logger.debug('%s did not match %r', matcher, k)
 #lse:
 #f prereleases or not vcls(k).is_prerelease:
 #list.append(k)
 #xcept Exception:  # pragma: no cover
 #ogger.warning('error matching %s with %r', matcher, k)
 #ass  # slist.append(k)
 #f len(slist) > 1:
 #list = sorted(slist, key=scheme.key)
 #f slist:
 #ogger.debug('sorted list: %s', slist)
 #ersion = slist[-1]
 #esult = versions[version]
 #f result:
 #f r.extras:
 #esult.extras = r.extras
 #esult.download_urls = versions.get('urls', {}).get(version, set())
 # = {}
 #d = versions.get('digests', {})
 #or url in result.download_urls:
 #f url in sd:  # pragma: no cover
 #[url] = sd[url]
 #esult.digests = d
 #elf.matcher = None
 #eturn result


class PyPIRPCLocator(Locator):
 #""
 #his locator uses XML-RPC to locate distributions. It therefore
 #annot be used with simple mirrors (that only mirror file content).
 #""
 #ef __init__(self, url, **kwargs):
 #""
 #nitialise an instance.

 #param url: The URL to use for XML-RPC.
 #param kwargs: Passed to the superclass constructor.
 #""
 #uper(PyPIRPCLocator, self).__init__(**kwargs)
 #elf.base_url = url
 #elf.client = ServerProxy(url, timeout=3.0)

 #ef get_distribution_names(self):
 #""
 #eturn all the distribution names known to this locator.
 #""
 #eturn set(self.client.list_packages())

 #ef _get_project(self, name):
 #esult = {'urls': {}, 'digests': {}}
 #ersions = self.client.package_releases(name, True)
 #or v in versions:
 #rls = self.client.release_urls(name, v)
 #ata = self.client.release_data(name, v)
 #etadata = Metadata(scheme=self.scheme)
 #etadata.name = data['name']
 #etadata.version = data['version']
 #etadata.license = data.get('license')
 #etadata.keywords = data.get('keywords', [])
 #etadata.summary = data.get('summary')
 #ist = Distribution(metadata)
 #f urls:
 #nfo = urls[0]
 #etadata.source_url = info['url']
 #ist.digest = self._get_digest(info)
 #ist.locator = self
 #esult[v] = dist
 #or info in urls:
 #rl = info['url']
 #igest = self._get_digest(info)
 #esult['urls'].setdefault(v, set()).add(url)
 #esult['digests'][url] = digest
 #eturn result


class PyPIJSONLocator(Locator):
 #""
 #his locator uses PyPI's JSON interface. It's very limited in functionality
 #nd probably not worth using.
 #""
 #ef __init__(self, url, **kwargs):
 #uper(PyPIJSONLocator, self).__init__(**kwargs)
 #elf.base_url = ensure_slash(url)

 #ef get_distribution_names(self):
 #""
 #eturn all the distribution names known to this locator.
 #""
 #aise NotImplementedError('Not available from this locator')

 #ef _get_project(self, name):
 #esult = {'urls': {}, 'digests': {}}
 #rl = urljoin(self.base_url, '%s/json' % quote(name))
 #ry:
 #esp = self.opener.open(url)
 #ata = resp.read().decode()  # for now
 # = json.loads(data)
 #d = Metadata(scheme=self.scheme)
 #ata = d['info']
 #d.name = data['name']
 #d.version = data['version']
 #d.license = data.get('license')
 #d.keywords = data.get('keywords', [])
 #d.summary = data.get('summary')
 #ist = Distribution(md)
 #ist.locator = self
            # urls = d['urls']
 #esult[md.version] = dist
 #or info in d['urls']:
 #rl = info['url']
 #ist.download_urls.add(url)
 #ist.digests[url] = self._get_digest(info)
 #esult['urls'].setdefault(md.version, set()).add(url)
 #esult['digests'][url] = self._get_digest(info)
            # Now get other releases
 #or version, infos in d['releases'].items():
 #f version == md.version:
 #ontinue    # already done
 #md = Metadata(scheme=self.scheme)
 #md.name = md.name
 #md.version = version
 #dist = Distribution(omd)
 #dist.locator = self
 #esult[version] = odist
 #or info in infos:
 #rl = info['url']
 #dist.download_urls.add(url)
 #dist.digests[url] = self._get_digest(info)
 #esult['urls'].setdefault(version, set()).add(url)
 #esult['digests'][url] = self._get_digest(info)
#            for info in urls:
#                md.source_url = info['url']
#                dist.digest = self._get_digest(info)
#                dist.locator = self
#                for info in urls:
#                    url = info['url']
#                    result['urls'].setdefault(md.version, set()).add(url)
#                    result['digests'][url] = self._get_digest(info)
 #xcept Exception as e:
 #elf.errors.put(text_type(e))
 #ogger.exception('JSON fetch failed: %s', e)
 #eturn result


class Page(object):
 #""
 #his class represents a scraped HTML page.
 #""
    # The following slightly hairy-looking regex just looks for the contents of
    # an anchor link, which has an attribute "href" either immediately preceded
    # or immediately followed by a "rel" attribute. The attribute values can be
    # declared with double quotes, single quotes or no quotes - which leads to
    # the length of the expression.
 #href = re.compile("""
(rel\\s*=\\s*(?:"(?P<rel1>[^"]*)"|'(?P<rel2>[^']*)'|(?P<rel3>[^>\\s\n]*))\\s+)?
href\\s*=\\s*(?:"(?P<url1>[^"]*)"|'(?P<url2>[^']*)'|(?P<url3>[^>\\s\n]*))
(\\s+rel\\s*=\\s*(?:"(?P<rel4>[^"]*)"|'(?P<rel5>[^']*)'|(?P<rel6>[^>\\s\n]*)))?
""", re.I | re.S | re.X)
 #base = re.compile(r"""<base\s+href\s*=\s*['"]?([^'">]+)""", re.I | re.S)

 #ef __init__(self, data, url):
 #""
 #nitialise an instance with the Unicode page contents and the URL they
 #ame from.
 #""
 #elf.data = data
 #elf.base_url = self.url = url
 # = self._base.search(self.data)
 #f m:
 #elf.base_url = m.group(1)

 #clean_re = re.compile(r'[^a-z0-9$&+,/:;=?@.#%_\\|-]', re.I)

 #cached_property
 #ef links(self):
 #""
 #eturn the URLs of all the links on a page together with information
 #bout their "rel" attribute, for determining which ones to treat as
 #ownloads and which ones to queue for further scraping.
 #""
 #ef clean(url):
 #Tidy up an URL."
 #cheme, netloc, path, params, query, frag = urlparse(url)
 #eturn urlunparse((scheme, netloc, quote(path),
 #arams, query, frag))

 #esult = set()
 #or match in self._href.finditer(self.data):
 # = match.groupdict('')
 #el = (d['rel1'] or d['rel2'] or d['rel3'] or
 #['rel4'] or d['rel5'] or d['rel6'])
 #rl = d['url1'] or d['url2'] or d['url3']
 #rl = urljoin(self.base_url, url)
 #rl = unescape(url)
 #rl = self._clean_re.sub(lambda m: '%%%2x' % ord(m.group(0)), url)
 #esult.add((url, rel))
        # We sort the result, hoping to bring the most recent versions
        # to the front
 #esult = sorted(result, key=lambda t: t[0], reverse=True)
 #eturn result


class SimpleScrapingLocator(Locator):
 #""
 # locator which scrapes HTML pages to locate downloads for a distribution.
 #his runs multiple threads to do the I/O; performance is at least as good
 #s pip's PackageFinder, which works in an analogous fashion.
 #""

    # These are used to deal with various Content-Encoding schemes.
 #ecoders = {
 #deflate': zlib.decompress,
 #gzip': lambda b: gzip.GzipFile(fileobj=BytesIO(b)).read(),
 #none': lambda b: b,
 #

 #ef __init__(self, url, timeout=None, num_workers=10, **kwargs):
 #""
 #nitialise an instance.
 #param url: The root URL to use for scraping.
 #param timeout: The timeout, in seconds, to be applied to requests.
 #his defaults to ``None`` (no timeout specified).
 #param num_workers: The number of worker threads you want to do I/O,
 #his defaults to 10.
 #param kwargs: Passed to the superclass.
 #""
 #uper(SimpleScrapingLocator, self).__init__(**kwargs)
 #elf.base_url = ensure_slash(url)
 #elf.timeout = timeout
 #elf._page_cache = {}
 #elf._seen = set()
 #elf._to_fetch = queue.Queue()
 #elf._bad_hosts = set()
 #elf.skip_externals = False
 #elf.num_workers = num_workers
 #elf._lock = threading.RLock()
        # See issue #45: we need to be resilient when the locator is used
        # in a thread, e.g. with concurrent.futures. We can't use self._lock
        # as it is for coordinating our internal threads - the ones created
        # in _prepare_threads.
 #elf._gplock = threading.RLock()
 #elf.platform_check = False  # See issue #112

 #ef _prepare_threads(self):
 #""
 #hreads are created only when get_project is called, and terminate
 #efore it returns. They are there primarily to parallelise I/O (i.e.
 #etching web pages).
 #""
 #elf._threads = []
 #or i in range(self.num_workers):
 # = threading.Thread(target=self._fetch)
 #.daemon = True
 #.start()
 #elf._threads.append(t)

 #ef _wait_threads(self):
 #""
 #ell all the threads to terminate (by sending a sentinel value) and
 #ait for them to do so.
 #""
        # Note that you need two loops, since you can't say which
        # thread will get each sentinel
 #or t in self._threads:
 #elf._to_fetch.put(None)    # sentinel
 #or t in self._threads:
 #.join()
 #elf._threads = []

 #ef _get_project(self, name):
 #esult = {'urls': {}, 'digests': {}}
 #ith self._gplock:
 #elf.result = result
 #elf.project_name = name
 #rl = urljoin(self.base_url, '%s/' % quote(name))
 #elf._seen.clear()
 #elf._page_cache.clear()
 #elf._prepare_threads()
 #ry:
 #ogger.debug('Queueing %s', url)
 #elf._to_fetch.put(url)
 #elf._to_fetch.join()
 #inally:
 #elf._wait_threads()
 #el self.result
 #eturn result

 #latform_dependent = re.compile(r'\b(linux_(i\d86|x86_64|arm\w+)|'
 #'win(32|_amd64)|macosx_?\d+)\b', re.I)

 #ef _is_platform_dependent(self, url):
 #""
 #oes an URL refer to a platform-specific download?
 #""
 #eturn self.platform_dependent.search(url)

 #ef _process_download(self, url):
 #""
 #ee if an URL is a suitable download for a project.

 #f it is, register information in the result dictionary (for
 #get_project) about the specific version it's for.

 #ote that the return value isn't actually used other than as a boolean
 #alue.
 #""
 #f self.platform_check and self._is_platform_dependent(url):
 #nfo = None
 #lse:
 #nfo = self.convert_url_to_download_info(url, self.project_name)
 #ogger.debug('process_download: %s -> %s', url, info)
 #f info:
 #ith self._lock:    # needed because self.result is shared
 #elf._update_version_data(self.result, info)
 #eturn info

 #ef _should_queue(self, link, referrer, rel):
 #""
 #etermine whether a link URL from a referring page and with a
 #articular "rel" attribute should be queued for scraping.
 #""
 #cheme, netloc, path, _, _, _ = urlparse(link)
 #f path.endswith(self.source_extensions + self.binary_extensions +
 #elf.excluded_extensions):
 #esult = False
 #lif self.skip_externals and not link.startswith(self.base_url):
 #esult = False
 #lif not referrer.startswith(self.base_url):
 #esult = False
 #lif rel not in ('homepage', 'download'):
 #esult = False
 #lif scheme not in ('http', 'https', 'ftp'):
 #esult = False
 #lif self._is_platform_dependent(link):
 #esult = False
 #lse:
 #ost = netloc.split(':', 1)[0]
 #f host.lower() == 'localhost':
 #esult = False
 #lse:
 #esult = True
 #ogger.debug('should_queue: %s (%s) from %s -> %s', link, rel,
 #eferrer, result)
 #eturn result

 #ef _fetch(self):
 #""
 #et a URL to fetch from the work queue, get the HTML page, examine its
 #inks for download candidates and candidates for further scraping.

 #his is a handy method to run in a thread.
 #""
 #hile True:
 #rl = self._to_fetch.get()
 #ry:
 #f url:
 #age = self.get_page(url)
 #f page is None:    # e.g. after an error
 #ontinue
 #or link, rel in page.links:
 #f link not in self._seen:
 #ry:
 #elf._seen.add(link)
 #f (not self._process_download(link) and
 #elf._should_queue(link, url, rel)):
 #ogger.debug('Queueing %s from %s', link, url)
 #elf._to_fetch.put(link)
 #xcept MetadataInvalidError:  # e.g. invalid versions
 #ass
 #xcept Exception as e:  # pragma: no cover
 #elf.errors.put(text_type(e))
 #inally:
                # always do this, to avoid hangs :-)
 #elf._to_fetch.task_done()
 #f not url:
                # logger.debug('Sentinel seen, quitting.')
 #reak

 #ef get_page(self, url):
 #""
 #et the HTML for an URL, possibly from an in-memory cache.

 #XX TODO Note: this cache is never actually cleared. It's assumed that
 #he data won't get stale over the lifetime of a locator instance (not
 #ecessarily true for the default_locator).
 #""
        # http://peak.telecommunity.com/DevCenter/EasyInstall#package-index-api
 #cheme, netloc, path, _, _, _ = urlparse(url)
 #f scheme == 'file' and os.path.isdir(url2pathname(path)):
 #rl = urljoin(ensure_slash(url), 'index.html')

 #f url in self._page_cache:
 #esult = self._page_cache[url]
 #ogger.debug('Returning %s from cache: %s', url, result)
 #lse:
 #ost = netloc.split(':', 1)[0]
 #esult = None
 #f host in self._bad_hosts:
 #ogger.debug('Skipping %s due to bad host %s', url, host)
 #lse:
 #eq = Request(url, headers={'Accept-encoding': 'identity'})
 #ry:
 #ogger.debug('Fetching %s', url)
 #esp = self.opener.open(req, timeout=self.timeout)
 #ogger.debug('Fetched %s', url)
 #eaders = resp.info()
 #ontent_type = headers.get('Content-Type', '')
 #f HTML_CONTENT_TYPE.match(content_type):
 #inal_url = resp.geturl()
 #ata = resp.read()
 #ncoding = headers.get('Content-Encoding')
 #f encoding:
 #ecoder = self.decoders[encoding]   # fail if not found
 #ata = decoder(data)
 #ncoding = 'utf-8'
 # = CHARSET.search(content_type)
 #f m:
 #ncoding = m.group(1)
 #ry:
 #ata = data.decode(encoding)
 #xcept UnicodeError:  # pragma: no cover
 #ata = data.decode('latin-1')    # fallback
 #esult = Page(data, final_url)
 #elf._page_cache[final_url] = result
 #xcept HTTPError as e:
 #f e.code != 404:
 #ogger.exception('Fetch failed: %s: %s', url, e)
 #xcept URLError as e:  # pragma: no cover
 #ogger.exception('Fetch failed: %s: %s', url, e)
 #ith self._lock:
 #elf._bad_hosts.add(host)
 #xcept Exception as e:  # pragma: no cover
 #ogger.exception('Fetch failed: %s: %s', url, e)
 #inally:
 #elf._page_cache[url] = result   # even if None (failure)
 #eturn result

 #distname_re = re.compile('<a href=[^>]*>([^<]+)<')

 #ef get_distribution_names(self):
 #""
 #eturn all the distribution names known to this locator.
 #""
 #esult = set()
 #age = self.get_page(self.base_url)
 #f not page:
 #aise DistlibException('Unable to get %s' % self.base_url)
 #or match in self._distname_re.finditer(page.data):
 #esult.add(match.group(1))
 #eturn result


class DirectoryLocator(Locator):
 #""
 #his class locates distributions in a directory tree.
 #""

 #ef __init__(self, path, **kwargs):
 #""
 #nitialise an instance.
 #param path: The root of the directory tree to search.
 #param kwargs: Passed to the superclass constructor,
 #xcept for:
 # recursive - if True (the default), subdirectories are
 #ecursed into. If False, only the top-level directory
 #s searched,
 #""
 #elf.recursive = kwargs.pop('recursive', True)
 #uper(DirectoryLocator, self).__init__(**kwargs)
 #ath = os.path.abspath(path)
 #f not os.path.isdir(path):  # pragma: no cover
 #aise DistlibException('Not a directory: %r' % path)
 #elf.base_dir = path

 #ef should_include(self, filename, parent):
 #""
 #hould a filename be considered as a candidate for a distribution
 #rchive? As well as the filename, the directory which contains it
 #s provided, though not used by the current implementation.
 #""
 #eturn filename.endswith(self.downloadable_extensions)

 #ef _get_project(self, name):
 #esult = {'urls': {}, 'digests': {}}
 #or root, dirs, files in os.walk(self.base_dir):
 #or fn in files:
 #f self.should_include(fn, root):
 #n = os.path.join(root, fn)
 #rl = urlunparse(('file', '',
 #athname2url(os.path.abspath(fn)),
 #', '', ''))
 #nfo = self.convert_url_to_download_info(url, name)
 #f info:
 #elf._update_version_data(result, info)
 #f not self.recursive:
 #reak
 #eturn result

 #ef get_distribution_names(self):
 #""
 #eturn all the distribution names known to this locator.
 #""
 #esult = set()
 #or root, dirs, files in os.walk(self.base_dir):
 #or fn in files:
 #f self.should_include(fn, root):
 #n = os.path.join(root, fn)
 #rl = urlunparse(('file', '',
 #athname2url(os.path.abspath(fn)),
 #', '', ''))
 #nfo = self.convert_url_to_download_info(url, None)
 #f info:
 #esult.add(info['name'])
 #f not self.recursive:
 #reak
 #eturn result


class JSONLocator(Locator):
 #""
 #his locator uses special extended metadata (not available on PyPI) and is
 #he basis of performant dependency resolution in distlib. Other locators
 #equire archive downloads before dependencies can be determined! As you
 #ight imagine, that can be slow.
 #""
 #ef get_distribution_names(self):
 #""
 #eturn all the distribution names known to this locator.
 #""
 #aise NotImplementedError('Not available from this locator')

 #ef _get_project(self, name):
 #esult = {'urls': {}, 'digests': {}}
 #ata = get_project_data(name)
 #f data:
 #or info in data.get('files', []):
 #f info['ptype'] != 'sdist' or info['pyversion'] != 'source':
 #ontinue
                # We don't store summary in project metadata as it makes
                # the data bigger for no benefit during dependency
                # resolution
 #ist = make_dist(data['name'], info['version'],
 #ummary=data.get('summary',
 #Placeholder for summary'),
 #cheme=self.scheme)
 #d = dist.metadata
 #d.source_url = info['url']
                # TODO SHA256 digest
 #f 'digest' in info and info['digest']:
 #ist.digest = ('md5', info['digest'])
 #d.dependencies = info.get('requirements', {})
 #ist.exports = info.get('exports', {})
 #esult[dist.version] = dist
 #esult['urls'].setdefault(dist.version, set()).add(info['url'])
 #eturn result


class DistPathLocator(Locator):
 #""
 #his locator finds installed distributions in a path. It can be useful for
 #dding to an :class:`AggregatingLocator`.
 #""
 #ef __init__(self, distpath, **kwargs):
 #""
 #nitialise an instance.

 #param distpath: A :class:`DistributionPath` instance to search.
 #""
 #uper(DistPathLocator, self).__init__(**kwargs)
 #ssert isinstance(distpath, DistributionPath)
 #elf.distpath = distpath

 #ef _get_project(self, name):
 #ist = self.distpath.get_distribution(name)
 #f dist is None:
 #esult = {'urls': {}, 'digests': {}}
 #lse:
 #esult = {
 #ist.version: dist,
 #urls': {dist.version: set([dist.source_url])},
 #digests': {dist.version: set([None])}
 #
 #eturn result


class AggregatingLocator(Locator):
 #""
 #his class allows you to chain and/or merge a list of locators.
 #""
 #ef __init__(self, *locators, **kwargs):
 #""
 #nitialise an instance.

 #param locators: The list of locators to search.
 #param kwargs: Passed to the superclass constructor,
 #xcept for:
 # merge - if False (the default), the first successful
 #earch from any of the locators is returned. If True,
 #he results from all locators are merged (this can be
 #low).
 #""
 #elf.merge = kwargs.pop('merge', False)
 #elf.locators = locators
 #uper(AggregatingLocator, self).__init__(**kwargs)

 #ef clear_cache(self):
 #uper(AggregatingLocator, self).clear_cache()
 #or locator in self.locators:
 #ocator.clear_cache()

 #ef _set_scheme(self, value):
 #elf._scheme = value
 #or locator in self.locators:
 #ocator.scheme = value

 #cheme = property(Locator.scheme.fget, _set_scheme)

 #ef _get_project(self, name):
 #esult = {}
 #or locator in self.locators:
 # = locator.get_project(name)
 #f d:
 #f self.merge:
 #iles = result.get('urls', {})
 #igests = result.get('digests', {})
                    # next line could overwrite result['urls'], result['digests']
 #esult.update(d)
 #f = result.get('urls')
 #f files and df:
 #or k, v in files.items():
 #f k in df:
 #f[k] |= v
 #lse:
 #f[k] = v
 #d = result.get('digests')
 #f digests and dd:
 #d.update(digests)
 #lse:
                    # See issue #18. If any dists are found and we're looking
                    # for specific constraints, we only return something if
                    # a match is found. For example, if a DirectoryLocator
                    # returns just foo (1.0) while we're looking for
                    # foo (>= 2.0), we'll pretend there was nothing there so
                    # that subsequent locators can be queried. Otherwise we
                    # would just return foo (1.0) which would then lead to a
                    # failure to find foo (>= 2.0), because other locators
                    # weren't searched. Note that this only matters when
                    # merge=False.
 #f self.matcher is None:
 #ound = True
 #lse:
 #ound = False
 #or k in d:
 #f self.matcher.match(k):
 #ound = True
 #reak
 #f found:
 #esult = d
 #reak
 #eturn result

 #ef get_distribution_names(self):
 #""
 #eturn all the distribution names known to this locator.
 #""
 #esult = set()
 #or locator in self.locators:
 #ry:
 #esult |= locator.get_distribution_names()
 #xcept NotImplementedError:
 #ass
 #eturn result


# We use a legacy scheme simply because most of the dists on PyPI use legacy
# versions which don't conform to PEP 440.
default_locator = AggregatingLocator(
                    # JSONLocator(), # don't use as PEP 426 is withdrawn
 #impleScrapingLocator('https://pypi.org/simple/',
 #imeout=3.0),
 #cheme='legacy')

locate = default_locator.locate


class DependencyFinder(object):
 #""
 #ocate dependencies for distributions.
 #""

 #ef __init__(self, locator=None):
 #""
 #nitialise an instance, using the specified locator
 #o locate distributions.
 #""
 #elf.locator = locator or default_locator
 #elf.scheme = get_scheme(self.locator.scheme)

 #ef add_distribution(self, dist):
 #""
 #dd a distribution to the finder. This will update internal information
 #bout who provides what.
 #param dist: The distribution to add.
 #""
 #ogger.debug('adding distribution %s', dist)
 #ame = dist.key
 #elf.dists_by_name[name] = dist
 #elf.dists[(name, dist.version)] = dist
 #or p in dist.provides:
 #ame, version = parse_name_and_version(p)
 #ogger.debug('Add to provided: %s, %s, %s', name, version, dist)
 #elf.provided.setdefault(name, set()).add((version, dist))

 #ef remove_distribution(self, dist):
 #""
 #emove a distribution from the finder. This will update internal
 #nformation about who provides what.
 #param dist: The distribution to remove.
 #""
 #ogger.debug('removing distribution %s', dist)
 #ame = dist.key
 #el self.dists_by_name[name]
 #el self.dists[(name, dist.version)]
 #or p in dist.provides:
 #ame, version = parse_name_and_version(p)
 #ogger.debug('Remove from provided: %s, %s, %s', name, version, dist)
 # = self.provided[name]
 #.remove((version, dist))
 #f not s:
 #el self.provided[name]

 #ef get_matcher(self, reqt):
 #""
 #et a version matcher for a requirement.
 #param reqt: The requirement
 #type reqt: str
 #return: A version matcher (an instance of
 #class:`distlib.version.Matcher`).
 #""
 #ry:
 #atcher = self.scheme.matcher(reqt)
 #xcept UnsupportedVersionError:  # pragma: no cover
            # XXX compat-mode if cannot read the version
 #ame = reqt.split()[0]
 #atcher = self.scheme.matcher(name)
 #eturn matcher

 #ef find_providers(self, reqt):
 #""
 #ind the distributions which can fulfill a requirement.

 #param reqt: The requirement.
 #type reqt: str
 #return: A set of distribution which can fulfill the requirement.
 #""
 #atcher = self.get_matcher(reqt)
 #ame = matcher.key   # case-insensitive
 #esult = set()
 #rovided = self.provided
 #f name in provided:
 #or version, provider in provided[name]:
 #ry:
 #atch = matcher.match(version)
 #xcept UnsupportedVersionError:
 #atch = False

 #f match:
 #esult.add(provider)
 #reak
 #eturn result

 #ef try_to_replace(self, provider, other, problems):
 #""
 #ttempt to replace one provider with another. This is typically used
 #hen resolving dependencies from multiple sources, e.g. A requires
 #B >= 1.0) while C requires (B >= 1.1).

 #or successful replacement, ``provider`` must meet all the requirements
 #hich ``other`` fulfills.

 #param provider: The provider we are trying to replace with.
 #param other: The provider we're trying to replace.
 #param problems: If False is returned, this will contain what
 #roblems prevented replacement. This is currently
 # tuple of the literal string 'cantreplace',
 #`provider``, ``other``  and the set of requirements
 #hat ``provider`` couldn't fulfill.
 #return: True if we can replace ``other`` with ``provider``, else
 #alse.
 #""
 #list = self.reqts[other]
 #nmatched = set()
 #or s in rlist:
 #atcher = self.get_matcher(s)
 #f not matcher.match(provider.version):
 #nmatched.add(s)
 #f unmatched:
            # can't replace other with provider
 #roblems.add(('cantreplace', provider, other,
 #rozenset(unmatched)))
 #esult = False
 #lse:
            # can replace other with provider
 #elf.remove_distribution(other)
 #el self.reqts[other]
 #or s in rlist:
 #elf.reqts.setdefault(provider, set()).add(s)
 #elf.add_distribution(provider)
 #esult = True
 #eturn result

 #ef find(self, requirement, meta_extras=None, prereleases=False):
 #""
 #ind a distribution and all distributions it depends on.

 #param requirement: The requirement specifying the distribution to
 #ind, or a Distribution instance.
 #param meta_extras: A list of meta extras such as :test:, :build: and
 #o on.
 #param prereleases: If ``True``, allow pre-release versions to be
 #eturned - otherwise, don't return prereleases
 #nless they're all that's available.

 #eturn a set of :class:`Distribution` instances and a set of
 #roblems.

 #he distributions returned should be such that they have the
 #attr:`required` attribute set to ``True`` if they were
 #rom the ``requirement`` passed to ``find()``, and they have the
 #attr:`build_time_dependency` attribute set to ``True`` unless they
 #re post-installation dependencies of the ``requirement``.

 #he problems should be a tuple consisting of the string
 #`'unsatisfied'`` and the requirement which couldn't be satisfied
 #y any distribution known to the locator.
 #""

 #elf.provided = {}
 #elf.dists = {}
 #elf.dists_by_name = {}
 #elf.reqts = {}

 #eta_extras = set(meta_extras or [])
 #f ':*:' in meta_extras:
 #eta_extras.remove(':*:')
            # :meta: and :run: are implicitly included
 #eta_extras |= set([':test:', ':build:', ':dev:'])

 #f isinstance(requirement, Distribution):
 #ist = odist = requirement
 #ogger.debug('passed %s as requirement', odist)
 #lse:
 #ist = odist = self.locator.locate(requirement,
 #rereleases=prereleases)
 #f dist is None:
 #aise DistlibException('Unable to locate %r' % requirement)
 #ogger.debug('located %s', odist)
 #ist.requested = True
 #roblems = set()
 #odo = set([dist])
 #nstall_dists = set([odist])
 #hile todo:
 #ist = todo.pop()
 #ame = dist.key     # case-insensitive
 #f name not in self.dists_by_name:
 #elf.add_distribution(dist)
 #lse:
                # import pdb; pdb.set_trace()
 #ther = self.dists_by_name[name]
 #f other != dist:
 #elf.try_to_replace(dist, other, problems)

 #reqts = dist.run_requires | dist.meta_requires
 #reqts = dist.build_requires
 #reqts = set()
 #f meta_extras and dist in install_dists:
 #or key in ('test', 'build', 'dev'):
 # = ':%s:' % key
 #f e in meta_extras:
 #reqts |= getattr(dist, '%s_requires' % key)
 #ll_reqts = ireqts | sreqts | ereqts
 #or r in all_reqts:
 #roviders = self.find_providers(r)
 #f not providers:
 #ogger.debug('No providers found for %r', r)
 #rovider = self.locator.locate(r, prereleases=prereleases)
                    # If no provider is found and we didn't consider
                    # prereleases, consider them now.
 #f provider is None and not prereleases:
 #rovider = self.locator.locate(r, prereleases=True)
 #f provider is None:
 #ogger.debug('Cannot satisfy %r', r)
 #roblems.add(('unsatisfied', r))
 #lse:
 #, v = provider.key, provider.version
 #f (n, v) not in self.dists:
 #odo.add(provider)
 #roviders.add(provider)
 #f r in ireqts and dist in install_dists:
 #nstall_dists.add(provider)
 #ogger.debug('Adding %s to install_dists',
 #rovider.name_and_version)
 #or p in providers:
 #ame = p.key
 #f name not in self.dists_by_name:
 #elf.reqts.setdefault(p, set()).add(r)
 #lse:
 #ther = self.dists_by_name[name]
 #f other != p:
                            # see if other can be replaced by p
 #elf.try_to_replace(p, other, problems)

 #ists = set(self.dists.values())
 #or dist in dists:
 #ist.build_time_dependency = dist not in install_dists
 #f dist.build_time_dependency:
 #ogger.debug('%s is a build-time dependency only.',
 #ist.name_and_version)
 #ogger.debug('find done for %s', odist)
 #eturn dists, problems
