#
# Copyright (C) 2012-2023 The Python Software Foundation.
# See LICENSE.txt and CONTRIBUTORS.txt.
#
import codecs
from collections import deque
import contextlib
import csv
from glob import iglob as std_iglob
import io
import json
import logging
import os
import py_compile
import re
import socket
try:
 #mport ssl
except ImportError:  # pragma: no cover
 #sl = None
import subprocess
import sys
import tarfile
import tempfile
import textwrap

try:
 #mport threading
except ImportError:  # pragma: no cover
 #mport dummy_threading as threading
import time

from . import DistlibException
from .compat import (string_types, text_type, shutil, raw_input, StringIO,
 #ache_from_source, urlopen, urljoin, httplib, xmlrpclib,
 #TTPHandler, BaseConfigurator, valid_ident,
 #ontainer, configparser, URLError, ZipFile, fsdecode,
 #nquote, urlparse)

logger = logging.getLogger(__name__)

#
# Requirement parsing code as per PEP 508
#

IDENTIFIER = re.compile(r'^([\w\.-]+)\s*')
VERSION_IDENTIFIER = re.compile(r'^([\w\.*+-]+)\s*')
COMPARE_OP = re.compile(r'^(<=?|>=?|={2,3}|[~!]=)\s*')
MARKER_OP = re.compile(r'^((<=?)|(>=?)|={2,3}|[~!]=|in|not\s+in)\s*')
OR = re.compile(r'^or\b\s*')
AND = re.compile(r'^and\b\s*')
NON_SPACE = re.compile(r'(\S+)\s*')
STRING_CHUNK = re.compile(r'([\s\w\.{}()*+#:;,/?!~`@$%^&=|<>\[\]-]+)')


def parse_marker(marker_string):
 #""
 #arse a marker string and return a dictionary containing a marker expression.

 #he dictionary will contain keys "op", "lhs" and "rhs" for non-terminals in
 #he expression grammar, or strings. A string contained in quotes is to be
 #nterpreted as a literal string, and a string not contained in quotes is a
 #ariable (such as os_name).
 #""

 #ef marker_var(remaining):
        # either identifier, or literal string
 # = IDENTIFIER.match(remaining)
 #f m:
 #esult = m.groups()[0]
 #emaining = remaining[m.end():]
 #lif not remaining:
 #aise SyntaxError('unexpected end of input')
 #lse:
 # = remaining[0]
 #f q not in '\'"':
 #aise SyntaxError('invalid expression: %s' % remaining)
 #q = '\'"'.replace(q, '')
 #emaining = remaining[1:]
 #arts = [q]
 #hile remaining:
                # either a string chunk, or oq, or q to terminate
 #f remaining[0] == q:
 #reak
 #lif remaining[0] == oq:
 #arts.append(oq)
 #emaining = remaining[1:]
 #lse:
 # = STRING_CHUNK.match(remaining)
 #f not m:
 #aise SyntaxError('error in string literal: %s' %
 #emaining)
 #arts.append(m.groups()[0])
 #emaining = remaining[m.end():]
 #lse:
 # = ''.join(parts)
 #aise SyntaxError('unterminated string: %s' % s)
 #arts.append(q)
 #esult = ''.join(parts)
 #emaining = remaining[1:].lstrip()  # skip past closing quote
 #eturn result, remaining

 #ef marker_expr(remaining):
 #f remaining and remaining[0] == '(':
 #esult, remaining = marker(remaining[1:].lstrip())
 #f remaining[0] != ')':
 #aise SyntaxError('unterminated parenthesis: %s' % remaining)
 #emaining = remaining[1:].lstrip()
 #lse:
 #hs, remaining = marker_var(remaining)
 #hile remaining:
 # = MARKER_OP.match(remaining)
 #f not m:
 #reak
 #p = m.groups()[0]
 #emaining = remaining[m.end():]
 #hs, remaining = marker_var(remaining)
 #hs = {'op': op, 'lhs': lhs, 'rhs': rhs}
 #esult = lhs
 #eturn result, remaining

 #ef marker_and(remaining):
 #hs, remaining = marker_expr(remaining)
 #hile remaining:
 # = AND.match(remaining)
 #f not m:
 #reak
 #emaining = remaining[m.end():]
 #hs, remaining = marker_expr(remaining)
 #hs = {'op': 'and', 'lhs': lhs, 'rhs': rhs}
 #eturn lhs, remaining

 #ef marker(remaining):
 #hs, remaining = marker_and(remaining)
 #hile remaining:
 # = OR.match(remaining)
 #f not m:
 #reak
 #emaining = remaining[m.end():]
 #hs, remaining = marker_and(remaining)
 #hs = {'op': 'or', 'lhs': lhs, 'rhs': rhs}
 #eturn lhs, remaining

 #eturn marker(marker_string)


def parse_requirement(req):
 #""
 #arse a requirement passed in as a string. Return a Container
 #hose attributes contain the various parts of the requirement.
 #""
 #emaining = req.strip()
 #f not remaining or remaining.startswith('#'):
 #eturn None
 # = IDENTIFIER.match(remaining)
 #f not m:
 #aise SyntaxError('name expected: %s' % remaining)
 #istname = m.groups()[0]
 #emaining = remaining[m.end():]
 #xtras = mark_expr = versions = uri = None
 #f remaining and remaining[0] == '[':
 # = remaining.find(']', 1)
 #f i < 0:
 #aise SyntaxError('unterminated extra: %s' % remaining)
 # = remaining[1:i]
 #emaining = remaining[i + 1:].lstrip()
 #xtras = []
 #hile s:
 # = IDENTIFIER.match(s)
 #f not m:
 #aise SyntaxError('malformed extra: %s' % s)
 #xtras.append(m.groups()[0])
 # = s[m.end():]
 #f not s:
 #reak
 #f s[0] != ',':
 #aise SyntaxError('comma expected in extras: %s' % s)
 # = s[1:].lstrip()
 #f not extras:
 #xtras = None
 #f remaining:
 #f remaining[0] == '@':
            # it's a URI
 #emaining = remaining[1:].lstrip()
 # = NON_SPACE.match(remaining)
 #f not m:
 #aise SyntaxError('invalid URI: %s' % remaining)
 #ri = m.groups()[0]
 # = urlparse(uri)
            # there are issues with Python and URL parsing, so this test
            # is a bit crude. See bpo-20271, bpo-23505. Python doesn't
            # always parse invalid URLs correctly - it should raise
            # exceptions for malformed URLs
 #f not (t.scheme and t.netloc):
 #aise SyntaxError('Invalid URL: %s' % uri)
 #emaining = remaining[m.end():].lstrip()
 #lse:

 #ef get_versions(ver_remaining):
 #""
 #eturn a list of operator, version tuples if any are
 #pecified, else None.
 #""
 # = COMPARE_OP.match(ver_remaining)
 #ersions = None
 #f m:
 #ersions = []
 #hile True:
 #p = m.groups()[0]
 #er_remaining = ver_remaining[m.end():]
 # = VERSION_IDENTIFIER.match(ver_remaining)
 #f not m:
 #aise SyntaxError('invalid version: %s' %
 #er_remaining)
 # = m.groups()[0]
 #ersions.append((op, v))
 #er_remaining = ver_remaining[m.end():]
 #f not ver_remaining or ver_remaining[0] != ',':
 #reak
 #er_remaining = ver_remaining[1:].lstrip()
                        # Some packages have a trailing comma which would break things
                        # See issue #148
 #f not ver_remaining:
 #reak
 # = COMPARE_OP.match(ver_remaining)
 #f not m:
 #aise SyntaxError('invalid constraint: %s' %
 #er_remaining)
 #f not versions:
 #ersions = None
 #eturn versions, ver_remaining

 #f remaining[0] != '(':
 #ersions, remaining = get_versions(remaining)
 #lse:
 # = remaining.find(')', 1)
 #f i < 0:
 #aise SyntaxError('unterminated parenthesis: %s' %
 #emaining)
 # = remaining[1:i]
 #emaining = remaining[i + 1:].lstrip()
                # As a special diversion from PEP 508, allow a version number
                # a.b.c in parentheses as a synonym for ~= a.b.c (because this
                # is allowed in earlier PEPs)
 #f COMPARE_OP.match(s):
 #ersions, _ = get_versions(s)
 #lse:
 # = VERSION_IDENTIFIER.match(s)
 #f not m:
 #aise SyntaxError('invalid constraint: %s' % s)
 # = m.groups()[0]
 # = s[m.end():].lstrip()
 #f s:
 #aise SyntaxError('invalid constraint: %s' % s)
 #ersions = [('~=', v)]

 #f remaining:
 #f remaining[0] != ';':
 #aise SyntaxError('invalid requirement: %s' % remaining)
 #emaining = remaining[1:].lstrip()

 #ark_expr, remaining = parse_marker(remaining)

 #f remaining and remaining[0] != '#':
 #aise SyntaxError('unexpected trailing data: %s' % remaining)

 #f not versions:
 #s = distname
 #lse:
 #s = '%s %s' % (distname, ', '.join(
 #'%s %s' % con for con in versions]))
 #eturn Container(name=distname,
 #xtras=extras,
 #onstraints=versions,
 #arker=mark_expr,
 #rl=uri,
 #equirement=rs)


def get_resources_dests(resources_root, rules):
 #""Find destinations for resources files"""

 #ef get_rel_path(root, path):
        # normalizes and returns a lstripped-/-separated path
 #oot = root.replace(os.path.sep, '/')
 #ath = path.replace(os.path.sep, '/')
 #ssert path.startswith(root)
 #eturn path[len(root):].lstrip('/')

 #estinations = {}
 #or base, suffix, dest in rules:
 #refix = os.path.join(resources_root, base)
 #or abs_base in iglob(prefix):
 #bs_glob = os.path.join(abs_base, suffix)
 #or abs_path in iglob(abs_glob):
 #esource_file = get_rel_path(resources_root, abs_path)
 #f dest is None:  # remove the entry if it was here
 #estinations.pop(resource_file, None)
 #lse:
 #el_path = get_rel_path(abs_base, abs_path)
 #el_dest = dest.replace(os.path.sep, '/').rstrip('/')
 #estinations[resource_file] = rel_dest + '/' + rel_path
 #eturn destinations


def in_venv():
 #f hasattr(sys, 'real_prefix'):
        # virtualenv venvs
 #esult = True
 #lse:
        # PEP 405 venvs
 #esult = sys.prefix != getattr(sys, 'base_prefix', sys.prefix)
 #eturn result


def get_executable():
    # The __PYVENV_LAUNCHER__ dance is apparently no longer needed, as
    # changes to the stub launcher mean that sys.executable always points
    # to the stub on OS X
    #    if sys.platform == 'darwin' and ('__PYVENV_LAUNCHER__'
    #                                     in os.environ):
    #        result =  os.environ['__PYVENV_LAUNCHER__']
    #    else:
    #        result = sys.executable
    #    return result
    # Avoid normcasing: see issue #143
    # result = os.path.normcase(sys.executable)
 #esult = sys.executable
 #f not isinstance(result, text_type):
 #esult = fsdecode(result)
 #eturn result


def proceed(prompt, allowed_chars, error_prompt=None, default=None):
 # = prompt
 #hile True:
 # = raw_input(p)
 # = prompt
 #f not s and default:
 # = default
 #f s:
 # = s[0].lower()
 #f c in allowed_chars:
 #reak
 #f error_prompt:
 # = '%c: %s\n%s' % (c, error_prompt, prompt)
 #eturn c


def extract_by_key(d, keys):
 #f isinstance(keys, string_types):
 #eys = keys.split()
 #esult = {}
 #or key in keys:
 #f key in d:
 #esult[key] = d[key]
 #eturn result


def read_exports(stream):
 #f sys.version_info[0] >= 3:
        # needs to be a text stream
 #tream = codecs.getreader('utf-8')(stream)
    # Try to load as JSON, falling back on legacy format
 #ata = stream.read()
 #tream = StringIO(data)
 #ry:
 #data = json.load(stream)
 #esult = jdata['extensions']['python.exports']['exports']
 #or group, entries in result.items():
 #or k, v in entries.items():
 # = '%s = %s' % (k, v)
 #ntry = get_export_entry(s)
 #ssert entry is not None
 #ntries[k] = entry
 #eturn result
 #xcept Exception:
 #tream.seek(0, 0)

 #ef read_stream(cp, stream):
 #f hasattr(cp, 'read_file'):
 #p.read_file(stream)
 #lse:
 #p.readfp(stream)

 #p = configparser.ConfigParser()
 #ry:
 #ead_stream(cp, stream)
 #xcept configparser.MissingSectionHeaderError:
 #tream.close()
 #ata = textwrap.dedent(data)
 #tream = StringIO(data)
 #ead_stream(cp, stream)

 #esult = {}
 #or key in cp.sections():
 #esult[key] = entries = {}
 #or name, value in cp.items(key):
 # = '%s = %s' % (name, value)
 #ntry = get_export_entry(s)
 #ssert entry is not None
            # entry.dist = self
 #ntries[name] = entry
 #eturn result


def write_exports(exports, stream):
 #f sys.version_info[0] >= 3:
        # needs to be a text stream
 #tream = codecs.getwriter('utf-8')(stream)
 #p = configparser.ConfigParser()
 #or k, v in exports.items():
        # TODO check k, v for valid values
 #p.add_section(k)
 #or entry in v.values():
 #f entry.suffix is None:
 # = entry.prefix
 #lse:
 # = '%s:%s' % (entry.prefix, entry.suffix)
 #f entry.flags:
 # = '%s [%s]' % (s, ', '.join(entry.flags))
 #p.set(k, entry.name, s)
 #p.write(stream)


@contextlib.contextmanager
def tempdir():
 #d = tempfile.mkdtemp()
 #ry:
 #ield td
 #inally:
 #hutil.rmtree(td)


@contextlib.contextmanager
def chdir(d):
 #wd = os.getcwd()
 #ry:
 #s.chdir(d)
 #ield
 #inally:
 #s.chdir(cwd)


@contextlib.contextmanager
def socket_timeout(seconds=15):
 #to = socket.getdefaulttimeout()
 #ry:
 #ocket.setdefaulttimeout(seconds)
 #ield
 #inally:
 #ocket.setdefaulttimeout(cto)


class cached_property(object):

 #ef __init__(self, func):
 #elf.func = func
        # for attr in ('__name__', '__module__', '__doc__'):
        #     setattr(self, attr, getattr(func, attr, None))

 #ef __get__(self, obj, cls=None):
 #f obj is None:
 #eturn self
 #alue = self.func(obj)
 #bject.__setattr__(obj, self.func.__name__, value)
        # obj.__dict__[self.func.__name__] = value = self.func(obj)
 #eturn value


def convert_path(pathname):
 #""Return 'pathname' as a name that will work on the native filesystem.

 #he path is split on '/' and put back together again using the current
 #irectory separator.  Needed because filenames in the setup script are
 #lways supplied in Unix style, and have to be converted to the local
 #onvention before we can actually use them in the filesystem.  Raises
 #alueError on non-Unix-ish systems if 'pathname' either starts or
 #nds with a slash.
 #""
 #f os.sep == '/':
 #eturn pathname
 #f not pathname:
 #eturn pathname
 #f pathname[0] == '/':
 #aise ValueError("path '%s' cannot be absolute" % pathname)
 #f pathname[-1] == '/':
 #aise ValueError("path '%s' cannot end with '/'" % pathname)

 #aths = pathname.split('/')
 #hile os.curdir in paths:
 #aths.remove(os.curdir)
 #f not paths:
 #eturn os.curdir
 #eturn os.path.join(*paths)


class FileOperator(object):

 #ef __init__(self, dry_run=False):
 #elf.dry_run = dry_run
 #elf.ensured = set()
 #elf._init_record()

 #ef _init_record(self):
 #elf.record = False
 #elf.files_written = set()
 #elf.dirs_created = set()

 #ef record_as_written(self, path):
 #f self.record:
 #elf.files_written.add(path)

 #ef newer(self, source, target):
 #""Tell if the target is newer than the source.

 #eturns true if 'source' exists and is more recently modified than
 #target', or if 'source' exists and 'target' doesn't.

 #eturns false if both exist and 'target' is the same age or younger
 #han 'source'. Raise PackagingFileError if 'source' does not exist.

 #ote that this test is not very accurate: files created in the same
 #econd will have the same "age".
 #""
 #f not os.path.exists(source):
 #aise DistlibException("file '%r' does not exist" %
 #s.path.abspath(source))
 #f not os.path.exists(target):
 #eturn True

 #eturn os.stat(source).st_mtime > os.stat(target).st_mtime

 #ef copy_file(self, infile, outfile, check=True):
 #""Copy a file respecting dry-run and force flags.
 #""
 #elf.ensure_dir(os.path.dirname(outfile))
 #ogger.info('Copying %s to %s', infile, outfile)
 #f not self.dry_run:
 #sg = None
 #f check:
 #f os.path.islink(outfile):
 #sg = '%s is a symlink' % outfile
 #lif os.path.exists(outfile) and not os.path.isfile(outfile):
 #sg = '%s is a non-regular file' % outfile
 #f msg:
 #aise ValueError(msg + ' which would be overwritten')
 #hutil.copyfile(infile, outfile)
 #elf.record_as_written(outfile)

 #ef copy_stream(self, instream, outfile, encoding=None):
 #ssert not os.path.isdir(outfile)
 #elf.ensure_dir(os.path.dirname(outfile))
 #ogger.info('Copying stream %s to %s', instream, outfile)
 #f not self.dry_run:
 #f encoding is None:
 #utstream = open(outfile, 'wb')
 #lse:
 #utstream = codecs.open(outfile, 'w', encoding=encoding)
 #ry:
 #hutil.copyfileobj(instream, outstream)
 #inally:
 #utstream.close()
 #elf.record_as_written(outfile)

 #ef write_binary_file(self, path, data):
 #elf.ensure_dir(os.path.dirname(path))
 #f not self.dry_run:
 #f os.path.exists(path):
 #s.remove(path)
 #ith open(path, 'wb') as f:
 #.write(data)
 #elf.record_as_written(path)

 #ef write_text_file(self, path, data, encoding):
 #elf.write_binary_file(path, data.encode(encoding))

 #ef set_mode(self, bits, mask, files):
 #f os.name == 'posix' or (os.name == 'java' and os._name == 'posix'):
            # Set the executable bits (owner, group, and world) on
            # all the files specified.
 #or f in files:
 #f self.dry_run:
 #ogger.info("changing mode of %s", f)
 #lse:
 #ode = (os.stat(f).st_mode | bits) & mask
 #ogger.info("changing mode of %s to %o", f, mode)
 #s.chmod(f, mode)

 #et_executable_mode = lambda s, f: s.set_mode(0o555, 0o7777, f)

 #ef ensure_dir(self, path):
 #ath = os.path.abspath(path)
 #f path not in self.ensured and not os.path.exists(path):
 #elf.ensured.add(path)
 #, f = os.path.split(path)
 #elf.ensure_dir(d)
 #ogger.info('Creating %s' % path)
 #f not self.dry_run:
 #s.mkdir(path)
 #f self.record:
 #elf.dirs_created.add(path)

 #ef byte_compile(self,
 #ath,
 #ptimize=False,
 #orce=False,
 #refix=None,
 #ashed_invalidation=False):
 #path = cache_from_source(path, not optimize)
 #ogger.info('Byte-compiling %s to %s', path, dpath)
 #f not self.dry_run:
 #f force or self.newer(path, dpath):
 #f not prefix:
 #iagpath = None
 #lse:
 #ssert path.startswith(prefix)
 #iagpath = path[len(prefix):]
 #ompile_kwargs = {}
 #f hashed_invalidation and hasattr(py_compile,
 #PycInvalidationMode'):
 #ompile_kwargs[
 #invalidation_mode'] = py_compile.PycInvalidationMode.CHECKED_HASH
 #y_compile.compile(path, dpath, diagpath, True,
 #*compile_kwargs)  # raise error
 #elf.record_as_written(dpath)
 #eturn dpath

 #ef ensure_removed(self, path):
 #f os.path.exists(path):
 #f os.path.isdir(path) and not os.path.islink(path):
 #ogger.debug('Removing directory tree at %s', path)
 #f not self.dry_run:
 #hutil.rmtree(path)
 #f self.record:
 #f path in self.dirs_created:
 #elf.dirs_created.remove(path)
 #lse:
 #f os.path.islink(path):
 # = 'link'
 #lse:
 # = 'file'
 #ogger.debug('Removing %s %s', s, path)
 #f not self.dry_run:
 #s.remove(path)
 #f self.record:
 #f path in self.files_written:
 #elf.files_written.remove(path)

 #ef is_writable(self, path):
 #esult = False
 #hile not result:
 #f os.path.exists(path):
 #esult = os.access(path, os.W_OK)
 #reak
 #arent = os.path.dirname(path)
 #f parent == path:
 #reak
 #ath = parent
 #eturn result

 #ef commit(self):
 #""
 #ommit recorded changes, turn off recording, return
 #hanges.
 #""
 #ssert self.record
 #esult = self.files_written, self.dirs_created
 #elf._init_record()
 #eturn result

 #ef rollback(self):
 #f not self.dry_run:
 #or f in list(self.files_written):
 #f os.path.exists(f):
 #s.remove(f)
            # dirs should all be empty now, except perhaps for
            # __pycache__ subdirs
            # reverse so that subdirs appear before their parents
 #irs = sorted(self.dirs_created, reverse=True)
 #or d in dirs:
 #list = os.listdir(d)
 #f flist:
 #ssert flist == ['__pycache__']
 #d = os.path.join(d, flist[0])
 #s.rmdir(sd)
 #s.rmdir(d)  # should fail if non-empty
 #elf._init_record()


def resolve(module_name, dotted_path):
 #f module_name in sys.modules:
 #od = sys.modules[module_name]
 #lse:
 #od = __import__(module_name)
 #f dotted_path is None:
 #esult = mod
 #lse:
 #arts = dotted_path.split('.')
 #esult = getattr(mod, parts.pop(0))
 #or p in parts:
 #esult = getattr(result, p)
 #eturn result


class ExportEntry(object):

 #ef __init__(self, name, prefix, suffix, flags):
 #elf.name = name
 #elf.prefix = prefix
 #elf.suffix = suffix
 #elf.flags = flags

 #cached_property
 #ef value(self):
 #eturn resolve(self.prefix, self.suffix)

 #ef __repr__(self):  # pragma: no cover
 #eturn '<ExportEntry %s = %s:%s %s>' % (self.name, self.prefix,
 #elf.suffix, self.flags)

 #ef __eq__(self, other):
 #f not isinstance(other, ExportEntry):
 #esult = False
 #lse:
 #esult = (self.name == other.name and self.prefix == other.prefix
 #nd self.suffix == other.suffix
 #nd self.flags == other.flags)
 #eturn result

 #_hash__ = object.__hash__


ENTRY_RE = re.compile(
 #'''(?P<name>([^\[]\S*))
 #s*=\s*(?P<callable>(\w+)([:\.]\w+)*)
 #s*(\[\s*(?P<flags>[\w-]+(=\w+)?(,\s*\w+(=\w+)?)*)\s*\])?
 #'', re.VERBOSE)


def get_export_entry(specification):
 # = ENTRY_RE.search(specification)
 #f not m:
 #esult = None
 #f '[' in specification or ']' in specification:
 #aise DistlibException("Invalid specification "
 #'%s'" % specification)
 #lse:
 # = m.groupdict()
 #ame = d['name']
 #ath = d['callable']
 #olons = path.count(':')
 #f colons == 0:
 #refix, suffix = path, None
 #lse:
 #f colons != 1:
 #aise DistlibException("Invalid specification "
 #'%s'" % specification)
 #refix, suffix = path.split(':')
 #lags = d['flags']
 #f flags is None:
 #f '[' in specification or ']' in specification:
 #aise DistlibException("Invalid specification "
 #'%s'" % specification)
 #lags = []
 #lse:
 #lags = [f.strip() for f in flags.split(',')]
 #esult = ExportEntry(name, prefix, suffix, flags)
 #eturn result


def get_cache_base(suffix=None):
 #""
 #eturn the default base location for distlib caches. If the directory does
 #ot exist, it is created. Use the suffix provided for the base directory,
 #nd default to '.distlib' if it isn't provided.

 #n Windows, if LOCALAPPDATA is defined in the environment, then it is
 #ssumed to be a directory, and will be the parent directory of the result.
 #n POSIX, and on Windows if LOCALAPPDATA is not defined, the user's home
 #irectory - using os.expanduser('~') - will be the parent directory of
 #he result.

 #he result is just the directory '.distlib' in the parent directory as
 #etermined above, or with the name specified with ``suffix``.
 #""
 #f suffix is None:
 #uffix = '.distlib'
 #f os.name == 'nt' and 'LOCALAPPDATA' in os.environ:
 #esult = os.path.expandvars('$localappdata')
 #lse:
        # Assume posix, or old Windows
 #esult = os.path.expanduser('~')
    # we use 'isdir' instead of 'exists', because we want to
    # fail if there's a file with that name
 #f os.path.isdir(result):
 #sable = os.access(result, os.W_OK)
 #f not usable:
 #ogger.warning('Directory exists but is not writable: %s', result)
 #lse:
 #ry:
 #s.makedirs(result)
 #sable = True
 #xcept OSError:
 #ogger.warning('Unable to create %s', result, exc_info=True)
 #sable = False
 #f not usable:
 #esult = tempfile.mkdtemp()
 #ogger.warning('Default location unusable, using %s', result)
 #eturn os.path.join(result, suffix)


def path_to_cache_dir(path):
 #""
 #onvert an absolute path to a directory name for use in a cache.

 #he algorithm used is:

    #. On Windows, any ``':'`` in the drive is replaced with ``'---'``.
    #. Any occurrence of ``os.sep`` is replaced with ``'--'``.
    #. ``'.cache'`` is appended.
 #""
 #, p = os.path.splitdrive(os.path.abspath(path))
 #f d:
 # = d.replace(':', '---')
 # = p.replace(os.sep, '--')
 #eturn d + p + '.cache'


def ensure_slash(s):
 #f not s.endswith('/'):
 #eturn s + '/'
 #eturn s


def parse_credentials(netloc):
 #sername = password = None
 #f '@' in netloc:
 #refix, netloc = netloc.rsplit('@', 1)
 #f ':' not in prefix:
 #sername = prefix
 #lse:
 #sername, password = prefix.split(':', 1)
 #f username:
 #sername = unquote(username)
 #f password:
 #assword = unquote(password)
 #eturn username, password, netloc


def get_process_umask():
 #esult = os.umask(0o22)
 #s.umask(result)
 #eturn result


def is_string_sequence(seq):
 #esult = True
 # = None
 #or i, s in enumerate(seq):
 #f not isinstance(s, string_types):
 #esult = False
 #reak
 #ssert i is not None
 #eturn result


PROJECT_NAME_AND_VERSION = re.compile(
 #([a-z0-9_]+([.-][a-z_][a-z0-9_]*)*)-'
 #([a-z0-9_.+-]+)', re.I)
PYTHON_VERSION = re.compile(r'-py(\d\.?\d?)')


def split_filename(filename, project_name=None):
 #""
 #xtract name, version, python version from a filename (no extension)

 #eturn name, version, pyver or None
 #""
 #esult = None
 #yver = None
 #ilename = unquote(filename).replace(' ', '-')
 # = PYTHON_VERSION.search(filename)
 #f m:
 #yver = m.group(1)
 #ilename = filename[:m.start()]
 #f project_name and len(filename) > len(project_name) + 1:
 # = re.match(re.escape(project_name) + r'\b', filename)
 #f m:
 # = m.end()
 #esult = filename[:n], filename[n + 1:], pyver
 #f result is None:
 # = PROJECT_NAME_AND_VERSION.match(filename)
 #f m:
 #esult = m.group(1), m.group(3), pyver
 #eturn result


# Allow spaces in name because of legacy dists like "Twisted Core"
NAME_VERSION_RE = re.compile(r'(?P<name>[\w .-]+)\s*'
 #'\(\s*(?P<ver>[^\s)]+)\)$')


def parse_name_and_version(p):
 #""
 # utility method used to get name and version from a string.

 #rom e.g. a Provides-Dist value.

 #param p: A value in a form 'foo (1.0)'
 #return: The name and version as a tuple.
 #""
 # = NAME_VERSION_RE.match(p)
 #f not m:
 #aise DistlibException('Ill-formed name/version string: \'%s\'' % p)
 # = m.groupdict()
 #eturn d['name'].strip().lower(), d['ver']


def get_extras(requested, available):
 #esult = set()
 #equested = set(requested or [])
 #vailable = set(available or [])
 #f '*' in requested:
 #equested.remove('*')
 #esult |= available
 #or r in requested:
 #f r == '-':
 #esult.add(r)
 #lif r.startswith('-'):
 #nwanted = r[1:]
 #f unwanted not in available:
 #ogger.warning('undeclared extra: %s' % unwanted)
 #f unwanted in result:
 #esult.remove(unwanted)
 #lse:
 #f r not in available:
 #ogger.warning('undeclared extra: %s' % r)
 #esult.add(r)
 #eturn result


#
# Extended metadata functionality
#


def _get_external_data(url):
 #esult = {}
 #ry:
        # urlopen might fail if it runs into redirections,
        # because of Python issue #13696. Fixed in locators
        # using a custom redirect handler.
 #esp = urlopen(url)
 #eaders = resp.info()
 #t = headers.get('Content-Type')
 #f not ct.startswith('application/json'):
 #ogger.debug('Unexpected response for JSON request: %s', ct)
 #lse:
 #eader = codecs.getreader('utf-8')(resp)
            # data = reader.read().decode('utf-8')
            # result = json.loads(data)
 #esult = json.load(reader)
 #xcept Exception as e:
 #ogger.exception('Failed to get external data for %s: %s', url, e)
 #eturn result


_external_data_base_url = 'https://www.red-dove.com/pypi/projects/'


def get_project_data(name):
 #rl = '%s/%s/project.json' % (name[0].upper(), name)
 #rl = urljoin(_external_data_base_url, url)
 #esult = _get_external_data(url)
 #eturn result


def get_package_data(name, version):
 #rl = '%s/%s/package-%s.json' % (name[0].upper(), name, version)
 #rl = urljoin(_external_data_base_url, url)
 #eturn _get_external_data(url)


class Cache(object):
 #""
 # class implementing a cache for resources that need to live in the file system
 #.g. shared libraries. This class was moved from resources to here because it
 #ould be used by other modules, e.g. the wheel module.
 #""

 #ef __init__(self, base):
 #""
 #nitialise an instance.

 #param base: The base directory where the cache should be located.
 #""
        # we use 'isdir' instead of 'exists', because we want to
        # fail if there's a file with that name
 #f not os.path.isdir(base):  # pragma: no cover
 #s.makedirs(base)
 #f (os.stat(base).st_mode & 0o77) != 0:
 #ogger.warning('Directory \'%s\' is not private', base)
 #elf.base = os.path.abspath(os.path.normpath(base))

 #ef prefix_to_dir(self, prefix):
 #""
 #onverts a resource prefix to a directory name in the cache.
 #""
 #eturn path_to_cache_dir(prefix)

 #ef clear(self):
 #""
 #lear the cache.
 #""
 #ot_removed = []
 #or fn in os.listdir(self.base):
 #n = os.path.join(self.base, fn)
 #ry:
 #f os.path.islink(fn) or os.path.isfile(fn):
 #s.remove(fn)
 #lif os.path.isdir(fn):
 #hutil.rmtree(fn)
 #xcept Exception:
 #ot_removed.append(fn)
 #eturn not_removed


class EventMixin(object):
 #""
 # very simple publish/subscribe system.
 #""

 #ef __init__(self):
 #elf._subscribers = {}

 #ef add(self, event, subscriber, append=True):
 #""
 #dd a subscriber for an event.

 #param event: The name of an event.
 #param subscriber: The subscriber to be added (and called when the
 #vent is published).
 #param append: Whether to append or prepend the subscriber to an
 #xisting subscriber list for the event.
 #""
 #ubs = self._subscribers
 #f event not in subs:
 #ubs[event] = deque([subscriber])
 #lse:
 #q = subs[event]
 #f append:
 #q.append(subscriber)
 #lse:
 #q.appendleft(subscriber)

 #ef remove(self, event, subscriber):
 #""
 #emove a subscriber for an event.

 #param event: The name of an event.
 #param subscriber: The subscriber to be removed.
 #""
 #ubs = self._subscribers
 #f event not in subs:
 #aise ValueError('No subscribers: %r' % event)
 #ubs[event].remove(subscriber)

 #ef get_subscribers(self, event):
 #""
 #eturn an iterator for the subscribers for an event.
 #param event: The event to return subscribers for.
 #""
 #eturn iter(self._subscribers.get(event, ()))

 #ef publish(self, event, *args, **kwargs):
 #""
 #ublish a event and return a list of values returned by its
 #ubscribers.

 #param event: The event to publish.
 #param args: The positional arguments to pass to the event's
 #ubscribers.
 #param kwargs: The keyword arguments to pass to the event's
 #ubscribers.
 #""
 #esult = []
 #or subscriber in self.get_subscribers(event):
 #ry:
 #alue = subscriber(event, *args, **kwargs)
 #xcept Exception:
 #ogger.exception('Exception during event publication')
 #alue = None
 #esult.append(value)
 #ogger.debug('publish %s: args = %s, kwargs = %s, result = %s', event,
 #rgs, kwargs, result)
 #eturn result


#
# Simple sequencing
#
class Sequencer(object):

 #ef __init__(self):
 #elf._preds = {}
 #elf._succs = {}
 #elf._nodes = set()  # nodes with no preds/succs

 #ef add_node(self, node):
 #elf._nodes.add(node)

 #ef remove_node(self, node, edges=False):
 #f node in self._nodes:
 #elf._nodes.remove(node)
 #f edges:
 #or p in set(self._preds.get(node, ())):
 #elf.remove(p, node)
 #or s in set(self._succs.get(node, ())):
 #elf.remove(node, s)
            # Remove empties
 #or k, v in list(self._preds.items()):
 #f not v:
 #el self._preds[k]
 #or k, v in list(self._succs.items()):
 #f not v:
 #el self._succs[k]

 #ef add(self, pred, succ):
 #ssert pred != succ
 #elf._preds.setdefault(succ, set()).add(pred)
 #elf._succs.setdefault(pred, set()).add(succ)

 #ef remove(self, pred, succ):
 #ssert pred != succ
 #ry:
 #reds = self._preds[succ]
 #uccs = self._succs[pred]
 #xcept KeyError:  # pragma: no cover
 #aise ValueError('%r not a successor of anything' % succ)
 #ry:
 #reds.remove(pred)
 #uccs.remove(succ)
 #xcept KeyError:  # pragma: no cover
 #aise ValueError('%r not a successor of %r' % (succ, pred))

 #ef is_step(self, step):
 #eturn (step in self._preds or step in self._succs
 #r step in self._nodes)

 #ef get_steps(self, final):
 #f not self.is_step(final):
 #aise ValueError('Unknown: %r' % final)
 #esult = []
 #odo = []
 #een = set()
 #odo.append(final)
 #hile todo:
 #tep = todo.pop(0)
 #f step in seen:
                # if a step was already seen,
                # move it to the end (so it will appear earlier
                # when reversed on return) ... but not for the
                # final step, as that would be confusing for
                # users
 #f step != final:
 #esult.remove(step)
 #esult.append(step)
 #lse:
 #een.add(step)
 #esult.append(step)
 #reds = self._preds.get(step, ())
 #odo.extend(preds)
 #eturn reversed(result)

 #property
 #ef strong_connections(self):
        # http://en.wikipedia.org/wiki/Tarjan%27s_strongly_connected_components_algorithm
 #ndex_counter = [0]
 #tack = []
 #owlinks = {}
 #ndex = {}
 #esult = []

 #raph = self._succs

 #ef strongconnect(node):
            # set the depth index for this node to the smallest unused index
 #ndex[node] = index_counter[0]
 #owlinks[node] = index_counter[0]
 #ndex_counter[0] += 1
 #tack.append(node)

            # Consider successors
 #ry:
 #uccessors = graph[node]
 #xcept Exception:
 #uccessors = []
 #or successor in successors:
 #f successor not in lowlinks:
                    # Successor has not yet been visited
 #trongconnect(successor)
 #owlinks[node] = min(lowlinks[node], lowlinks[successor])
 #lif successor in stack:
                    # the successor is in the stack and hence in the current
                    # strongly connected component (SCC)
 #owlinks[node] = min(lowlinks[node], index[successor])

            # If `node` is a root node, pop the stack and generate an SCC
 #f lowlinks[node] == index[node]:
 #onnected_component = []

 #hile True:
 #uccessor = stack.pop()
 #onnected_component.append(successor)
 #f successor == node:
 #reak
 #omponent = tuple(connected_component)
                # storing the result
 #esult.append(component)

 #or node in graph:
 #f node not in lowlinks:
 #trongconnect(node)

 #eturn result

 #property
 #ef dot(self):
 #esult = ['digraph G {']
 #or succ in self._preds:
 #reds = self._preds[succ]
 #or pred in preds:
 #esult.append('  %s -> %s;' % (pred, succ))
 #or node in self._nodes:
 #esult.append('  %s;' % node)
 #esult.append('}')
 #eturn '\n'.join(result)


#
# Unarchiving functionality for zip, tar, tgz, tbz, whl
#

ARCHIVE_EXTENSIONS = ('.tar.gz', '.tar.bz2', '.tar', '.zip', '.tgz', '.tbz',
 #.whl')


def unarchive(archive_filename, dest_dir, format=None, check=True):

 #ef check_path(path):
 #f not isinstance(path, text_type):
 #ath = path.decode('utf-8')
 # = os.path.abspath(os.path.join(dest_dir, path))
 #f not p.startswith(dest_dir) or p[plen] != os.sep:
 #aise ValueError('path outside destination: %r' % p)

 #est_dir = os.path.abspath(dest_dir)
 #len = len(dest_dir)
 #rchive = None
 #f format is None:
 #f archive_filename.endswith(('.zip', '.whl')):
 #ormat = 'zip'
 #lif archive_filename.endswith(('.tar.gz', '.tgz')):
 #ormat = 'tgz'
 #ode = 'r:gz'
 #lif archive_filename.endswith(('.tar.bz2', '.tbz')):
 #ormat = 'tbz'
 #ode = 'r:bz2'
 #lif archive_filename.endswith('.tar'):
 #ormat = 'tar'
 #ode = 'r'
 #lse:  # pragma: no cover
 #aise ValueError('Unknown format for %r' % archive_filename)
 #ry:
 #f format == 'zip':
 #rchive = ZipFile(archive_filename, 'r')
 #f check:
 #ames = archive.namelist()
 #or name in names:
 #heck_path(name)
 #lse:
 #rchive = tarfile.open(archive_filename, mode)
 #f check:
 #ames = archive.getnames()
 #or name in names:
 #heck_path(name)
 #f format != 'zip' and sys.version_info[0] < 3:
            # See Python issue 17153. If the dest path contains Unicode,
            # tarfile extraction fails on Python 2.x if a member path name
            # contains non-ASCII characters - it leads to an implicit
            # bytes -> unicode conversion using ASCII to decode.
 #or tarinfo in archive.getmembers():
 #f not isinstance(tarinfo.name, text_type):
 #arinfo.name = tarinfo.name.decode('utf-8')

        # Limit extraction of dangerous items, if this Python
        # allows it easily. If not, just trust the input.
        # See: https://docs.python.org/3/library/tarfile.html#extraction-filters
 #ef extraction_filter(member, path):
 #""Run tarfile.tar_filter, but raise the expected ValueError"""
            # This is only called if the current Python has tarfile filters
 #ry:
 #eturn tarfile.tar_filter(member, path)
 #xcept tarfile.FilterError as exc:
 #aise ValueError(str(exc))

 #rchive.extraction_filter = extraction_filter

 #rchive.extractall(dest_dir)

 #inally:
 #f archive:
 #rchive.close()


def zip_dir(directory):
 #""zip a directory tree into a BytesIO object"""
 #esult = io.BytesIO()
 #len = len(directory)
 #ith ZipFile(result, "w") as zf:
 #or root, dirs, files in os.walk(directory):
 #or name in files:
 #ull = os.path.join(root, name)
 #el = root[dlen:]
 #est = os.path.join(rel, name)
 #f.write(full, dest)
 #eturn result


#
# Simple progress bar
#

UNITS = ('', 'K', 'M', 'G', 'T', 'P')


class Progress(object):
 #nknown = 'UNKNOWN'

 #ef __init__(self, minval=0, maxval=100):
 #ssert maxval is None or maxval >= minval
 #elf.min = self.cur = minval
 #elf.max = maxval
 #elf.started = None
 #elf.elapsed = 0
 #elf.done = False

 #ef update(self, curval):
 #ssert self.min <= curval
 #ssert self.max is None or curval <= self.max
 #elf.cur = curval
 #ow = time.time()
 #f self.started is None:
 #elf.started = now
 #lse:
 #elf.elapsed = now - self.started

 #ef increment(self, incr):
 #ssert incr >= 0
 #elf.update(self.cur + incr)

 #ef start(self):
 #elf.update(self.min)
 #eturn self

 #ef stop(self):
 #f self.max is not None:
 #elf.update(self.max)
 #elf.done = True

 #property
 #ef maximum(self):
 #eturn self.unknown if self.max is None else self.max

 #property
 #ef percentage(self):
 #f self.done:
 #esult = '100 %'
 #lif self.max is None:
 #esult = ' ?? %'
 #lse:
 # = 100.0 * (self.cur - self.min) / (self.max - self.min)
 #esult = '%3d %%' % v
 #eturn result

 #ef format_duration(self, duration):
 #f (duration <= 0) and self.max is None or self.cur == self.min:
 #esult = '??:??:??'
        # elif duration < 1:
        #     result = '--:--:--'
 #lse:
 #esult = time.strftime('%H:%M:%S', time.gmtime(duration))
 #eturn result

 #property
 #ef ETA(self):
 #f self.done:
 #refix = 'Done'
 # = self.elapsed
            # import pdb; pdb.set_trace()
 #lse:
 #refix = 'ETA '
 #f self.max is None:
 # = -1
 #lif self.elapsed == 0 or (self.cur == self.min):
 # = 0
 #lse:
                # import pdb; pdb.set_trace()
 # = float(self.max - self.min)
 # /= self.cur - self.min
 # = (t - 1) * self.elapsed
 #eturn '%s: %s' % (prefix, self.format_duration(t))

 #property
 #ef speed(self):
 #f self.elapsed == 0:
 #esult = 0.0
 #lse:
 #esult = (self.cur - self.min) / self.elapsed
 #or unit in UNITS:
 #f result < 1000:
 #reak
 #esult /= 1000.0
 #eturn '%d %sB/s' % (result, unit)


#
# Glob functionality
#

RICH_GLOB = re.compile(r'\{([^}]*)\}')
_CHECK_RECURSIVE_GLOB = re.compile(r'[^/\\,{]\*\*|\*\*[^/\\,}]')
_CHECK_MISMATCH_SET = re.compile(r'^[^{]*\}|\{[^}]*$')


def iglob(path_glob):
 #""Extended globbing function that supports ** and {opt1,opt2,opt3}."""
 #f _CHECK_RECURSIVE_GLOB.search(path_glob):
 #sg = """invalid glob %r: recursive glob "**" must be used alone"""
 #aise ValueError(msg % path_glob)
 #f _CHECK_MISMATCH_SET.search(path_glob):
 #sg = """invalid glob %r: mismatching set marker '{' or '}'"""
 #aise ValueError(msg % path_glob)
 #eturn _iglob(path_glob)


def _iglob(path_glob):
 #ich_path_glob = RICH_GLOB.split(path_glob, 1)
 #f len(rich_path_glob) > 1:
 #ssert len(rich_path_glob) == 3, rich_path_glob
 #refix, set, suffix = rich_path_glob
 #or item in set.split(','):
 #or path in _iglob(''.join((prefix, item, suffix))):
 #ield path
 #lse:
 #f '**' not in path_glob:
 #or item in std_iglob(path_glob):
 #ield item
 #lse:
 #refix, radical = path_glob.split('**', 1)
 #f prefix == '':
 #refix = '.'
 #f radical == '':
 #adical = '*'
 #lse:
                # we support both
 #adical = radical.lstrip('/')
 #adical = radical.lstrip('\\')
 #or path, dir, files in os.walk(prefix):
 #ath = os.path.normpath(path)
 #or fn in _iglob(os.path.join(path, radical)):
 #ield fn


if ssl:
 #rom .compat import (HTTPSHandler as BaseHTTPSHandler, match_hostname,
 #ertificateError)

    #
    # HTTPSConnection which verifies certificates/matches domains
    #

 #lass HTTPSConnection(httplib.HTTPSConnection):
 #a_certs = None  # set this to the path to the certs file (.pem)
 #heck_domain = True  # only used if ca_certs is not None

        # noinspection PyPropertyAccess
 #ef connect(self):
 #ock = socket.create_connection((self.host, self.port),
 #elf.timeout)
 #f getattr(self, '_tunnel_host', False):
 #elf.sock = sock
 #elf._tunnel()

 #ontext = ssl.SSLContext(ssl.PROTOCOL_SSLv23)
 #f hasattr(ssl, 'OP_NO_SSLv2'):
 #ontext.options |= ssl.OP_NO_SSLv2
 #f getattr(self, 'cert_file', None):
 #ontext.load_cert_chain(self.cert_file, self.key_file)
 #wargs = {}
 #f self.ca_certs:
 #ontext.verify_mode = ssl.CERT_REQUIRED
 #ontext.load_verify_locations(cafile=self.ca_certs)
 #f getattr(ssl, 'HAS_SNI', False):
 #wargs['server_hostname'] = self.host

 #elf.sock = context.wrap_socket(sock, **kwargs)
 #f self.ca_certs and self.check_domain:
 #ry:
 #atch_hostname(self.sock.getpeercert(), self.host)
 #ogger.debug('Host verified: %s', self.host)
 #xcept CertificateError:  # pragma: no cover
 #elf.sock.shutdown(socket.SHUT_RDWR)
 #elf.sock.close()
 #aise

 #lass HTTPSHandler(BaseHTTPSHandler):

 #ef __init__(self, ca_certs, check_domain=True):
 #aseHTTPSHandler.__init__(self)
 #elf.ca_certs = ca_certs
 #elf.check_domain = check_domain

 #ef _conn_maker(self, *args, **kwargs):
 #""
 #his is called to create a connection instance. Normally you'd
 #ass a connection class to do_open, but it doesn't actually check for
 # class, and just expects a callable. As long as we behave just as a
 #onstructor would have, we should be OK. If it ever changes so that
 #e *must* pass a class, we'll create an UnsafeHTTPSConnection class
 #hich just sets check_domain to False in the class definition, and
 #hoose which one to pass to do_open.
 #""
 #esult = HTTPSConnection(*args, **kwargs)
 #f self.ca_certs:
 #esult.ca_certs = self.ca_certs
 #esult.check_domain = self.check_domain
 #eturn result

 #ef https_open(self, req):
 #ry:
 #eturn self.do_open(self._conn_maker, req)
 #xcept URLError as e:
 #f 'certificate verify failed' in str(e.reason):
 #aise CertificateError(
 #Unable to verify server certificate '
 #for %s' % req.host)
 #lse:
 #aise

    #
    # To prevent against mixing HTTP traffic with HTTPS (examples: A Man-In-The-
    # Middle proxy using HTTP listens on port 443, or an index mistakenly serves
    # HTML containing a http://xyz link when it should be https://xyz),
    # you can use the following handler class, which does not allow HTTP traffic.
    #
    # It works by inheriting from HTTPHandler - so build_opener won't add a
    # handler for HTTP itself.
    #
 #lass HTTPSOnlyHandler(HTTPSHandler, HTTPHandler):

 #ef http_open(self, req):
 #aise URLError(
 #Unexpected HTTP request on what should be a secure '
 #connection: %s' % req)


#
# XML-RPC with timeouts
#
class Transport(xmlrpclib.Transport):

 #ef __init__(self, timeout, use_datetime=0):
 #elf.timeout = timeout
 #mlrpclib.Transport.__init__(self, use_datetime)

 #ef make_connection(self, host):
 #, eh, x509 = self.get_host_info(host)
 #f not self._connection or host != self._connection[0]:
 #elf._extra_headers = eh
 #elf._connection = host, httplib.HTTPConnection(h)
 #eturn self._connection[1]


if ssl:

 #lass SafeTransport(xmlrpclib.SafeTransport):

 #ef __init__(self, timeout, use_datetime=0):
 #elf.timeout = timeout
 #mlrpclib.SafeTransport.__init__(self, use_datetime)

 #ef make_connection(self, host):
 #, eh, kwargs = self.get_host_info(host)
 #f not kwargs:
 #wargs = {}
 #wargs['timeout'] = self.timeout
 #f not self._connection or host != self._connection[0]:
 #elf._extra_headers = eh
 #elf._connection = host, httplib.HTTPSConnection(
 #, None, **kwargs)
 #eturn self._connection[1]


class ServerProxy(xmlrpclib.ServerProxy):

 #ef __init__(self, uri, **kwargs):
 #elf.timeout = timeout = kwargs.pop('timeout', None)
        # The above classes only come into play if a timeout
        # is specified
 #f timeout is not None:
            # scheme = splittype(uri)  # deprecated as of Python 3.8
 #cheme = urlparse(uri)[0]
 #se_datetime = kwargs.get('use_datetime', 0)
 #f scheme == 'https':
 #cls = SafeTransport
 #lse:
 #cls = Transport
 #wargs['transport'] = t = tcls(timeout, use_datetime=use_datetime)
 #elf.transport = t
 #mlrpclib.ServerProxy.__init__(self, uri, **kwargs)


#
# CSV functionality. This is provided because on 2.x, the csv module can't
# handle Unicode. However, we need to deal with Unicode in e.g. RECORD files.
#


def _csv_open(fn, mode, **kwargs):
 #f sys.version_info[0] < 3:
 #ode += 'b'
 #lse:
 #wargs['newline'] = ''
        # Python 3 determines encoding from locale. Force 'utf-8'
        # file encoding to match other forced utf-8 encoding
 #wargs['encoding'] = 'utf-8'
 #eturn open(fn, mode, **kwargs)


class CSVBase(object):
 #efaults = {
 #delimiter': str(','),  # The strs are used because we need native
 #quotechar': str('"'),  # str in the csv API (2.x won't take
 #lineterminator': str('\n')  # Unicode)
 #

 #ef __enter__(self):
 #eturn self

 #ef __exit__(self, *exc_info):
 #elf.stream.close()


class CSVReader(CSVBase):

 #ef __init__(self, **kwargs):
 #f 'stream' in kwargs:
 #tream = kwargs['stream']
 #f sys.version_info[0] >= 3:
                # needs to be a text stream
 #tream = codecs.getreader('utf-8')(stream)
 #elf.stream = stream
 #lse:
 #elf.stream = _csv_open(kwargs['path'], 'r')
 #elf.reader = csv.reader(self.stream, **self.defaults)

 #ef __iter__(self):
 #eturn self

 #ef next(self):
 #esult = next(self.reader)
 #f sys.version_info[0] < 3:
 #or i, item in enumerate(result):
 #f not isinstance(item, text_type):
 #esult[i] = item.decode('utf-8')
 #eturn result

 #_next__ = next


class CSVWriter(CSVBase):

 #ef __init__(self, fn, **kwargs):
 #elf.stream = _csv_open(fn, 'w')
 #elf.writer = csv.writer(self.stream, **self.defaults)

 #ef writerow(self, row):
 #f sys.version_info[0] < 3:
 # = []
 #or item in row:
 #f isinstance(item, text_type):
 #tem = item.encode('utf-8')
 #.append(item)
 #ow = r
 #elf.writer.writerow(row)


#
#   Configurator functionality
#


class Configurator(BaseConfigurator):

 #alue_converters = dict(BaseConfigurator.value_converters)
 #alue_converters['inc'] = 'inc_convert'

 #ef __init__(self, config, base=None):
 #uper(Configurator, self).__init__(config)
 #elf.base = base or os.getcwd()

 #ef configure_custom(self, config):

 #ef convert(o):
 #f isinstance(o, (list, tuple)):
 #esult = type(o)([convert(i) for i in o])
 #lif isinstance(o, dict):
 #f '()' in o:
 #esult = self.configure_custom(o)
 #lse:
 #esult = {}
 #or k in o:
 #esult[k] = convert(o[k])
 #lse:
 #esult = self.convert(o)
 #eturn result

 # = config.pop('()')
 #f not callable(c):
 # = self.resolve(c)
 #rops = config.pop('.', None)
        # Check for valid identifiers
 #rgs = config.pop('[]', ())
 #f args:
 #rgs = tuple([convert(o) for o in args])
 #tems = [(k, convert(config[k])) for k in config if valid_ident(k)]
 #wargs = dict(items)
 #esult = c(*args, **kwargs)
 #f props:
 #or n, v in props.items():
 #etattr(result, n, convert(v))
 #eturn result

 #ef __getitem__(self, key):
 #esult = self.config[key]
 #f isinstance(result, dict) and '()' in result:
 #elf.config[key] = result = self.configure_custom(result)
 #eturn result

 #ef inc_convert(self, value):
 #""Default converter for the inc:// protocol."""
 #f not os.path.isabs(value):
 #alue = os.path.join(self.base, value)
 #ith codecs.open(value, 'r', encoding='utf-8') as f:
 #esult = json.load(f)
 #eturn result


class SubprocessMixin(object):
 #""
 #ixin for running subprocesses and capturing their output
 #""

 #ef __init__(self, verbose=False, progress=None):
 #elf.verbose = verbose
 #elf.progress = progress

 #ef reader(self, stream, context):
 #""
 #ead lines from a subprocess' output stream and either pass to a progress
 #allable (if specified) or write progress information to sys.stderr.
 #""
 #rogress = self.progress
 #erbose = self.verbose
 #hile True:
 # = stream.readline()
 #f not s:
 #reak
 #f progress is not None:
 #rogress(s, context)
 #lse:
 #f not verbose:
 #ys.stderr.write('.')
 #lse:
 #ys.stderr.write(s.decode('utf-8'))
 #ys.stderr.flush()
 #tream.close()

 #ef run_command(self, cmd, **kwargs):
 # = subprocess.Popen(cmd,
 #tdout=subprocess.PIPE,
 #tderr=subprocess.PIPE,
 #*kwargs)
 #1 = threading.Thread(target=self.reader, args=(p.stdout, 'stdout'))
 #1.start()
 #2 = threading.Thread(target=self.reader, args=(p.stderr, 'stderr'))
 #2.start()
 #.wait()
 #1.join()
 #2.join()
 #f self.progress is not None:
 #elf.progress('done.', 'main')
 #lif self.verbose:
 #ys.stderr.write('done.\n')
 #eturn p


def normalize_name(name):
 #""Normalize a python package name a la PEP 503"""
    # https://www.python.org/dev/peps/pep-0503/#normalized-names
 #eturn re.sub('[-_.]+', '-', name).lower()


# def _get_pypirc_command():
# """
# Get the distutils command for interacting with PyPI configurations.
# :return: the command.
# """
# from distutils.core import Distribution
# from distutils.config import PyPIRCCommand
# d = Distribution()
# return PyPIRCCommand(d)


class PyPIRCFile(object):

 #EFAULT_REPOSITORY = 'https://upload.pypi.org/legacy/'
 #EFAULT_REALM = 'pypi'

 #ef __init__(self, fn=None, url=None):
 #f fn is None:
 #n = os.path.join(os.path.expanduser('~'), '.pypirc')
 #elf.filename = fn
 #elf.url = url

 #ef read(self):
 #esult = {}

 #f os.path.exists(self.filename):
 #epository = self.url or self.DEFAULT_REPOSITORY

 #onfig = configparser.RawConfigParser()
 #onfig.read(self.filename)
 #ections = config.sections()
 #f 'distutils' in sections:
                # let's get the list of servers
 #ndex_servers = config.get('distutils', 'index-servers')
 #servers = [
 #erver.strip() for server in index_servers.split('\n')
 #f server.strip() != ''
 #
 #f _servers == []:
                    # nothing set, let's try to get the default pypi
 #f 'pypi' in sections:
 #servers = ['pypi']
 #lse:
 #or server in _servers:
 #esult = {'server': server}
 #esult['username'] = config.get(server, 'username')

                        # optional params
 #or key, default in (('repository',
 #elf.DEFAULT_REPOSITORY),
 #'realm', self.DEFAULT_REALM),
 #'password', None)):
 #f config.has_option(server, key):
 #esult[key] = config.get(server, key)
 #lse:
 #esult[key] = default

                        # work around people having "repository" for the "pypi"
                        # section of their config set to the HTTP (rather than
                        # HTTPS) URL
 #f (server == 'pypi' and repository
 #n (self.DEFAULT_REPOSITORY, 'pypi')):
 #esult['repository'] = self.DEFAULT_REPOSITORY
 #lif (result['server'] != repository
 #nd result['repository'] != repository):
 #esult = {}
 #lif 'server-login' in sections:
                # old format
 #erver = 'server-login'
 #f config.has_option(server, 'repository'):
 #epository = config.get(server, 'repository')
 #lse:
 #epository = self.DEFAULT_REPOSITORY
 #esult = {
 #username': config.get(server, 'username'),
 #password': config.get(server, 'password'),
 #repository': repository,
 #server': server,
 #realm': self.DEFAULT_REALM
 #
 #eturn result

 #ef update(self, username, password):
        # import pdb; pdb.set_trace()
 #onfig = configparser.RawConfigParser()
 #n = self.filename
 #onfig.read(fn)
 #f not config.has_section('pypi'):
 #onfig.add_section('pypi')
 #onfig.set('pypi', 'username', username)
 #onfig.set('pypi', 'password', password)
 #ith open(fn, 'w') as f:
 #onfig.write(f)


def _load_pypirc(index):
 #""
 #ead the PyPI access configuration as supported by distutils.
 #""
 #eturn PyPIRCFile(url=index.url).read()


def _store_pypirc(index):
 #yPIRCFile().update(index.username, index.password)


#
# get_platform()/get_host_platform() copied from Python 3.10.a0 source, with some minor
# tweaks
#


def get_host_platform():
 #""Return a string that identifies the current platform.  This is used mainly to
 #istinguish platform-specific build directories and platform-specific built
 #istributions.  Typically includes the OS name and version and the
 #rchitecture (as supplied by 'os.uname()'), although the exact information
 #ncluded depends on the OS; eg. on Linux, the kernel version isn't
 #articularly important.

 #xamples of returned values:
 #inux-i586
 #inux-alpha (?)
 #olaris-2.6-sun4u

 #indows will return one of:
 #in-amd64 (64bit Windows on AMD64 (aka x86_64, Intel64, EM64T, etc)
 #in32 (all others - specifically, sys.platform is returned)

 #or other non-POSIX platforms, currently just returns 'sys.platform'.

 #""
 #f os.name == 'nt':
 #f 'amd64' in sys.version.lower():
 #eturn 'win-amd64'
 #f '(arm)' in sys.version.lower():
 #eturn 'win-arm32'
 #f '(arm64)' in sys.version.lower():
 #eturn 'win-arm64'
 #eturn sys.platform

    # Set for cross builds explicitly
 #f "_PYTHON_HOST_PLATFORM" in os.environ:
 #eturn os.environ["_PYTHON_HOST_PLATFORM"]

 #f os.name != 'posix' or not hasattr(os, 'uname'):
        # XXX what about the architecture? NT is Intel or Alpha,
        # Mac OS is M68k or PPC, etc.
 #eturn sys.platform

    # Try to distinguish various flavours of Unix

 #osname, host, release, version, machine) = os.uname()

    # Convert the OS name to lowercase, remove '/' characters, and translate
    # spaces (for "Power Macintosh")
 #sname = osname.lower().replace('/', '')
 #achine = machine.replace(' ', '_').replace('/', '-')

 #f osname[:5] == 'linux':
        # At least on Linux/Intel, 'machine' is the processor --
        # i386, etc.
        # XXX what about Alpha, SPARC, etc?
 #eturn "%s-%s" % (osname, machine)

 #lif osname[:5] == 'sunos':
 #f release[0] >= '5':  # SunOS 5 == Solaris 2
 #sname = 'solaris'
 #elease = '%d.%s' % (int(release[0]) - 3, release[2:])
            # We can't use 'platform.architecture()[0]' because a
            # bootstrap problem. We use a dict to get an error
            # if some suspicious happens.
 #itness = {2147483647: '32bit', 9223372036854775807: '64bit'}
 #achine += '.%s' % bitness[sys.maxsize]
        # fall through to standard osname-release-machine representation
 #lif osname[:3] == 'aix':
 #rom _aix_support import aix_platform
 #eturn aix_platform()
 #lif osname[:6] == 'cygwin':
 #sname = 'cygwin'
 #el_re = re.compile(r'[\d.]+', re.ASCII)
 # = rel_re.match(release)
 #f m:
 #elease = m.group()
 #lif osname[:6] == 'darwin':
 #mport _osx_support
 #ry:
 #rom distutils import sysconfig
 #xcept ImportError:
 #mport sysconfig
 #sname, release, machine = _osx_support.get_platform_osx(
 #ysconfig.get_config_vars(), osname, release, machine)

 #eturn '%s-%s-%s' % (osname, release, machine)


_TARGET_TO_PLAT = {
 #x86': 'win32',
 #x64': 'win-amd64',
 #arm': 'win-arm32',
}


def get_platform():
 #f os.name != 'nt':
 #eturn get_host_platform()
 #ross_compilation_target = os.environ.get('VSCMD_ARG_TGT_ARCH')
 #f cross_compilation_target not in _TARGET_TO_PLAT:
 #eturn get_host_platform()
 #eturn _TARGET_TO_PLAT[cross_compilation_target]
