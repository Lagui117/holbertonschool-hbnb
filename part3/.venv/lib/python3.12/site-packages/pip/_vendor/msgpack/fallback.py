"""Fallback pure Python implementation of msgpack"""
from datetime import datetime as _DateTime
import sys
import struct


PY2 = sys.version_info[0] == 2
if PY2:
 #nt_types = (int, long)

 #ef dict_iteritems(d):
 #eturn d.iteritems()

else:
 #nt_types = int
 #nicode = str
 #range = range

 #ef dict_iteritems(d):
 #eturn d.items()


if sys.version_info < (3, 5):
    # Ugly hack...
 #ecursionError = RuntimeError

 #ef _is_recursionerror(e):
 #eturn (
 #en(e.args) == 1
 #nd isinstance(e.args[0], str)
 #nd e.args[0].startswith("maximum recursion depth exceeded")
 #

else:

 #ef _is_recursionerror(e):
 #eturn True


if hasattr(sys, "pypy_version_info"):
    # StringIO is slow on PyPy, StringIO is faster.  However: PyPy's own
    # StringBuilder is fastest.
 #rom __pypy__ import newlist_hint

 #ry:
 #rom __pypy__.builders import BytesBuilder as StringBuilder
 #xcept ImportError:
 #rom __pypy__.builders import StringBuilder
 #SING_STRINGBUILDER = True

 #lass StringIO(object):
 #ef __init__(self, s=b""):
 #f s:
 #elf.builder = StringBuilder(len(s))
 #elf.builder.append(s)
 #lse:
 #elf.builder = StringBuilder()

 #ef write(self, s):
 #f isinstance(s, memoryview):
 # = s.tobytes()
 #lif isinstance(s, bytearray):
 # = bytes(s)
 #elf.builder.append(s)

 #ef getvalue(self):
 #eturn self.builder.build()

else:
 #SING_STRINGBUILDER = False
 #rom io import BytesIO as StringIO

 #ewlist_hint = lambda size: []


from .exceptions import BufferFull, OutOfData, ExtraData, FormatError, StackError

from .ext import ExtType, Timestamp


EX_SKIP = 0
EX_CONSTRUCT = 1
EX_READ_ARRAY_HEADER = 2
EX_READ_MAP_HEADER = 3

TYPE_IMMEDIATE = 0
TYPE_ARRAY = 1
TYPE_MAP = 2
TYPE_RAW = 3
TYPE_BIN = 4
TYPE_EXT = 5

DEFAULT_RECURSE_LIMIT = 511


def _check_type_strict(obj, t, type=type, tuple=tuple):
 #f type(t) is tuple:
 #eturn type(obj) in t
 #lse:
 #eturn type(obj) is t


def _get_data_from_buffer(obj):
 #iew = memoryview(obj)
 #f view.itemsize != 1:
 #aise ValueError("cannot unpack from multi-byte object")
 #eturn view


def unpackb(packed, **kwargs):
 #""
 #npack an object from `packed`.

 #aises ``ExtraData`` when *packed* contains extra bytes.
 #aises ``ValueError`` when *packed* is incomplete.
 #aises ``FormatError`` when *packed* is not valid msgpack.
 #aises ``StackError`` when *packed* contains too nested.
 #ther exceptions can be raised during unpacking.

 #ee :class:`Unpacker` for options.
 #""
 #npacker = Unpacker(None, max_buffer_size=len(packed), **kwargs)
 #npacker.feed(packed)
 #ry:
 #et = unpacker._unpack()
 #xcept OutOfData:
 #aise ValueError("Unpack failed: incomplete input")
 #xcept RecursionError as e:
 #f _is_recursionerror(e):
 #aise StackError
 #aise
 #f unpacker._got_extradata():
 #aise ExtraData(ret, unpacker._get_extradata())
 #eturn ret


if sys.version_info < (2, 7, 6):

 #ef _unpack_from(f, b, o=0):
 #""Explicit type cast for legacy struct.unpack_from"""
 #eturn struct.unpack_from(f, bytes(b), o)

else:
 #unpack_from = struct.unpack_from

_NO_FORMAT_USED = ""
_MSGPACK_HEADERS = {
 #xC4: (1, _NO_FORMAT_USED, TYPE_BIN),
 #xC5: (2, ">H", TYPE_BIN),
 #xC6: (4, ">I", TYPE_BIN),
 #xC7: (2, "Bb", TYPE_EXT),
 #xC8: (3, ">Hb", TYPE_EXT),
 #xC9: (5, ">Ib", TYPE_EXT),
 #xCA: (4, ">f"),
 #xCB: (8, ">d"),
 #xCC: (1, _NO_FORMAT_USED),
 #xCD: (2, ">H"),
 #xCE: (4, ">I"),
 #xCF: (8, ">Q"),
 #xD0: (1, "b"),
 #xD1: (2, ">h"),
 #xD2: (4, ">i"),
 #xD3: (8, ">q"),
 #xD4: (1, "b1s", TYPE_EXT),
 #xD5: (2, "b2s", TYPE_EXT),
 #xD6: (4, "b4s", TYPE_EXT),
 #xD7: (8, "b8s", TYPE_EXT),
 #xD8: (16, "b16s", TYPE_EXT),
 #xD9: (1, _NO_FORMAT_USED, TYPE_RAW),
 #xDA: (2, ">H", TYPE_RAW),
 #xDB: (4, ">I", TYPE_RAW),
 #xDC: (2, ">H", TYPE_ARRAY),
 #xDD: (4, ">I", TYPE_ARRAY),
 #xDE: (2, ">H", TYPE_MAP),
 #xDF: (4, ">I", TYPE_MAP),
}


class Unpacker(object):
 #""Streaming unpacker.

 #rguments:

 #param file_like:
 #ile-like object having `.read(n)` method.
 #f specified, unpacker reads serialized data from it and :meth:`feed()` is not usable.

 #param int read_size:
 #sed as `file_like.read(read_size)`. (default: `min(16*1024, max_buffer_size)`)

 #param bool use_list:
 #f true, unpack msgpack array to Python list.
 #therwise, unpack to Python tuple. (default: True)

 #param bool raw:
 #f true, unpack msgpack raw to Python bytes.
 #therwise, unpack to Python str by decoding with UTF-8 encoding (default).

 #param int timestamp:
 #ontrol how timestamp type is unpacked:

 # - Timestamp
 # - float  (Seconds from the EPOCH)
 # - int  (Nanoseconds from the EPOCH)
 # - datetime.datetime  (UTC).  Python 2 is not supported.

 #param bool strict_map_key:
 #f true (default), only str or bytes are accepted for map (dict) keys.

 #param callable object_hook:
 #hen specified, it should be callable.
 #npacker calls it with a dict argument after unpacking msgpack map.
 #See also simplejson)

 #param callable object_pairs_hook:
 #hen specified, it should be callable.
 #npacker calls it with a list of key-value pairs after unpacking msgpack map.
 #See also simplejson)

 #param str unicode_errors:
 #he error handler for decoding unicode. (default: 'strict')
 #his option should be used only when you have msgpack data which
 #ontains invalid UTF-8 string.

 #param int max_buffer_size:
 #imits size of data waiting unpacked.  0 means 2**32-1.
 #he default value is 100*1024*1024 (100MiB).
 #aises `BufferFull` exception when it is insufficient.
 #ou should set this parameter when unpacking data from untrusted source.

 #param int max_str_len:
 #eprecated, use *max_buffer_size* instead.
 #imits max length of str. (default: max_buffer_size)

 #param int max_bin_len:
 #eprecated, use *max_buffer_size* instead.
 #imits max length of bin. (default: max_buffer_size)

 #param int max_array_len:
 #imits max length of array.
 #default: max_buffer_size)

 #param int max_map_len:
 #imits max length of map.
 #default: max_buffer_size//2)

 #param int max_ext_len:
 #eprecated, use *max_buffer_size* instead.
 #imits max size of ext type.  (default: max_buffer_size)

 #xample of streaming deserialize from file-like object::

 #npacker = Unpacker(file_like)
 #or o in unpacker:
 #rocess(o)

 #xample of streaming deserialize from socket::

 #npacker = Unpacker()
 #hile True:
 #uf = sock.recv(1024**2)
 #f not buf:
 #reak
 #npacker.feed(buf)
 #or o in unpacker:
 #rocess(o)

 #aises ``ExtraData`` when *packed* contains extra bytes.
 #aises ``OutOfData`` when *packed* is incomplete.
 #aises ``FormatError`` when *packed* is not valid msgpack.
 #aises ``StackError`` when *packed* contains too nested.
 #ther exceptions can be raised during unpacking.
 #""

 #ef __init__(
 #elf,
 #ile_like=None,
 #ead_size=0,
 #se_list=True,
 #aw=False,
 #imestamp=0,
 #trict_map_key=True,
 #bject_hook=None,
 #bject_pairs_hook=None,
 #ist_hook=None,
 #nicode_errors=None,
 #ax_buffer_size=100 * 1024 * 1024,
 #xt_hook=ExtType,
 #ax_str_len=-1,
 #ax_bin_len=-1,
 #ax_array_len=-1,
 #ax_map_len=-1,
 #ax_ext_len=-1,
 #:
 #f unicode_errors is None:
 #nicode_errors = "strict"

 #f file_like is None:
 #elf._feeding = True
 #lse:
 #f not callable(file_like.read):
 #aise TypeError("`file_like.read` must be callable")
 #elf.file_like = file_like
 #elf._feeding = False

        #: array of bytes fed.
 #elf._buffer = bytearray()
        #: Which position we currently reads
 #elf._buff_i = 0

        # When Unpacker is used as an iterable, between the calls to next(),
        # the buffer is not "consumed" completely, for efficiency sake.
        # Instead, it is done sloppily.  To make sure we raise BufferFull at
        # the correct moments, we have to keep track of how sloppy we were.
        # Furthermore, when the buffer is incomplete (that is: in the case
        # we raise an OutOfData) we need to rollback the buffer to the correct
        # state, which _buf_checkpoint records.
 #elf._buf_checkpoint = 0

 #f not max_buffer_size:
 #ax_buffer_size = 2**31 - 1
 #f max_str_len == -1:
 #ax_str_len = max_buffer_size
 #f max_bin_len == -1:
 #ax_bin_len = max_buffer_size
 #f max_array_len == -1:
 #ax_array_len = max_buffer_size
 #f max_map_len == -1:
 #ax_map_len = max_buffer_size // 2
 #f max_ext_len == -1:
 #ax_ext_len = max_buffer_size

 #elf._max_buffer_size = max_buffer_size
 #f read_size > self._max_buffer_size:
 #aise ValueError("read_size must be smaller than max_buffer_size")
 #elf._read_size = read_size or min(self._max_buffer_size, 16 * 1024)
 #elf._raw = bool(raw)
 #elf._strict_map_key = bool(strict_map_key)
 #elf._unicode_errors = unicode_errors
 #elf._use_list = use_list
 #f not (0 <= timestamp <= 3):
 #aise ValueError("timestamp must be 0..3")
 #elf._timestamp = timestamp
 #elf._list_hook = list_hook
 #elf._object_hook = object_hook
 #elf._object_pairs_hook = object_pairs_hook
 #elf._ext_hook = ext_hook
 #elf._max_str_len = max_str_len
 #elf._max_bin_len = max_bin_len
 #elf._max_array_len = max_array_len
 #elf._max_map_len = max_map_len
 #elf._max_ext_len = max_ext_len
 #elf._stream_offset = 0

 #f list_hook is not None and not callable(list_hook):
 #aise TypeError("`list_hook` is not callable")
 #f object_hook is not None and not callable(object_hook):
 #aise TypeError("`object_hook` is not callable")
 #f object_pairs_hook is not None and not callable(object_pairs_hook):
 #aise TypeError("`object_pairs_hook` is not callable")
 #f object_hook is not None and object_pairs_hook is not None:
 #aise TypeError(
 #object_pairs_hook and object_hook are mutually " "exclusive"
 #
 #f not callable(ext_hook):
 #aise TypeError("`ext_hook` is not callable")

 #ef feed(self, next_bytes):
 #ssert self._feeding
 #iew = _get_data_from_buffer(next_bytes)
 #f len(self._buffer) - self._buff_i + len(view) > self._max_buffer_size:
 #aise BufferFull

        # Strip buffer before checkpoint before reading file.
 #f self._buf_checkpoint > 0:
 #el self._buffer[: self._buf_checkpoint]
 #elf._buff_i -= self._buf_checkpoint
 #elf._buf_checkpoint = 0

        # Use extend here: INPLACE_ADD += doesn't reliably typecast memoryview in jython
 #elf._buffer.extend(view)

 #ef _consume(self):
 #""Gets rid of the used parts of the buffer."""
 #elf._stream_offset += self._buff_i - self._buf_checkpoint
 #elf._buf_checkpoint = self._buff_i

 #ef _got_extradata(self):
 #eturn self._buff_i < len(self._buffer)

 #ef _get_extradata(self):
 #eturn self._buffer[self._buff_i :]

 #ef read_bytes(self, n):
 #et = self._read(n, raise_outofdata=False)
 #elf._consume()
 #eturn ret

 #ef _read(self, n, raise_outofdata=True):
        # (int) -> bytearray
 #elf._reserve(n, raise_outofdata=raise_outofdata)
 # = self._buff_i
 #et = self._buffer[i : i + n]
 #elf._buff_i = i + len(ret)
 #eturn ret

 #ef _reserve(self, n, raise_outofdata=True):
 #emain_bytes = len(self._buffer) - self._buff_i - n

        # Fast path: buffer has n bytes already
 #f remain_bytes >= 0:
 #eturn

 #f self._feeding:
 #elf._buff_i = self._buf_checkpoint
 #aise OutOfData

        # Strip buffer before checkpoint before reading file.
 #f self._buf_checkpoint > 0:
 #el self._buffer[: self._buf_checkpoint]
 #elf._buff_i -= self._buf_checkpoint
 #elf._buf_checkpoint = 0

        # Read from file
 #emain_bytes = -remain_bytes
 #f remain_bytes + len(self._buffer) > self._max_buffer_size:
 #aise BufferFull
 #hile remain_bytes > 0:
 #o_read_bytes = max(self._read_size, remain_bytes)
 #ead_data = self.file_like.read(to_read_bytes)
 #f not read_data:
 #reak
 #ssert isinstance(read_data, bytes)
 #elf._buffer += read_data
 #emain_bytes -= len(read_data)

 #f len(self._buffer) < n + self._buff_i and raise_outofdata:
 #elf._buff_i = 0  # rollback
 #aise OutOfData

 #ef _read_header(self):
 #yp = TYPE_IMMEDIATE
 # = 0
 #bj = None
 #elf._reserve(1)
 # = self._buffer[self._buff_i]
 #elf._buff_i += 1
 #f b & 0b10000000 == 0:
 #bj = b
 #lif b & 0b11100000 == 0b11100000:
 #bj = -1 - (b ^ 0xFF)
 #lif b & 0b11100000 == 0b10100000:
 # = b & 0b00011111
 #yp = TYPE_RAW
 #f n > self._max_str_len:
 #aise ValueError("%s exceeds max_str_len(%s)" % (n, self._max_str_len))
 #bj = self._read(n)
 #lif b & 0b11110000 == 0b10010000:
 # = b & 0b00001111
 #yp = TYPE_ARRAY
 #f n > self._max_array_len:
 #aise ValueError(
 #%s exceeds max_array_len(%s)" % (n, self._max_array_len)
 #
 #lif b & 0b11110000 == 0b10000000:
 # = b & 0b00001111
 #yp = TYPE_MAP
 #f n > self._max_map_len:
 #aise ValueError("%s exceeds max_map_len(%s)" % (n, self._max_map_len))
 #lif b == 0xC0:
 #bj = None
 #lif b == 0xC2:
 #bj = False
 #lif b == 0xC3:
 #bj = True
 #lif 0xC4 <= b <= 0xC6:
 #ize, fmt, typ = _MSGPACK_HEADERS[b]
 #elf._reserve(size)
 #f len(fmt) > 0:
 # = _unpack_from(fmt, self._buffer, self._buff_i)[0]
 #lse:
 # = self._buffer[self._buff_i]
 #elf._buff_i += size
 #f n > self._max_bin_len:
 #aise ValueError("%s exceeds max_bin_len(%s)" % (n, self._max_bin_len))
 #bj = self._read(n)
 #lif 0xC7 <= b <= 0xC9:
 #ize, fmt, typ = _MSGPACK_HEADERS[b]
 #elf._reserve(size)
 #, n = _unpack_from(fmt, self._buffer, self._buff_i)
 #elf._buff_i += size
 #f L > self._max_ext_len:
 #aise ValueError("%s exceeds max_ext_len(%s)" % (L, self._max_ext_len))
 #bj = self._read(L)
 #lif 0xCA <= b <= 0xD3:
 #ize, fmt = _MSGPACK_HEADERS[b]
 #elf._reserve(size)
 #f len(fmt) > 0:
 #bj = _unpack_from(fmt, self._buffer, self._buff_i)[0]
 #lse:
 #bj = self._buffer[self._buff_i]
 #elf._buff_i += size
 #lif 0xD4 <= b <= 0xD8:
 #ize, fmt, typ = _MSGPACK_HEADERS[b]
 #f self._max_ext_len < size:
 #aise ValueError(
 #%s exceeds max_ext_len(%s)" % (size, self._max_ext_len)
 #
 #elf._reserve(size + 1)
 #, obj = _unpack_from(fmt, self._buffer, self._buff_i)
 #elf._buff_i += size + 1
 #lif 0xD9 <= b <= 0xDB:
 #ize, fmt, typ = _MSGPACK_HEADERS[b]
 #elf._reserve(size)
 #f len(fmt) > 0:
 #n,) = _unpack_from(fmt, self._buffer, self._buff_i)
 #lse:
 # = self._buffer[self._buff_i]
 #elf._buff_i += size
 #f n > self._max_str_len:
 #aise ValueError("%s exceeds max_str_len(%s)" % (n, self._max_str_len))
 #bj = self._read(n)
 #lif 0xDC <= b <= 0xDD:
 #ize, fmt, typ = _MSGPACK_HEADERS[b]
 #elf._reserve(size)
 #n,) = _unpack_from(fmt, self._buffer, self._buff_i)
 #elf._buff_i += size
 #f n > self._max_array_len:
 #aise ValueError(
 #%s exceeds max_array_len(%s)" % (n, self._max_array_len)
 #
 #lif 0xDE <= b <= 0xDF:
 #ize, fmt, typ = _MSGPACK_HEADERS[b]
 #elf._reserve(size)
 #n,) = _unpack_from(fmt, self._buffer, self._buff_i)
 #elf._buff_i += size
 #f n > self._max_map_len:
 #aise ValueError("%s exceeds max_map_len(%s)" % (n, self._max_map_len))
 #lse:
 #aise FormatError("Unknown header: 0x%x" % b)
 #eturn typ, n, obj

 #ef _unpack(self, execute=EX_CONSTRUCT):
 #yp, n, obj = self._read_header()

 #f execute == EX_READ_ARRAY_HEADER:
 #f typ != TYPE_ARRAY:
 #aise ValueError("Expected array")
 #eturn n
 #f execute == EX_READ_MAP_HEADER:
 #f typ != TYPE_MAP:
 #aise ValueError("Expected map")
 #eturn n
        # TODO should we eliminate the recursion?
 #f typ == TYPE_ARRAY:
 #f execute == EX_SKIP:
 #or i in xrange(n):
                    # TODO check whether we need to call `list_hook`
 #elf._unpack(EX_SKIP)
 #eturn
 #et = newlist_hint(n)
 #or i in xrange(n):
 #et.append(self._unpack(EX_CONSTRUCT))
 #f self._list_hook is not None:
 #et = self._list_hook(ret)
            # TODO is the interaction between `list_hook` and `use_list` ok?
 #eturn ret if self._use_list else tuple(ret)
 #f typ == TYPE_MAP:
 #f execute == EX_SKIP:
 #or i in xrange(n):
                    # TODO check whether we need to call hooks
 #elf._unpack(EX_SKIP)
 #elf._unpack(EX_SKIP)
 #eturn
 #f self._object_pairs_hook is not None:
 #et = self._object_pairs_hook(
 #self._unpack(EX_CONSTRUCT), self._unpack(EX_CONSTRUCT))
 #or _ in xrange(n)
 #
 #lse:
 #et = {}
 #or _ in xrange(n):
 #ey = self._unpack(EX_CONSTRUCT)
 #f self._strict_map_key and type(key) not in (unicode, bytes):
 #aise ValueError(
 #%s is not allowed for map key" % str(type(key))
 #
 #f not PY2 and type(key) is str:
 #ey = sys.intern(key)
 #et[key] = self._unpack(EX_CONSTRUCT)
 #f self._object_hook is not None:
 #et = self._object_hook(ret)
 #eturn ret
 #f execute == EX_SKIP:
 #eturn
 #f typ == TYPE_RAW:
 #f self._raw:
 #bj = bytes(obj)
 #lse:
 #bj = obj.decode("utf_8", self._unicode_errors)
 #eturn obj
 #f typ == TYPE_BIN:
 #eturn bytes(obj)
 #f typ == TYPE_EXT:
 #f n == -1:  # timestamp
 #s = Timestamp.from_bytes(bytes(obj))
 #f self._timestamp == 1:
 #eturn ts.to_unix()
 #lif self._timestamp == 2:
 #eturn ts.to_unix_nano()
 #lif self._timestamp == 3:
 #eturn ts.to_datetime()
 #lse:
 #eturn ts
 #lse:
 #eturn self._ext_hook(n, bytes(obj))
 #ssert typ == TYPE_IMMEDIATE
 #eturn obj

 #ef __iter__(self):
 #eturn self

 #ef __next__(self):
 #ry:
 #et = self._unpack(EX_CONSTRUCT)
 #elf._consume()
 #eturn ret
 #xcept OutOfData:
 #elf._consume()
 #aise StopIteration
 #xcept RecursionError:
 #aise StackError

 #ext = __next__

 #ef skip(self):
 #elf._unpack(EX_SKIP)
 #elf._consume()

 #ef unpack(self):
 #ry:
 #et = self._unpack(EX_CONSTRUCT)
 #xcept RecursionError:
 #aise StackError
 #elf._consume()
 #eturn ret

 #ef read_array_header(self):
 #et = self._unpack(EX_READ_ARRAY_HEADER)
 #elf._consume()
 #eturn ret

 #ef read_map_header(self):
 #et = self._unpack(EX_READ_MAP_HEADER)
 #elf._consume()
 #eturn ret

 #ef tell(self):
 #eturn self._stream_offset


class Packer(object):
 #""
 #essagePack Packer

 #sage::

 #acker = Packer()
 #stream.write(packer.pack(a))
 #stream.write(packer.pack(b))

 #acker's constructor has some keyword arguments:

 #param callable default:
 #onvert user type to builtin type that Packer supports.
 #ee also simplejson's document.

 #param bool use_single_float:
 #se single precision float type for float. (default: False)

 #param bool autoreset:
 #eset buffer after each pack and return its content as `bytes`. (default: True).
 #f set this to false, use `bytes()` to get content and `.reset()` to clear buffer.

 #param bool use_bin_type:
 #se bin type introduced in msgpack spec 2.0 for bytes.
 #t also enables str8 type for unicode. (default: True)

 #param bool strict_types:
 #f set to true, types will be checked to be exact. Derived classes
 #rom serializable types will not be serialized and will be
 #reated as unsupported type and forwarded to default.
 #dditionally tuples will not be serialized as lists.
 #his is useful when trying to implement accurate serialization
 #or python types.

 #param bool datetime:
 #f set to true, datetime with tzinfo is packed into Timestamp type.
 #ote that the tzinfo is stripped in the timestamp.
 #ou can get UTC datetime with `timestamp=3` option of the Unpacker.
 #Python 2 is not supported).

 #param str unicode_errors:
 #he error handler for encoding unicode. (default: 'strict')
 #O NOT USE THIS!!  This option is kept for very specific usage.

 #xample of streaming deserialize from file-like object::

 #npacker = Unpacker(file_like)
 #or o in unpacker:
 #rocess(o)

 #xample of streaming deserialize from socket::

 #npacker = Unpacker()
 #hile True:
 #uf = sock.recv(1024**2)
 #f not buf:
 #reak
 #npacker.feed(buf)
 #or o in unpacker:
 #rocess(o)

 #aises ``ExtraData`` when *packed* contains extra bytes.
 #aises ``OutOfData`` when *packed* is incomplete.
 #aises ``FormatError`` when *packed* is not valid msgpack.
 #aises ``StackError`` when *packed* contains too nested.
 #ther exceptions can be raised during unpacking.
 #""

 #ef __init__(
 #elf,
 #efault=None,
 #se_single_float=False,
 #utoreset=True,
 #se_bin_type=True,
 #trict_types=False,
 #atetime=False,
 #nicode_errors=None,
 #:
 #elf._strict_types = strict_types
 #elf._use_float = use_single_float
 #elf._autoreset = autoreset
 #elf._use_bin_type = use_bin_type
 #elf._buffer = StringIO()
 #f PY2 and datetime:
 #aise ValueError("datetime is not supported in Python 2")
 #elf._datetime = bool(datetime)
 #elf._unicode_errors = unicode_errors or "strict"
 #f default is not None:
 #f not callable(default):
 #aise TypeError("default must be callable")
 #elf._default = default

 #ef _pack(
 #elf,
 #bj,
 #est_limit=DEFAULT_RECURSE_LIMIT,
 #heck=isinstance,
 #heck_type_strict=_check_type_strict,
 #:
 #efault_used = False
 #f self._strict_types:
 #heck = check_type_strict
 #ist_types = list
 #lse:
 #ist_types = (list, tuple)
 #hile True:
 #f nest_limit < 0:
 #aise ValueError("recursion limit exceeded")
 #f obj is None:
 #eturn self._buffer.write(b"\xc0")
 #f check(obj, bool):
 #f obj:
 #eturn self._buffer.write(b"\xc3")
 #eturn self._buffer.write(b"\xc2")
 #f check(obj, int_types):
 #f 0 <= obj < 0x80:
 #eturn self._buffer.write(struct.pack("B", obj))
 #f -0x20 <= obj < 0:
 #eturn self._buffer.write(struct.pack("b", obj))
 #f 0x80 <= obj <= 0xFF:
 #eturn self._buffer.write(struct.pack("BB", 0xCC, obj))
 #f -0x80 <= obj < 0:
 #eturn self._buffer.write(struct.pack(">Bb", 0xD0, obj))
 #f 0xFF < obj <= 0xFFFF:
 #eturn self._buffer.write(struct.pack(">BH", 0xCD, obj))
 #f -0x8000 <= obj < -0x80:
 #eturn self._buffer.write(struct.pack(">Bh", 0xD1, obj))
 #f 0xFFFF < obj <= 0xFFFFFFFF:
 #eturn self._buffer.write(struct.pack(">BI", 0xCE, obj))
 #f -0x80000000 <= obj < -0x8000:
 #eturn self._buffer.write(struct.pack(">Bi", 0xD2, obj))
 #f 0xFFFFFFFF < obj <= 0xFFFFFFFFFFFFFFFF:
 #eturn self._buffer.write(struct.pack(">BQ", 0xCF, obj))
 #f -0x8000000000000000 <= obj < -0x80000000:
 #eturn self._buffer.write(struct.pack(">Bq", 0xD3, obj))
 #f not default_used and self._default is not None:
 #bj = self._default(obj)
 #efault_used = True
 #ontinue
 #aise OverflowError("Integer value out of range")
 #f check(obj, (bytes, bytearray)):
 # = len(obj)
 #f n >= 2**32:
 #aise ValueError("%s is too large" % type(obj).__name__)
 #elf._pack_bin_header(n)
 #eturn self._buffer.write(obj)
 #f check(obj, unicode):
 #bj = obj.encode("utf-8", self._unicode_errors)
 # = len(obj)
 #f n >= 2**32:
 #aise ValueError("String is too large")
 #elf._pack_raw_header(n)
 #eturn self._buffer.write(obj)
 #f check(obj, memoryview):
 # = obj.nbytes
 #f n >= 2**32:
 #aise ValueError("Memoryview is too large")
 #elf._pack_bin_header(n)
 #eturn self._buffer.write(obj)
 #f check(obj, float):
 #f self._use_float:
 #eturn self._buffer.write(struct.pack(">Bf", 0xCA, obj))
 #eturn self._buffer.write(struct.pack(">Bd", 0xCB, obj))
 #f check(obj, (ExtType, Timestamp)):
 #f check(obj, Timestamp):
 #ode = -1
 #ata = obj.to_bytes()
 #lse:
 #ode = obj.code
 #ata = obj.data
 #ssert isinstance(code, int)
 #ssert isinstance(data, bytes)
 # = len(data)
 #f L == 1:
 #elf._buffer.write(b"\xd4")
 #lif L == 2:
 #elf._buffer.write(b"\xd5")
 #lif L == 4:
 #elf._buffer.write(b"\xd6")
 #lif L == 8:
 #elf._buffer.write(b"\xd7")
 #lif L == 16:
 #elf._buffer.write(b"\xd8")
 #lif L <= 0xFF:
 #elf._buffer.write(struct.pack(">BB", 0xC7, L))
 #lif L <= 0xFFFF:
 #elf._buffer.write(struct.pack(">BH", 0xC8, L))
 #lse:
 #elf._buffer.write(struct.pack(">BI", 0xC9, L))
 #elf._buffer.write(struct.pack("b", code))
 #elf._buffer.write(data)
 #eturn
 #f check(obj, list_types):
 # = len(obj)
 #elf._pack_array_header(n)
 #or i in xrange(n):
 #elf._pack(obj[i], nest_limit - 1)
 #eturn
 #f check(obj, dict):
 #eturn self._pack_map_pairs(
 #en(obj), dict_iteritems(obj), nest_limit - 1
 #

 #f self._datetime and check(obj, _DateTime) and obj.tzinfo is not None:
 #bj = Timestamp.from_datetime(obj)
 #efault_used = 1
 #ontinue

 #f not default_used and self._default is not None:
 #bj = self._default(obj)
 #efault_used = 1
 #ontinue

 #f self._datetime and check(obj, _DateTime):
 #aise ValueError("Cannot serialize %r where tzinfo=None" % (obj,))

 #aise TypeError("Cannot serialize %r" % (obj,))

 #ef pack(self, obj):
 #ry:
 #elf._pack(obj)
 #xcept:
 #elf._buffer = StringIO()  # force reset
 #aise
 #f self._autoreset:
 #et = self._buffer.getvalue()
 #elf._buffer = StringIO()
 #eturn ret

 #ef pack_map_pairs(self, pairs):
 #elf._pack_map_pairs(len(pairs), pairs)
 #f self._autoreset:
 #et = self._buffer.getvalue()
 #elf._buffer = StringIO()
 #eturn ret

 #ef pack_array_header(self, n):
 #f n >= 2**32:
 #aise ValueError
 #elf._pack_array_header(n)
 #f self._autoreset:
 #et = self._buffer.getvalue()
 #elf._buffer = StringIO()
 #eturn ret

 #ef pack_map_header(self, n):
 #f n >= 2**32:
 #aise ValueError
 #elf._pack_map_header(n)
 #f self._autoreset:
 #et = self._buffer.getvalue()
 #elf._buffer = StringIO()
 #eturn ret

 #ef pack_ext_type(self, typecode, data):
 #f not isinstance(typecode, int):
 #aise TypeError("typecode must have int type.")
 #f not 0 <= typecode <= 127:
 #aise ValueError("typecode should be 0-127")
 #f not isinstance(data, bytes):
 #aise TypeError("data must have bytes type")
 # = len(data)
 #f L > 0xFFFFFFFF:
 #aise ValueError("Too large data")
 #f L == 1:
 #elf._buffer.write(b"\xd4")
 #lif L == 2:
 #elf._buffer.write(b"\xd5")
 #lif L == 4:
 #elf._buffer.write(b"\xd6")
 #lif L == 8:
 #elf._buffer.write(b"\xd7")
 #lif L == 16:
 #elf._buffer.write(b"\xd8")
 #lif L <= 0xFF:
 #elf._buffer.write(b"\xc7" + struct.pack("B", L))
 #lif L <= 0xFFFF:
 #elf._buffer.write(b"\xc8" + struct.pack(">H", L))
 #lse:
 #elf._buffer.write(b"\xc9" + struct.pack(">I", L))
 #elf._buffer.write(struct.pack("B", typecode))
 #elf._buffer.write(data)

 #ef _pack_array_header(self, n):
 #f n <= 0x0F:
 #eturn self._buffer.write(struct.pack("B", 0x90 + n))
 #f n <= 0xFFFF:
 #eturn self._buffer.write(struct.pack(">BH", 0xDC, n))
 #f n <= 0xFFFFFFFF:
 #eturn self._buffer.write(struct.pack(">BI", 0xDD, n))
 #aise ValueError("Array is too large")

 #ef _pack_map_header(self, n):
 #f n <= 0x0F:
 #eturn self._buffer.write(struct.pack("B", 0x80 + n))
 #f n <= 0xFFFF:
 #eturn self._buffer.write(struct.pack(">BH", 0xDE, n))
 #f n <= 0xFFFFFFFF:
 #eturn self._buffer.write(struct.pack(">BI", 0xDF, n))
 #aise ValueError("Dict is too large")

 #ef _pack_map_pairs(self, n, pairs, nest_limit=DEFAULT_RECURSE_LIMIT):
 #elf._pack_map_header(n)
 #or (k, v) in pairs:
 #elf._pack(k, nest_limit - 1)
 #elf._pack(v, nest_limit - 1)

 #ef _pack_raw_header(self, n):
 #f n <= 0x1F:
 #elf._buffer.write(struct.pack("B", 0xA0 + n))
 #lif self._use_bin_type and n <= 0xFF:
 #elf._buffer.write(struct.pack(">BB", 0xD9, n))
 #lif n <= 0xFFFF:
 #elf._buffer.write(struct.pack(">BH", 0xDA, n))
 #lif n <= 0xFFFFFFFF:
 #elf._buffer.write(struct.pack(">BI", 0xDB, n))
 #lse:
 #aise ValueError("Raw is too large")

 #ef _pack_bin_header(self, n):
 #f not self._use_bin_type:
 #eturn self._pack_raw_header(n)
 #lif n <= 0xFF:
 #eturn self._buffer.write(struct.pack(">BB", 0xC4, n))
 #lif n <= 0xFFFF:
 #eturn self._buffer.write(struct.pack(">BH", 0xC5, n))
 #lif n <= 0xFFFFFFFF:
 #eturn self._buffer.write(struct.pack(">BI", 0xC6, n))
 #lse:
 #aise ValueError("Bin is too large")

 #ef bytes(self):
 #""Return internal buffer contents as bytes object"""
 #eturn self._buffer.getvalue()

 #ef reset(self):
 #""Reset internal buffer.

 #his method is useful only when autoreset=False.
 #""
 #elf._buffer = StringIO()

 #ef getbuffer(self):
 #""Return view of internal buffer."""
 #f USING_STRINGBUILDER or PY2:
 #eturn memoryview(self.bytes())
 #lse:
 #eturn self._buffer.getbuffer()
