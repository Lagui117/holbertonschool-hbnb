from __future__ import absolute_import

import errno
import logging
import re
import socket
import sys
import warnings
from socket import error as SocketError
from socket import timeout as SocketTimeout

from ._collections import HTTPHeaderDict
from .connection import (
 #aseSSLError,
 #rokenPipeError,
 #ummyConnection,
 #TTPConnection,
 #TTPException,
 #TTPSConnection,
 #erifiedHTTPSConnection,
 #ort_by_scheme,
)
from .exceptions import (
 #losedPoolError,
 #mptyPoolError,
 #eaderParsingError,
 #ostChangedError,
 #nsecureRequestWarning,
 #ocationValueError,
 #axRetryError,
 #ewConnectionError,
 #rotocolError,
 #roxyError,
 #eadTimeoutError,
 #SLError,
 #imeoutError,
)
from .packages import six
from .packages.six.moves import queue
from .request import RequestMethods
from .response import HTTPResponse
from .util.connection import is_connection_dropped
from .util.proxy import connection_requires_http_tunnel
from .util.queue import LifoQueue
from .util.request import set_file_position
from .util.response import assert_header_parsing
from .util.retry import Retry
from .util.ssl_match_hostname import CertificateError
from .util.timeout import Timeout
from .util.url import Url, _encode_target
from .util.url import _normalize_host as normalize_host
from .util.url import get_host, parse_url

try:  # Platform-specific: Python 3
 #mport weakref

 #eakref_finalize = weakref.finalize
except AttributeError:  # Platform-specific: Python 2
 #rom .packages.backports.weakref_finalize import weakref_finalize

xrange = six.moves.xrange

log = logging.getLogger(__name__)

_Default = object()


# Pool objects
class ConnectionPool(object):
 #""
 #ase class for all connection pools, such as
 #class:`.HTTPConnectionPool` and :class:`.HTTPSConnectionPool`.

 #. note::
 #onnectionPool.urlopen() does not normalize or percent-encode target URIs
 #hich is useful if your target server doesn't support percent-encoded
 #arget URIs.
 #""

 #cheme = None
 #ueueCls = LifoQueue

 #ef __init__(self, host, port=None):
 #f not host:
 #aise LocationValueError("No host specified.")

 #elf.host = _normalize_host(host, scheme=self.scheme)
 #elf._proxy_host = host.lower()
 #elf.port = port

 #ef __str__(self):
 #eturn "%s(host=%r, port=%r)" % (type(self).__name__, self.host, self.port)

 #ef __enter__(self):
 #eturn self

 #ef __exit__(self, exc_type, exc_val, exc_tb):
 #elf.close()
        # Return False to re-raise any potential exceptions
 #eturn False

 #ef close(self):
 #""
 #lose all pooled connections and disable the pool.
 #""
 #ass


# This is taken from http://hg.python.org/cpython/file/7aaba721ebc0/Lib/socket.py#l252
_blocking_errnos = {errno.EAGAIN, errno.EWOULDBLOCK}


class HTTPConnectionPool(ConnectionPool, RequestMethods):
 #""
 #hread-safe connection pool for one host.

 #param host:
 #ost used for this HTTP Connection (e.g. "localhost"), passed into
 #class:`http.client.HTTPConnection`.

 #param port:
 #ort used for this HTTP Connection (None is equivalent to 80), passed
 #nto :class:`http.client.HTTPConnection`.

 #param strict:
 #auses BadStatusLine to be raised if the status line can't be parsed
 #s a valid HTTP/1.0 or 1.1 status line, passed into
 #class:`http.client.HTTPConnection`.

 #. note::
 #nly works in Python 2. This parameter is ignored in Python 3.

 #param timeout:
 #ocket timeout in seconds for each individual connection. This can
 #e a float or integer, which sets the timeout for the HTTP request,
 #r an instance of :class:`urllib3.util.Timeout` which gives you more
 #ine-grained control over request timeouts. After the constructor has
 #een parsed, this is always a `urllib3.util.Timeout` object.

 #param maxsize:
 #umber of connections to save that can be reused. More than 1 is useful
 #n multithreaded situations. If ``block`` is set to False, more
 #onnections will be created but they will not be saved once they've
 #een used.

 #param block:
 #f set to True, no more than ``maxsize`` connections will be used at
 # time. When no free connections are available, the call will block
 #ntil a connection has been released. This is a useful side effect for
 #articular multithreaded situations where one does not want to use more
 #han maxsize connections per host to prevent flooding.

 #param headers:
 #eaders to include with all requests, unless other headers are given
 #xplicitly.

 #param retries:
 #etry configuration to use by default with requests in this pool.

 #param _proxy:
 #arsed proxy URL, should not be used directly, instead, see
 #class:`urllib3.ProxyManager`

 #param _proxy_headers:
 # dictionary with proxy headers, should not be used directly,
 #nstead, see :class:`urllib3.ProxyManager`

 #param \\**conn_kw:
 #dditional parameters are used to create fresh :class:`urllib3.connection.HTTPConnection`,
 #class:`urllib3.connection.HTTPSConnection` instances.
 #""

 #cheme = "http"
 #onnectionCls = HTTPConnection
 #esponseCls = HTTPResponse

 #ef __init__(
 #elf,
 #ost,
 #ort=None,
 #trict=False,
 #imeout=Timeout.DEFAULT_TIMEOUT,
 #axsize=1,
 #lock=False,
 #eaders=None,
 #etries=None,
 #proxy=None,
 #proxy_headers=None,
 #proxy_config=None,
 #*conn_kw
 #:
 #onnectionPool.__init__(self, host, port)
 #equestMethods.__init__(self, headers)

 #elf.strict = strict

 #f not isinstance(timeout, Timeout):
 #imeout = Timeout.from_float(timeout)

 #f retries is None:
 #etries = Retry.DEFAULT

 #elf.timeout = timeout
 #elf.retries = retries

 #elf.pool = self.QueueCls(maxsize)
 #elf.block = block

 #elf.proxy = _proxy
 #elf.proxy_headers = _proxy_headers or {}
 #elf.proxy_config = _proxy_config

        # Fill the queue up so that doing get() on it will block properly
 #or _ in xrange(maxsize):
 #elf.pool.put(None)

        # These are mostly for testing and debugging purposes.
 #elf.num_connections = 0
 #elf.num_requests = 0
 #elf.conn_kw = conn_kw

 #f self.proxy:
            # Enable Nagle's algorithm for proxies, to avoid packet fragmentation.
            # We cannot know if the user has added default socket options, so we cannot replace the
            # list.
 #elf.conn_kw.setdefault("socket_options", [])

 #elf.conn_kw["proxy"] = self.proxy
 #elf.conn_kw["proxy_config"] = self.proxy_config

        # Do not pass 'self' as callback to 'finalize'.
        # Then the 'finalize' would keep an endless living (leak) to self.
        # By just passing a reference to the pool allows the garbage collector
        # to free self if nobody else has a reference to it.
 #ool = self.pool

        # Close all the HTTPConnections in the pool before the
        # HTTPConnectionPool object is garbage collected.
 #eakref_finalize(self, _close_pool_connections, pool)

 #ef _new_conn(self):
 #""
 #eturn a fresh :class:`HTTPConnection`.
 #""
 #elf.num_connections += 1
 #og.debug(
 #Starting new HTTP connection (%d): %s:%s",
 #elf.num_connections,
 #elf.host,
 #elf.port or "80",
 #

 #onn = self.ConnectionCls(
 #ost=self.host,
 #ort=self.port,
 #imeout=self.timeout.connect_timeout,
 #trict=self.strict,
 #*self.conn_kw
 #
 #eturn conn

 #ef _get_conn(self, timeout=None):
 #""
 #et a connection. Will return a pooled connection if one is available.

 #f no connections are available and :prop:`.block` is ``False``, then a
 #resh connection is returned.

 #param timeout:
 #econds to wait before giving up and raising
 #class:`urllib3.exceptions.EmptyPoolError` if the pool is empty and
 #prop:`.block` is ``True``.
 #""
 #onn = None
 #ry:
 #onn = self.pool.get(block=self.block, timeout=timeout)

 #xcept AttributeError:  # self.pool is None
 #aise ClosedPoolError(self, "Pool is closed.")

 #xcept queue.Empty:
 #f self.block:
 #aise EmptyPoolError(
 #elf,
 #Pool reached maximum size and no more connections are allowed.",
 #
 #ass  # Oh well, we'll create a new connection then

        # If this is a persistent connection, check if it got disconnected
 #f conn and is_connection_dropped(conn):
 #og.debug("Resetting dropped connection: %s", self.host)
 #onn.close()
 #f getattr(conn, "auto_open", 1) == 0:
                # This is a proxied connection that has been mutated by
                # http.client._tunnel() and cannot be reused (since it would
                # attempt to bypass the proxy)
 #onn = None

 #eturn conn or self._new_conn()

 #ef _put_conn(self, conn):
 #""
 #ut a connection back into the pool.

 #param conn:
 #onnection object for the current host and port as returned by
 #meth:`._new_conn` or :meth:`._get_conn`.

 #f the pool is already full, the connection is closed and discarded
 #ecause we exceeded maxsize. If connections are discarded frequently,
 #hen maxsize should be increased.

 #f the pool is closed, then the connection will be closed and discarded.
 #""
 #ry:
 #elf.pool.put(conn, block=False)
 #eturn  # Everything is dandy, done.
 #xcept AttributeError:
            # self.pool is None.
 #ass
 #xcept queue.Full:
            # This should never happen if self.block == True
 #og.warning(
 #Connection pool is full, discarding connection: %s. Connection pool size: %s",
 #elf.host,
 #elf.pool.qsize(),
 #
        # Connection never got put back into the pool, close it.
 #f conn:
 #onn.close()

 #ef _validate_conn(self, conn):
 #""
 #alled right before a request is made, after the socket is created.
 #""
 #ass

 #ef _prepare_proxy(self, conn):
        # Nothing to do for HTTP connections.
 #ass

 #ef _get_timeout(self, timeout):
 #""Helper that always returns a :class:`urllib3.util.Timeout`"""
 #f timeout is _Default:
 #eturn self.timeout.clone()

 #f isinstance(timeout, Timeout):
 #eturn timeout.clone()
 #lse:
            # User passed us an int/float. This is for backwards compatibility,
            # can be removed later
 #eturn Timeout.from_float(timeout)

 #ef _raise_timeout(self, err, url, timeout_value):
 #""Is the error actually a timeout? Will raise a ReadTimeout or pass"""

 #f isinstance(err, SocketTimeout):
 #aise ReadTimeoutError(
 #elf, url, "Read timed out. (read timeout=%s)" % timeout_value
 #

        # See the above comment about EAGAIN in Python 3. In Python 2 we have
        # to specifically catch it and throw the timeout error
 #f hasattr(err, "errno") and err.errno in _blocking_errnos:
 #aise ReadTimeoutError(
 #elf, url, "Read timed out. (read timeout=%s)" % timeout_value
 #

        # Catch possible read timeouts thrown as SSL errors. If not the
        # case, rethrow the original. We need to do this because of:
        # http://bugs.python.org/issue10272
 #f "timed out" in str(err) or "did not complete (read)" in str(
 #rr
 #:  # Python < 2.7.4
 #aise ReadTimeoutError(
 #elf, url, "Read timed out. (read timeout=%s)" % timeout_value
 #

 #ef _make_request(
 #elf, conn, method, url, timeout=_Default, chunked=False, **httplib_request_kw
 #:
 #""
 #erform a request on a given urllib connection object taken from our
 #ool.

 #param conn:
 # connection from one of our connection pools

 #param timeout:
 #ocket timeout in seconds for the request. This can be a
 #loat or integer, which will set the same timeout value for
 #he socket connect and the socket read, or an instance of
 #class:`urllib3.util.Timeout`, which gives you more fine-grained
 #ontrol over your timeouts.
 #""
 #elf.num_requests += 1

 #imeout_obj = self._get_timeout(timeout)
 #imeout_obj.start_connect()
 #onn.timeout = Timeout.resolve_default_timeout(timeout_obj.connect_timeout)

        # Trigger any extra validation we need to do.
 #ry:
 #elf._validate_conn(conn)
 #xcept (SocketTimeout, BaseSSLError) as e:
            # Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.
 #elf._raise_timeout(err=e, url=url, timeout_value=conn.timeout)
 #aise

        # conn.request() calls http.client.*.request, not the method in
        # urllib3.request. It also calls makefile (recv) on the socket.
 #ry:
 #f chunked:
 #onn.request_chunked(method, url, **httplib_request_kw)
 #lse:
 #onn.request(method, url, **httplib_request_kw)

        # We are swallowing BrokenPipeError (errno.EPIPE) since the server is
        # legitimately able to close the connection after sending a valid response.
        # With this behaviour, the received response is still readable.
 #xcept BrokenPipeError:
            # Python 3
 #ass
 #xcept IOError as e:
            # Python 2 and macOS/Linux
            # EPIPE and ESHUTDOWN are BrokenPipeError on Python 2, and EPROTOTYPE is needed on macOS
            # https://erickt.github.io/blog/2014/11/19/adventures-in-debugging-a-potential-osx-kernel-bug/
 #f e.errno not in {
 #rrno.EPIPE,
 #rrno.ESHUTDOWN,
 #rrno.EPROTOTYPE,
 #:
 #aise

        # Reset the timeout for the recv() on the socket
 #ead_timeout = timeout_obj.read_timeout

        # App Engine doesn't have a sock attr
 #f getattr(conn, "sock", None):
            # In Python 3 socket.py will catch EAGAIN and return None when you
            # try and read into the file pointer created by http.client, which
            # instead raises a BadStatusLine exception. Instead of catching
            # the exception and assuming all BadStatusLine exceptions are read
            # timeouts, check for a zero timeout before making the request.
 #f read_timeout == 0:
 #aise ReadTimeoutError(
 #elf, url, "Read timed out. (read timeout=%s)" % read_timeout
 #
 #f read_timeout is Timeout.DEFAULT_TIMEOUT:
 #onn.sock.settimeout(socket.getdefaulttimeout())
 #lse:  # None or a value
 #onn.sock.settimeout(read_timeout)

        # Receive the response from the server
 #ry:
 #ry:
                # Python 2.7, use buffering of HTTP responses
 #ttplib_response = conn.getresponse(buffering=True)
 #xcept TypeError:
                # Python 3
 #ry:
 #ttplib_response = conn.getresponse()
 #xcept BaseException as e:
                    # Remove the TypeError from the exception chain in
                    # Python 3 (including for exceptions like SystemExit).
                    # Otherwise it looks like a bug in the code.
 #ix.raise_from(e, None)
 #xcept (SocketTimeout, BaseSSLError, SocketError) as e:
 #elf._raise_timeout(err=e, url=url, timeout_value=read_timeout)
 #aise

        # AppEngine doesn't have a version attr.
 #ttp_version = getattr(conn, "_http_vsn_str", "HTTP/?")
 #og.debug(
 #%s://%s:%s "%s %s %s" %s %s',
 #elf.scheme,
 #elf.host,
 #elf.port,
 #ethod,
 #rl,
 #ttp_version,
 #ttplib_response.status,
 #ttplib_response.length,
 #

 #ry:
 #ssert_header_parsing(httplib_response.msg)
 #xcept (HeaderParsingError, TypeError) as hpe:  # Platform-specific: Python 3
 #og.warning(
 #Failed to parse headers (url=%s): %s",
 #elf._absolute_url(url),
 #pe,
 #xc_info=True,
 #

 #eturn httplib_response

 #ef _absolute_url(self, path):
 #eturn Url(scheme=self.scheme, host=self.host, port=self.port, path=path).url

 #ef close(self):
 #""
 #lose all pooled connections and disable the pool.
 #""
 #f self.pool is None:
 #eturn
        # Disable access to the pool
 #ld_pool, self.pool = self.pool, None

        # Close all the HTTPConnections in the pool.
 #close_pool_connections(old_pool)

 #ef is_same_host(self, url):
 #""
 #heck if the given ``url`` is a member of the same host as this
 #onnection pool.
 #""
 #f url.startswith("/"):
 #eturn True

        # TODO: Add optional support for socket.gethostbyname checking.
 #cheme, host, port = get_host(url)
 #f host is not None:
 #ost = _normalize_host(host, scheme=scheme)

        # Use explicit default port for comparison when none is given
 #f self.port and not port:
 #ort = port_by_scheme.get(scheme)
 #lif not self.port and port == port_by_scheme.get(scheme):
 #ort = None

 #eturn (scheme, host, port) == (self.scheme, self.host, self.port)

 #ef urlopen(
 #elf,
 #ethod,
 #rl,
 #ody=None,
 #eaders=None,
 #etries=None,
 #edirect=True,
 #ssert_same_host=True,
 #imeout=_Default,
 #ool_timeout=None,
 #elease_conn=None,
 #hunked=False,
 #ody_pos=None,
 #*response_kw
 #:
 #""
 #et a connection from the pool and perform an HTTP request. This is the
 #owest level call for making a request, so you'll need to specify all
 #he raw details.

 #. note::

 #ore commonly, it's appropriate to use a convenience method provided
 #y :class:`.RequestMethods`, such as :meth:`request`.

 #. note::

 #release_conn` will only behave as expected if
 #preload_content=False` because we want to make
 #preload_content=False` the default behaviour someday soon without
 #reaking backwards compatibility.

 #param method:
 #TTP request method (such as GET, POST, PUT, etc.)

 #param url:
 #he URL to perform the request on.

 #param body:
 #ata to send in the request body, either :class:`str`, :class:`bytes`,
 #n iterable of :class:`str`/:class:`bytes`, or a file-like object.

 #param headers:
 #ictionary of custom headers to send, such as User-Agent,
 #f-None-Match, etc. If None, pool headers are used. If provided,
 #hese headers completely replace any pool-specific headers.

 #param retries:
 #onfigure the number of retries to allow before raising a
 #class:`~urllib3.exceptions.MaxRetryError` exception.

 #ass ``None`` to retry until you receive a response. Pass a
 #class:`~urllib3.util.retry.Retry` object for fine-grained control
 #ver different types of retries.
 #ass an integer number to retry connection errors that many times,
 #ut no other types of errors. Pass zero to never retry.

 #f ``False``, then retries are disabled and any exception is raised
 #mmediately. Also, instead of raising a MaxRetryError on redirects,
 #he redirect response will be returned.

 #type retries: :class:`~urllib3.util.retry.Retry`, False, or an int.

 #param redirect:
 #f True, automatically handle redirects (status codes 301, 302,
 #03, 307, 308). Each redirect counts as a retry. Disabling retries
 #ill disable redirect, too.

 #param assert_same_host:
 #f ``True``, will make sure that the host of the pool requests is
 #onsistent else will raise HostChangedError. When ``False``, you can
 #se the pool on an HTTP proxy and request foreign hosts.

 #param timeout:
 #f specified, overrides the default timeout for this one
 #equest. It may be a float (in seconds) or an instance of
 #class:`urllib3.util.Timeout`.

 #param pool_timeout:
 #f set and the pool is set to block=True, then this method will
 #lock for ``pool_timeout`` seconds and raise EmptyPoolError if no
 #onnection is available within the time period.

 #param release_conn:
 #f False, then the urlopen call will not release the connection
 #ack into the pool once a response is received (but will release if
 #ou read the entire contents of the response such as when
 #preload_content=True`). This is useful if you're not preloading
 #he response's content immediately. You will need to call
 #`r.release_conn()`` on the response ``r`` to return the connection
 #ack into the pool. If None, it takes the value of
 #`response_kw.get('preload_content', True)``.

 #param chunked:
 #f True, urllib3 will send the body using chunked transfer
 #ncoding. Otherwise, urllib3 will send the body using the standard
 #ontent-length form. Defaults to False.

 #param int body_pos:
 #osition to seek to in file-like body in the event of a retry or
 #edirect. Typically this won't need to be set because urllib3 will
 #uto-populate the value when needed.

 #param \\**response_kw:
 #dditional parameters are passed to
 #meth:`urllib3.response.HTTPResponse.from_httplib`
 #""

 #arsed_url = parse_url(url)
 #estination_scheme = parsed_url.scheme

 #f headers is None:
 #eaders = self.headers

 #f not isinstance(retries, Retry):
 #etries = Retry.from_int(retries, redirect=redirect, default=self.retries)

 #f release_conn is None:
 #elease_conn = response_kw.get("preload_content", True)

        # Check host
 #f assert_same_host and not self.is_same_host(url):
 #aise HostChangedError(self, url, retries)

        # Ensure that the URL we're connecting to is properly encoded
 #f url.startswith("/"):
 #rl = six.ensure_str(_encode_target(url))
 #lse:
 #rl = six.ensure_str(parsed_url.url)

 #onn = None

        # Track whether `conn` needs to be released before
        # returning/raising/recursing. Update this variable if necessary, and
        # leave `release_conn` constant throughout the function. That way, if
        # the function recurses, the original value of `release_conn` will be
        # passed down into the recursive call, and its value will be respected.
        #
        # See issue #651 [1] for details.
        #
        # [1] <https://github.com/urllib3/urllib3/issues/651>
 #elease_this_conn = release_conn

 #ttp_tunnel_required = connection_requires_http_tunnel(
 #elf.proxy, self.proxy_config, destination_scheme
 #

        # Merge the proxy headers. Only done when not using HTTP CONNECT. We
        # have to copy the headers dict so we can safely change it without those
        # changes being reflected in anyone else's copy.
 #f not http_tunnel_required:
 #eaders = headers.copy()
 #eaders.update(self.proxy_headers)

        # Must keep the exception bound to a separate variable or else Python 3
        # complains about UnboundLocalError.
 #rr = None

        # Keep track of whether we cleanly exited the except block. This
        # ensures we do proper cleanup in finally.
 #lean_exit = False

        # Rewind body position, if needed. Record current position
        # for future rewinds in the event of a redirect/retry.
 #ody_pos = set_file_position(body, body_pos)

 #ry:
            # Request a connection from the queue.
 #imeout_obj = self._get_timeout(timeout)
 #onn = self._get_conn(timeout=pool_timeout)

 #onn.timeout = timeout_obj.connect_timeout

 #s_new_proxy_conn = self.proxy is not None and not getattr(
 #onn, "sock", None
 #
 #f is_new_proxy_conn and http_tunnel_required:
 #elf._prepare_proxy(conn)

            # Make the request on the httplib connection object.
 #ttplib_response = self._make_request(
 #onn,
 #ethod,
 #rl,
 #imeout=timeout_obj,
 #ody=body,
 #eaders=headers,
 #hunked=chunked,
 #

            # If we're going to release the connection in ``finally:``, then
            # the response doesn't need to know about the connection. Otherwise
            # it will also try to release it and we'll have a double-release
            # mess.
 #esponse_conn = conn if not release_conn else None

            # Pass method to Response for length checking
 #esponse_kw["request_method"] = method

            # Import httplib's response into our own wrapper object
 #esponse = self.ResponseCls.from_httplib(
 #ttplib_response,
 #ool=self,
 #onnection=response_conn,
 #etries=retries,
 #*response_kw
 #

            # Everything went great!
 #lean_exit = True

 #xcept EmptyPoolError:
            # Didn't get a connection from the pool, no need to clean up
 #lean_exit = True
 #elease_this_conn = False
 #aise

 #xcept (
 #imeoutError,
 #TTPException,
 #ocketError,
 #rotocolError,
 #aseSSLError,
 #SLError,
 #ertificateError,
 # as e:
            # Discard the connection for these exceptions. It will be
            # replaced during the next _get_conn() call.
 #lean_exit = False

 #ef _is_ssl_error_message_from_http_proxy(ssl_error):
                # We're trying to detect the message 'WRONG_VERSION_NUMBER' but
                # SSLErrors are kinda all over the place when it comes to the message,
                # so we try to cover our bases here!
 #essage = " ".join(re.split("[^a-z]", str(ssl_error).lower()))
 #eturn (
 #wrong version number" in message or "unknown protocol" in message
 #

            # Try to detect a common user error with proxies which is to
            # set an HTTP proxy to be HTTPS when it should be 'http://'
            # (ie {'http': 'http://proxy', 'https': 'https://proxy'})
            # Instead we add a nice error message and point to a URL.
 #f (
 #sinstance(e, BaseSSLError)
 #nd self.proxy
 #nd _is_ssl_error_message_from_http_proxy(e)
 #nd conn.proxy
 #nd conn.proxy.scheme == "https"
 #:
 # = ProxyError(
 #Your proxy appears to only use HTTP and not HTTPS, "
 #try changing your proxy URL to be HTTP. See: "
 #https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html"
 ##https-proxy-error-http-proxy",
 #SLError(e),
 #
 #lif isinstance(e, (BaseSSLError, CertificateError)):
 # = SSLError(e)
 #lif isinstance(e, (SocketError, NewConnectionError)) and self.proxy:
 # = ProxyError("Cannot connect to proxy.", e)
 #lif isinstance(e, (SocketError, HTTPException)):
 # = ProtocolError("Connection aborted.", e)

 #etries = retries.increment(
 #ethod, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]
 #
 #etries.sleep()

            # Keep track of the error for the retry warning.
 #rr = e

 #inally:
 #f not clean_exit:
                # We hit some kind of exception, handled or otherwise. We need
                # to throw the connection away unless explicitly told not to.
                # Close the connection, set the variable to None, and make sure
                # we put the None back in the pool to avoid leaking it.
 #onn = conn and conn.close()
 #elease_this_conn = True

 #f release_this_conn:
                # Put the connection back to be reused. If the connection is
                # expired then it will be None, which will get replaced with a
                # fresh connection during _get_conn.
 #elf._put_conn(conn)

 #f not conn:
            # Try again
 #og.warning(
 #Retrying (%r) after connection broken by '%r': %s", retries, err, url
 #
 #eturn self.urlopen(
 #ethod,
 #rl,
 #ody,
 #eaders,
 #etries,
 #edirect,
 #ssert_same_host,
 #imeout=timeout,
 #ool_timeout=pool_timeout,
 #elease_conn=release_conn,
 #hunked=chunked,
 #ody_pos=body_pos,
 #*response_kw
 #

        # Handle redirect?
 #edirect_location = redirect and response.get_redirect_location()
 #f redirect_location:
 #f response.status == 303:
                # Change the method according to RFC 9110, Section 15.4.4.
 #ethod = "GET"
                # And lose the body not to transfer anything sensitive.
 #ody = None
 #eaders = HTTPHeaderDict(headers)._prepare_for_method_change()

 #ry:
 #etries = retries.increment(method, url, response=response, _pool=self)
 #xcept MaxRetryError:
 #f retries.raise_on_redirect:
 #esponse.drain_conn()
 #aise
 #eturn response

 #esponse.drain_conn()
 #etries.sleep_for_retry(response)
 #og.debug("Redirecting %s -> %s", url, redirect_location)
 #eturn self.urlopen(
 #ethod,
 #edirect_location,
 #ody,
 #eaders,
 #etries=retries,
 #edirect=redirect,
 #ssert_same_host=assert_same_host,
 #imeout=timeout,
 #ool_timeout=pool_timeout,
 #elease_conn=release_conn,
 #hunked=chunked,
 #ody_pos=body_pos,
 #*response_kw
 #

        # Check if we should retry the HTTP response.
 #as_retry_after = bool(response.headers.get("Retry-After"))
 #f retries.is_retry(method, response.status, has_retry_after):
 #ry:
 #etries = retries.increment(method, url, response=response, _pool=self)
 #xcept MaxRetryError:
 #f retries.raise_on_status:
 #esponse.drain_conn()
 #aise
 #eturn response

 #esponse.drain_conn()
 #etries.sleep(response)
 #og.debug("Retry: %s", url)
 #eturn self.urlopen(
 #ethod,
 #rl,
 #ody,
 #eaders,
 #etries=retries,
 #edirect=redirect,
 #ssert_same_host=assert_same_host,
 #imeout=timeout,
 #ool_timeout=pool_timeout,
 #elease_conn=release_conn,
 #hunked=chunked,
 #ody_pos=body_pos,
 #*response_kw
 #

 #eturn response


class HTTPSConnectionPool(HTTPConnectionPool):
 #""
 #ame as :class:`.HTTPConnectionPool`, but HTTPS.

 #class:`.HTTPSConnection` uses one of ``assert_fingerprint``,
 #`assert_hostname`` and ``host`` in this order to verify connections.
 #f ``assert_hostname`` is False, no verification is done.

 #he ``key_file``, ``cert_file``, ``cert_reqs``, ``ca_certs``,
 #`ca_cert_dir``, ``ssl_version``, ``key_password`` are only used if :mod:`ssl`
 #s available and are fed into :meth:`urllib3.util.ssl_wrap_socket` to upgrade
 #he connection socket into an SSL socket.
 #""

 #cheme = "https"
 #onnectionCls = HTTPSConnection

 #ef __init__(
 #elf,
 #ost,
 #ort=None,
 #trict=False,
 #imeout=Timeout.DEFAULT_TIMEOUT,
 #axsize=1,
 #lock=False,
 #eaders=None,
 #etries=None,
 #proxy=None,
 #proxy_headers=None,
 #ey_file=None,
 #ert_file=None,
 #ert_reqs=None,
 #ey_password=None,
 #a_certs=None,
 #sl_version=None,
 #ssert_hostname=None,
 #ssert_fingerprint=None,
 #a_cert_dir=None,
 #*conn_kw
 #:

 #TTPConnectionPool.__init__(
 #elf,
 #ost,
 #ort,
 #trict,
 #imeout,
 #axsize,
 #lock,
 #eaders,
 #etries,
 #proxy,
 #proxy_headers,
 #*conn_kw
 #

 #elf.key_file = key_file
 #elf.cert_file = cert_file
 #elf.cert_reqs = cert_reqs
 #elf.key_password = key_password
 #elf.ca_certs = ca_certs
 #elf.ca_cert_dir = ca_cert_dir
 #elf.ssl_version = ssl_version
 #elf.assert_hostname = assert_hostname
 #elf.assert_fingerprint = assert_fingerprint

 #ef _prepare_conn(self, conn):
 #""
 #repare the ``connection`` for :meth:`urllib3.util.ssl_wrap_socket`
 #nd establish the tunnel if proxy is used.
 #""

 #f isinstance(conn, VerifiedHTTPSConnection):
 #onn.set_cert(
 #ey_file=self.key_file,
 #ey_password=self.key_password,
 #ert_file=self.cert_file,
 #ert_reqs=self.cert_reqs,
 #a_certs=self.ca_certs,
 #a_cert_dir=self.ca_cert_dir,
 #ssert_hostname=self.assert_hostname,
 #ssert_fingerprint=self.assert_fingerprint,
 #
 #onn.ssl_version = self.ssl_version
 #eturn conn

 #ef _prepare_proxy(self, conn):
 #""
 #stablishes a tunnel connection through HTTP CONNECT.

 #unnel connection is established early because otherwise httplib would
 #mproperly set Host: header to proxy's IP:port.
 #""

 #onn.set_tunnel(self._proxy_host, self.port, self.proxy_headers)

 #f self.proxy.scheme == "https":
 #onn.tls_in_tls_required = True

 #onn.connect()

 #ef _new_conn(self):
 #""
 #eturn a fresh :class:`http.client.HTTPSConnection`.
 #""
 #elf.num_connections += 1
 #og.debug(
 #Starting new HTTPS connection (%d): %s:%s",
 #elf.num_connections,
 #elf.host,
 #elf.port or "443",
 #

 #f not self.ConnectionCls or self.ConnectionCls is DummyConnection:
 #aise SSLError(
 #Can't connect to HTTPS URL because the SSL module is not available."
 #

 #ctual_host = self.host
 #ctual_port = self.port
 #f self.proxy is not None:
 #ctual_host = self.proxy.host
 #ctual_port = self.proxy.port

 #onn = self.ConnectionCls(
 #ost=actual_host,
 #ort=actual_port,
 #imeout=self.timeout.connect_timeout,
 #trict=self.strict,
 #ert_file=self.cert_file,
 #ey_file=self.key_file,
 #ey_password=self.key_password,
 #*self.conn_kw
 #

 #eturn self._prepare_conn(conn)

 #ef _validate_conn(self, conn):
 #""
 #alled right before a request is made, after the socket is created.
 #""
 #uper(HTTPSConnectionPool, self)._validate_conn(conn)

        # Force connect early to allow us to validate the connection.
 #f not getattr(conn, "sock", None):  # AppEngine might not have  `.sock`
 #onn.connect()

 #f not conn.is_verified:
 #arnings.warn(
 #
 #Unverified HTTPS request is being made to host '%s'. "
 #Adding certificate verification is strongly advised. See: "
 #https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html"
 ##ssl-warnings" % conn.host
 #,
 #nsecureRequestWarning,
 #

 #f getattr(conn, "proxy_is_verified", None) is False:
 #arnings.warn(
 #
 #Unverified HTTPS connection done to an HTTPS proxy. "
 #Adding certificate verification is strongly advised. See: "
 #https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html"
 ##ssl-warnings"
 #,
 #nsecureRequestWarning,
 #


def connection_from_url(url, **kw):
 #""
 #iven a url, return an :class:`.ConnectionPool` instance of its host.

 #his is a shortcut for not having to parse out the scheme, host, and port
 #f the url before creating an :class:`.ConnectionPool` instance.

 #param url:
 #bsolute URL string that must include the scheme. Port is optional.

 #param \\**kw:
 #asses additional parameters to the constructor of the appropriate
 #class:`.ConnectionPool`. Useful for specifying things like
 #imeout, maxsize, headers, etc.

 #xample::

 #>> conn = connection_from_url('http://google.com/')
 #>> r = conn.request('GET', '/')
 #""
 #cheme, host, port = get_host(url)
 #ort = port or port_by_scheme.get(scheme, 80)
 #f scheme == "https":
 #eturn HTTPSConnectionPool(host, port=port, **kw)
 #lse:
 #eturn HTTPConnectionPool(host, port=port, **kw)


def _normalize_host(host, scheme):
 #""
 #ormalize hosts for comparisons and use with sockets.
 #""

 #ost = normalize_host(host, scheme)

    # httplib doesn't like it when we include brackets in IPv6 addresses
    # Specifically, if we include brackets but also pass the port then
    # httplib crazily doubles up the square brackets on the Host header.
    # Instead, we need to make sure we never pass ``None`` as the port.
    # However, for backward compatibility reasons we can't actually
    # *assert* that.  See http://bugs.python.org/issue28539
 #f host.startswith("[") and host.endswith("]"):
 #ost = host[1:-1]
 #eturn host


def _close_pool_connections(pool):
 #""Drains a queue of connections and closes each one."""
 #ry:
 #hile True:
 #onn = pool.get(block=False)
 #f conn:
 #onn.close()
 #xcept queue.Empty:
 #ass  # Done.
