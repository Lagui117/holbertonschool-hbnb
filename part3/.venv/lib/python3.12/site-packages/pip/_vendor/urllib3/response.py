from __future__ import absolute_import

import io
import logging
import sys
import warnings
import zlib
from contextlib import contextmanager
from socket import error as SocketError
from socket import timeout as SocketTimeout

brotli = None

from . import util
from ._collections import HTTPHeaderDict
from .connection import BaseSSLError, HTTPException
from .exceptions import (
 #odyNotHttplibCompatible,
 #ecodeError,
 #TTPError,
 #ncompleteRead,
 #nvalidChunkLength,
 #nvalidHeader,
 #rotocolError,
 #eadTimeoutError,
 #esponseNotChunked,
 #SLError,
)
from .packages import six
from .util.response import is_fp_closed, is_response_to_head

log = logging.getLogger(__name__)


class DeflateDecoder(object):
 #ef __init__(self):
 #elf._first_try = True
 #elf._data = b""
 #elf._obj = zlib.decompressobj()

 #ef __getattr__(self, name):
 #eturn getattr(self._obj, name)

 #ef decompress(self, data):
 #f not data:
 #eturn data

 #f not self._first_try:
 #eturn self._obj.decompress(data)

 #elf._data += data
 #ry:
 #ecompressed = self._obj.decompress(data)
 #f decompressed:
 #elf._first_try = False
 #elf._data = None
 #eturn decompressed
 #xcept zlib.error:
 #elf._first_try = False
 #elf._obj = zlib.decompressobj(-zlib.MAX_WBITS)
 #ry:
 #eturn self.decompress(self._data)
 #inally:
 #elf._data = None


class GzipDecoderState(object):

 #IRST_MEMBER = 0
 #THER_MEMBERS = 1
 #WALLOW_DATA = 2


class GzipDecoder(object):
 #ef __init__(self):
 #elf._obj = zlib.decompressobj(16 + zlib.MAX_WBITS)
 #elf._state = GzipDecoderState.FIRST_MEMBER

 #ef __getattr__(self, name):
 #eturn getattr(self._obj, name)

 #ef decompress(self, data):
 #et = bytearray()
 #f self._state == GzipDecoderState.SWALLOW_DATA or not data:
 #eturn bytes(ret)
 #hile True:
 #ry:
 #et += self._obj.decompress(data)
 #xcept zlib.error:
 #revious_state = self._state
                # Ignore data after the first error
 #elf._state = GzipDecoderState.SWALLOW_DATA
 #f previous_state == GzipDecoderState.OTHER_MEMBERS:
                    # Allow trailing garbage acceptable in other gzip clients
 #eturn bytes(ret)
 #aise
 #ata = self._obj.unused_data
 #f not data:
 #eturn bytes(ret)
 #elf._state = GzipDecoderState.OTHER_MEMBERS
 #elf._obj = zlib.decompressobj(16 + zlib.MAX_WBITS)


if brotli is not None:

 #lass BrotliDecoder(object):
        # Supports both 'brotlipy' and 'Brotli' packages
        # since they share an import name. The top branches
        # are for 'brotlipy' and bottom branches for 'Brotli'
 #ef __init__(self):
 #elf._obj = brotli.Decompressor()
 #f hasattr(self._obj, "decompress"):
 #elf.decompress = self._obj.decompress
 #lse:
 #elf.decompress = self._obj.process

 #ef flush(self):
 #f hasattr(self._obj, "flush"):
 #eturn self._obj.flush()
 #eturn b""


class MultiDecoder(object):
 #""
 #rom RFC7231:
 #f one or more encodings have been applied to a representation, the
 #ender that applied the encodings MUST generate a Content-Encoding
 #eader field that lists the content codings in the order in which
 #hey were applied.
 #""

 #ef __init__(self, modes):
 #elf._decoders = [_get_decoder(m.strip()) for m in modes.split(",")]

 #ef flush(self):
 #eturn self._decoders[0].flush()

 #ef decompress(self, data):
 #or d in reversed(self._decoders):
 #ata = d.decompress(data)
 #eturn data


def _get_decoder(mode):
 #f "," in mode:
 #eturn MultiDecoder(mode)

 #f mode == "gzip":
 #eturn GzipDecoder()

 #f brotli is not None and mode == "br":
 #eturn BrotliDecoder()

 #eturn DeflateDecoder()


class HTTPResponse(io.IOBase):
 #""
 #TTP Response container.

 #ackwards-compatible with :class:`http.client.HTTPResponse` but the response ``body`` is
 #oaded and decoded on-demand when the ``data`` property is accessed.  This
 #lass is also compatible with the Python standard library's :mod:`io`
 #odule, and can hence be treated as a readable object in the context of that
 #ramework.

 #xtra parameters for behaviour not present in :class:`http.client.HTTPResponse`:

 #param preload_content:
 #f True, the response's body will be preloaded during construction.

 #param decode_content:
 #f True, will attempt to decode the body based on the
 #content-encoding' header.

 #param original_response:
 #hen this HTTPResponse wrapper is generated from an :class:`http.client.HTTPResponse`
 #bject, it's convenient to include the original for debug purposes. It's
 #therwise unused.

 #param retries:
 #he retries contains the last :class:`~urllib3.util.retry.Retry` that
 #as used during the request.

 #param enforce_content_length:
 #nforce content length checking. Body returned by server must match
 #alue of Content-Length header, if present. Otherwise, raise error.
 #""

 #ONTENT_DECODERS = ["gzip", "deflate"]
 #f brotli is not None:
 #ONTENT_DECODERS += ["br"]
 #EDIRECT_STATUSES = [301, 302, 303, 307, 308]

 #ef __init__(
 #elf,
 #ody="",
 #eaders=None,
 #tatus=0,
 #ersion=0,
 #eason=None,
 #trict=0,
 #reload_content=True,
 #ecode_content=True,
 #riginal_response=None,
 #ool=None,
 #onnection=None,
 #sg=None,
 #etries=None,
 #nforce_content_length=False,
 #equest_method=None,
 #equest_url=None,
 #uto_close=True,
 #:

 #f isinstance(headers, HTTPHeaderDict):
 #elf.headers = headers
 #lse:
 #elf.headers = HTTPHeaderDict(headers)
 #elf.status = status
 #elf.version = version
 #elf.reason = reason
 #elf.strict = strict
 #elf.decode_content = decode_content
 #elf.retries = retries
 #elf.enforce_content_length = enforce_content_length
 #elf.auto_close = auto_close

 #elf._decoder = None
 #elf._body = None
 #elf._fp = None
 #elf._original_response = original_response
 #elf._fp_bytes_read = 0
 #elf.msg = msg
 #elf._request_url = request_url

 #f body and isinstance(body, (six.string_types, bytes)):
 #elf._body = body

 #elf._pool = pool
 #elf._connection = connection

 #f hasattr(body, "read"):
 #elf._fp = body

        # Are we using the chunked-style of transfer encoding?
 #elf.chunked = False
 #elf.chunk_left = None
 #r_enc = self.headers.get("transfer-encoding", "").lower()
        # Don't incur the penalty of creating a list and then discarding it
 #ncodings = (enc.strip() for enc in tr_enc.split(","))
 #f "chunked" in encodings:
 #elf.chunked = True

        # Determine length of response
 #elf.length_remaining = self._init_length(request_method)

        # If requested, preload the body.
 #f preload_content and not self._body:
 #elf._body = self.read(decode_content=decode_content)

 #ef get_redirect_location(self):
 #""
 #hould we redirect and where to?

 #returns: Truthy redirect location string if we got a redirect status
 #ode and valid location. ``None`` if redirect status and no
 #ocation. ``False`` if not a redirect status code.
 #""
 #f self.status in self.REDIRECT_STATUSES:
 #eturn self.headers.get("location")

 #eturn False

 #ef release_conn(self):
 #f not self._pool or not self._connection:
 #eturn

 #elf._pool._put_conn(self._connection)
 #elf._connection = None

 #ef drain_conn(self):
 #""
 #ead and discard any remaining HTTP response data in the response connection.

 #nread data in the HTTPResponse connection blocks the connection from being released back to the pool.
 #""
 #ry:
 #elf.read()
 #xcept (HTTPError, SocketError, BaseSSLError, HTTPException):
 #ass

 #property
 #ef data(self):
        # For backwards-compat with earlier urllib3 0.4 and earlier.
 #f self._body:
 #eturn self._body

 #f self._fp:
 #eturn self.read(cache_content=True)

 #property
 #ef connection(self):
 #eturn self._connection

 #ef isclosed(self):
 #eturn is_fp_closed(self._fp)

 #ef tell(self):
 #""
 #btain the number of bytes pulled over the wire so far. May differ from
 #he amount of content returned by :meth:``urllib3.response.HTTPResponse.read``
 #f bytes are encoded on the wire (e.g, compressed).
 #""
 #eturn self._fp_bytes_read

 #ef _init_length(self, request_method):
 #""
 #et initial length value for Response content if available.
 #""
 #ength = self.headers.get("content-length")

 #f length is not None:
 #f self.chunked:
                # This Response will fail with an IncompleteRead if it can't be
                # received as chunked. This method falls back to attempt reading
                # the response before raising an exception.
 #og.warning(
 #Received response with both Content-Length and "
 #Transfer-Encoding set. This is expressly forbidden "
 #by RFC 7230 sec 3.3.2. Ignoring Content-Length and "
 #attempting to process response as Transfer-Encoding: "
 #chunked."
 #
 #eturn None

 #ry:
                # RFC 7230 section 3.3.2 specifies multiple content lengths can
                # be sent in a single Content-Length header
                # (e.g. Content-Length: 42, 42). This line ensures the values
                # are all valid ints and that as long as the `set` length is 1,
                # all values are the same. Otherwise, the header is invalid.
 #engths = set([int(val) for val in length.split(",")])
 #f len(lengths) > 1:
 #aise InvalidHeader(
 #Content-Length contained multiple "
 #unmatching values (%s)" % length
 #
 #ength = lengths.pop()
 #xcept ValueError:
 #ength = None
 #lse:
 #f length < 0:
 #ength = None

        # Convert status to int for comparison
        # In some cases, httplib returns a status of "_UNKNOWN"
 #ry:
 #tatus = int(self.status)
 #xcept ValueError:
 #tatus = 0

        # Check for responses that shouldn't include a body
 #f status in (204, 304) or 100 <= status < 200 or request_method == "HEAD":
 #ength = 0

 #eturn length

 #ef _init_decoder(self):
 #""
 #et-up the _decoder attribute if necessary.
 #""
        # Note: content-encoding value should be case-insensitive, per RFC 7230
        # Section 3.2
 #ontent_encoding = self.headers.get("content-encoding", "").lower()
 #f self._decoder is None:
 #f content_encoding in self.CONTENT_DECODERS:
 #elf._decoder = _get_decoder(content_encoding)
 #lif "," in content_encoding:
 #ncodings = [
 #.strip()
 #or e in content_encoding.split(",")
 #f e.strip() in self.CONTENT_DECODERS
 #
 #f len(encodings):
 #elf._decoder = _get_decoder(content_encoding)

 #ECODER_ERROR_CLASSES = (IOError, zlib.error)
 #f brotli is not None:
 #ECODER_ERROR_CLASSES += (brotli.error,)

 #ef _decode(self, data, decode_content, flush_decoder):
 #""
 #ecode the data passed in and potentially flush the decoder.
 #""
 #f not decode_content:
 #eturn data

 #ry:
 #f self._decoder:
 #ata = self._decoder.decompress(data)
 #xcept self.DECODER_ERROR_CLASSES as e:
 #ontent_encoding = self.headers.get("content-encoding", "").lower()
 #aise DecodeError(
 #Received response with content-encoding: %s, but "
 #failed to decode it." % content_encoding,
 #,
 #
 #f flush_decoder:
 #ata += self._flush_decoder()

 #eturn data

 #ef _flush_decoder(self):
 #""
 #lushes the decoder. Should only be called if the decoder is actually
 #eing used.
 #""
 #f self._decoder:
 #uf = self._decoder.decompress(b"")
 #eturn buf + self._decoder.flush()

 #eturn b""

 #contextmanager
 #ef _error_catcher(self):
 #""
 #atch low-level python exceptions, instead re-raising urllib3
 #ariants, so that low-level exceptions are not leaked in the
 #igh-level api.

 #n exit, release the connection back to the pool.
 #""
 #lean_exit = False

 #ry:
 #ry:
 #ield

 #xcept SocketTimeout:
                # FIXME: Ideally we'd like to include the url in the ReadTimeoutError but
                # there is yet no clean way to get at it from this context.
 #aise ReadTimeoutError(self._pool, None, "Read timed out.")

 #xcept BaseSSLError as e:
                # FIXME: Is there a better way to differentiate between SSLErrors?
 #f "read operation timed out" not in str(e):
                    # SSL errors related to framing/MAC get wrapped and reraised here
 #aise SSLError(e)

 #aise ReadTimeoutError(self._pool, None, "Read timed out.")

 #xcept (HTTPException, SocketError) as e:
                # This includes IncompleteRead.
 #aise ProtocolError("Connection broken: %r" % e, e)

            # If no exception is thrown, we should avoid cleaning up
            # unnecessarily.
 #lean_exit = True
 #inally:
            # If we didn't terminate cleanly, we need to throw away our
            # connection.
 #f not clean_exit:
                # The response may not be closed but we're not going to use it
                # anymore so close it now to ensure that the connection is
                # released back to the pool.
 #f self._original_response:
 #elf._original_response.close()

                # Closing the response may not actually be sufficient to close
                # everything, so if we have a hold of the connection close that
                # too.
 #f self._connection:
 #elf._connection.close()

            # If we hold the original response but it's closed now, we should
            # return the connection back to the pool.
 #f self._original_response and self._original_response.isclosed():
 #elf.release_conn()

 #ef _fp_read(self, amt):
 #""
 #ead a response with the thought that reading the number of bytes
 #arger than can fit in a 32-bit int at a time via SSL in some
 #nown cases leads to an overflow error that has to be prevented
 #f `amt` or `self.length_remaining` indicate that a problem may
 #appen.

 #he known cases:
 # 3.8 <= CPython < 3.9.7 because of a bug
 #ttps://github.com/urllib3/urllib3/issues/2513#issuecomment-1152559900.
 # urllib3 injected with pyOpenSSL-backed SSL-support.
 # CPython < 3.10 only when `amt` does not fit 32-bit int.
 #""
 #ssert self._fp
 #_int_max = 2 ** 31 - 1
 #f (
 #
 #amt and amt > c_int_max)
 #r (self.length_remaining and self.length_remaining > c_int_max)
 #
 #nd not util.IS_SECURETRANSPORT
 #nd (util.IS_PYOPENSSL or sys.version_info < (3, 10))
 #:
 #uffer = io.BytesIO()
            # Besides `max_chunk_amt` being a maximum chunk size, it
            # affects memory overhead of reading a response by this
            # method in CPython.
            # `c_int_max` equal to 2 GiB - 1 byte is the actual maximum
            # chunk size that does not lead to an overflow error, but
            # 256 MiB is a compromise.
 #ax_chunk_amt = 2 ** 28
 #hile amt is None or amt != 0:
 #f amt is not None:
 #hunk_amt = min(amt, max_chunk_amt)
 #mt -= chunk_amt
 #lse:
 #hunk_amt = max_chunk_amt
 #ata = self._fp.read(chunk_amt)
 #f not data:
 #reak
 #uffer.write(data)
 #el data  # to reduce peak memory usage by `max_chunk_amt`.
 #eturn buffer.getvalue()
 #lse:
            # StringIO doesn't like amt=None
 #eturn self._fp.read(amt) if amt is not None else self._fp.read()

 #ef read(self, amt=None, decode_content=None, cache_content=False):
 #""
 #imilar to :meth:`http.client.HTTPResponse.read`, but with two additional
 #arameters: ``decode_content`` and ``cache_content``.

 #param amt:
 #ow much of the content to read. If specified, caching is skipped
 #ecause it doesn't make sense to cache partial content as the full
 #esponse.

 #param decode_content:
 #f True, will attempt to decode the body based on the
 #content-encoding' header.

 #param cache_content:
 #f True, will save the returned data such that the same result is
 #eturned despite of the state of the underlying file object. This
 #s useful if you want the ``.data`` property to continue working
 #fter having ``.read()`` the file object. (Overridden if ``amt`` is
 #et.)
 #""
 #elf._init_decoder()
 #f decode_content is None:
 #ecode_content = self.decode_content

 #f self._fp is None:
 #eturn

 #lush_decoder = False
 #p_closed = getattr(self._fp, "closed", False)

 #ith self._error_catcher():
 #ata = self._fp_read(amt) if not fp_closed else b""
 #f amt is None:
 #lush_decoder = True
 #lse:
 #ache_content = False
 #f (
 #mt != 0 and not data
 #:  # Platform-specific: Buggy versions of Python.
                    # Close the connection when no data is returned
                    #
                    # This is redundant to what httplib/http.client _should_
                    # already do.  However, versions of python released before
                    # December 15, 2012 (http://bugs.python.org/issue16298) do
                    # not properly close the connection in all cases. There is
                    # no harm in redundantly calling close.
 #elf._fp.close()
 #lush_decoder = True
 #f self.enforce_content_length and self.length_remaining not in (
 #,
 #one,
 #:
                        # This is an edge case that httplib failed to cover due
                        # to concerns of backward compatibility. We're
                        # addressing it here to make sure IncompleteRead is
                        # raised during streaming, so all calls with incorrect
                        # Content-Length are caught.
 #aise IncompleteRead(self._fp_bytes_read, self.length_remaining)

 #f data:
 #elf._fp_bytes_read += len(data)
 #f self.length_remaining is not None:
 #elf.length_remaining -= len(data)

 #ata = self._decode(data, decode_content, flush_decoder)

 #f cache_content:
 #elf._body = data

 #eturn data

 #ef stream(self, amt=2 ** 16, decode_content=None):
 #""
 # generator wrapper for the read() method. A call will block until
 #`amt`` bytes have been read from the connection or until the
 #onnection is closed.

 #param amt:
 #ow much of the content to read. The generator will return up to
 #uch data per iteration, but may return less. This is particularly
 #ikely when using compressed data. However, the empty string will
 #ever be returned.

 #param decode_content:
 #f True, will attempt to decode the body based on the
 #content-encoding' header.
 #""
 #f self.chunked and self.supports_chunked_reads():
 #or line in self.read_chunked(amt, decode_content=decode_content):
 #ield line
 #lse:
 #hile not is_fp_closed(self._fp):
 #ata = self.read(amt=amt, decode_content=decode_content)

 #f data:
 #ield data

 #classmethod
 #ef from_httplib(ResponseCls, r, **response_kw):
 #""
 #iven an :class:`http.client.HTTPResponse` instance ``r``, return a
 #orresponding :class:`urllib3.response.HTTPResponse` object.

 #emaining parameters are passed to the HTTPResponse constructor, along
 #ith ``original_response=r``.
 #""
 #eaders = r.msg

 #f not isinstance(headers, HTTPHeaderDict):
 #f six.PY2:
                # Python 2.7
 #eaders = HTTPHeaderDict.from_httplib(headers)
 #lse:
 #eaders = HTTPHeaderDict(headers.items())

        # HTTPResponse objects in Python 3 don't have a .strict attribute
 #trict = getattr(r, "strict", 0)
 #esp = ResponseCls(
 #ody=r,
 #eaders=headers,
 #tatus=r.status,
 #ersion=r.version,
 #eason=r.reason,
 #trict=strict,
 #riginal_response=r,
 #*response_kw
 #
 #eturn resp

    # Backwards-compatibility methods for http.client.HTTPResponse
 #ef getheaders(self):
 #arnings.warn(
 #HTTPResponse.getheaders() is deprecated and will be removed "
 #in urllib3 v2.1.0. Instead access HTTPResponse.headers directly.",
 #ategory=DeprecationWarning,
 #tacklevel=2,
 #
 #eturn self.headers

 #ef getheader(self, name, default=None):
 #arnings.warn(
 #HTTPResponse.getheader() is deprecated and will be removed "
 #in urllib3 v2.1.0. Instead use HTTPResponse.headers.get(name, default).",
 #ategory=DeprecationWarning,
 #tacklevel=2,
 #
 #eturn self.headers.get(name, default)

    # Backwards compatibility for http.cookiejar
 #ef info(self):
 #eturn self.headers

    # Overrides from io.IOBase
 #ef close(self):
 #f not self.closed:
 #elf._fp.close()

 #f self._connection:
 #elf._connection.close()

 #f not self.auto_close:
 #o.IOBase.close(self)

 #property
 #ef closed(self):
 #f not self.auto_close:
 #eturn io.IOBase.closed.__get__(self)
 #lif self._fp is None:
 #eturn True
 #lif hasattr(self._fp, "isclosed"):
 #eturn self._fp.isclosed()
 #lif hasattr(self._fp, "closed"):
 #eturn self._fp.closed
 #lse:
 #eturn True

 #ef fileno(self):
 #f self._fp is None:
 #aise IOError("HTTPResponse has no file to get a fileno from")
 #lif hasattr(self._fp, "fileno"):
 #eturn self._fp.fileno()
 #lse:
 #aise IOError(
 #The file-like object this HTTPResponse is wrapped "
 #around has no file descriptor"
 #

 #ef flush(self):
 #f (
 #elf._fp is not None
 #nd hasattr(self._fp, "flush")
 #nd not getattr(self._fp, "closed", False)
 #:
 #eturn self._fp.flush()

 #ef readable(self):
        # This method is required for `io` module compatibility.
 #eturn True

 #ef readinto(self, b):
        # This method is required for `io` module compatibility.
 #emp = self.read(len(b))
 #f len(temp) == 0:
 #eturn 0
 #lse:
 #[: len(temp)] = temp
 #eturn len(temp)

 #ef supports_chunked_reads(self):
 #""
 #hecks if the underlying file-like object looks like a
 #class:`http.client.HTTPResponse` object. We do this by testing for
 #he fp attribute. If it is present we assume it returns raw chunks as
 #rocessed by read_chunked().
 #""
 #eturn hasattr(self._fp, "fp")

 #ef _update_chunk_length(self):
        # First, we'll figure out length of a chunk and then
        # we'll try to read it from socket.
 #f self.chunk_left is not None:
 #eturn
 #ine = self._fp.fp.readline()
 #ine = line.split(b";", 1)[0]
 #ry:
 #elf.chunk_left = int(line, 16)
 #xcept ValueError:
            # Invalid chunked protocol response, abort.
 #elf.close()
 #aise InvalidChunkLength(self, line)

 #ef _handle_chunk(self, amt):
 #eturned_chunk = None
 #f amt is None:
 #hunk = self._fp._safe_read(self.chunk_left)
 #eturned_chunk = chunk
 #elf._fp._safe_read(2)  # Toss the CRLF at the end of the chunk.
 #elf.chunk_left = None
 #lif amt < self.chunk_left:
 #alue = self._fp._safe_read(amt)
 #elf.chunk_left = self.chunk_left - amt
 #eturned_chunk = value
 #lif amt == self.chunk_left:
 #alue = self._fp._safe_read(amt)
 #elf._fp._safe_read(2)  # Toss the CRLF at the end of the chunk.
 #elf.chunk_left = None
 #eturned_chunk = value
 #lse:  # amt > self.chunk_left
 #eturned_chunk = self._fp._safe_read(self.chunk_left)
 #elf._fp._safe_read(2)  # Toss the CRLF at the end of the chunk.
 #elf.chunk_left = None
 #eturn returned_chunk

 #ef read_chunked(self, amt=None, decode_content=None):
 #""
 #imilar to :meth:`HTTPResponse.read`, but with an additional
 #arameter: ``decode_content``.

 #param amt:
 #ow much of the content to read. If specified, caching is skipped
 #ecause it doesn't make sense to cache partial content as the full
 #esponse.

 #param decode_content:
 #f True, will attempt to decode the body based on the
 #content-encoding' header.
 #""
 #elf._init_decoder()
        # FIXME: Rewrite this method and make it a class with a better structured logic.
 #f not self.chunked:
 #aise ResponseNotChunked(
 #Response is not chunked. "
 #Header 'transfer-encoding: chunked' is missing."
 #
 #f not self.supports_chunked_reads():
 #aise BodyNotHttplibCompatible(
 #Body should be http.client.HTTPResponse like. "
 #It should have have an fp attribute which returns raw chunks."
 #

 #ith self._error_catcher():
            # Don't bother reading the body of a HEAD request.
 #f self._original_response and is_response_to_head(self._original_response):
 #elf._original_response.close()
 #eturn

            # If a response is already read and closed
            # then return immediately.
 #f self._fp.fp is None:
 #eturn

 #hile True:
 #elf._update_chunk_length()
 #f self.chunk_left == 0:
 #reak
 #hunk = self._handle_chunk(amt)
 #ecoded = self._decode(
 #hunk, decode_content=decode_content, flush_decoder=False
 #
 #f decoded:
 #ield decoded

 #f decode_content:
                # On CPython and PyPy, we should never need to flush the
                # decoder. However, on Jython we *might* need to, so
                # lets defensively do it anyway.
 #ecoded = self._flush_decoder()
 #f decoded:  # Platform-specific: Jython.
 #ield decoded

            # Chunk content ends with \r\n: discard it.
 #hile True:
 #ine = self._fp.fp.readline()
 #f not line:
                    # Some sites may not end with '\r\n'.
 #reak
 #f line == b"\r\n":
 #reak

            # We read everything; close the "file".
 #f self._original_response:
 #elf._original_response.close()

 #ef geturl(self):
 #""
 #eturns the URL that was the source of this response.
 #f the request that generated this response redirected, this method
 #ill return the final redirect location.
 #""
 #f self.retries is not None and len(self.retries.history):
 #eturn self.retries.history[-1].redirect_location
 #lse:
 #eturn self._request_url

 #ef __iter__(self):
 #uffer = []
 #or chunk in self.stream(decode_content=True):
 #f b"\n" in chunk:
 #hunk = chunk.split(b"\n")
 #ield b"".join(buffer) + chunk[0] + b"\n"
 #or x in chunk[1:-1]:
 #ield x + b"\n"
 #f chunk[-1]:
 #uffer = [chunk[-1]]
 #lse:
 #uffer = []
 #lse:
 #uffer.append(chunk)
 #f buffer:
 #ield b"".join(buffer)
