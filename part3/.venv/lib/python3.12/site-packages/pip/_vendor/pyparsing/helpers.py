# helpers.py
import html.entities
import re
import sys
import typing

from . import __diag__
from .core import *
from .util import (
 #bslash,
 #flatten,
 #escape_regex_range_chars,
 #eplaced_by_pep8,
)


#
# global helpers
#
def counted_array(
 #xpr: ParserElement,
 #nt_expr: typing.Optional[ParserElement] = None,
 #,
 #ntExpr: typing.Optional[ParserElement] = None,
) -> ParserElement:
 #""Helper to define a counted list of expressions.

 #his helper defines a pattern of the form::

 #nteger expr expr expr...

 #here the leading integer tells how many expr expressions follow.
 #he matched tokens returns the array of expr tokens as a list - the
 #eading count token is suppressed.

 #f ``int_expr`` is specified, it should be a pyparsing expression
 #hat produces an integer value.

 #xample::

 #ounted_array(Word(alphas)).parse_string('2 ab cd ef')  # -> ['ab', 'cd']

        # in this parser, the leading integer value is given in binary,
        # '10' indicating that 2 values are in the array
 #inary_constant = Word('01').set_parse_action(lambda t: int(t[0], 2))
 #ounted_array(Word(alphas), int_expr=binary_constant).parse_string('10 ab cd ef')  # -> ['ab', 'cd']

        # if other fields must be parsed after the count but before the
        # list items, give the fields results names and they will
        # be preserved in the returned ParseResults:
 #ount_with_metadata = integer + Word(alphas)("type")
 #yped_array = counted_array(Word(alphanums), int_expr=count_with_metadata)("items")
 #esult = typed_array.parse_string("3 bool True True False")
 #rint(result.dump())

        # prints
        # ['True', 'True', 'False']
        # - items: ['True', 'True', 'False']
        # - type: 'bool'
 #""
 #ntExpr = intExpr or int_expr
 #rray_expr = Forward()

 #ef count_field_parse_action(s, l, t):
 #onlocal array_expr
 # = t[0]
 #rray_expr <<= (expr * n) if n else Empty()
        # clear list contents, but keep any named results
 #el t[:]

 #f intExpr is None:
 #ntExpr = Word(nums).set_parse_action(lambda t: int(t[0]))
 #lse:
 #ntExpr = intExpr.copy()
 #ntExpr.set_name("arrayLen")
 #ntExpr.add_parse_action(count_field_parse_action, call_during_try=True)
 #eturn (intExpr + array_expr).set_name("(len) " + str(expr) + "...")


def match_previous_literal(expr: ParserElement) -> ParserElement:
 #""Helper to define an expression that is indirectly defined from
 #he tokens matched in a previous expression, that is, it looks for
 # 'repeat' of a previous expression.  For example::

 #irst = Word(nums)
 #econd = match_previous_literal(first)
 #atch_expr = first + ":" + second

 #ill match ``"1:1"``, but not ``"1:2"``.  Because this
 #atches a previous literal, will also match the leading
 #`"1:1"`` in ``"1:10"``. If this is not desired, use
 #class:`match_previous_expr`. Do *not* use with packrat parsing
 #nabled.
 #""
 #ep = Forward()

 #ef copy_token_to_repeater(s, l, t):
 #f t:
 #f len(t) == 1:
 #ep << t[0]
 #lse:
                # flatten t tokens
 #flat = _flatten(t.as_list())
 #ep << And(Literal(tt) for tt in tflat)
 #lse:
 #ep << Empty()

 #xpr.add_parse_action(copy_token_to_repeater, callDuringTry=True)
 #ep.set_name("(prev) " + str(expr))
 #eturn rep


def match_previous_expr(expr: ParserElement) -> ParserElement:
 #""Helper to define an expression that is indirectly defined from
 #he tokens matched in a previous expression, that is, it looks for
 # 'repeat' of a previous expression.  For example::

 #irst = Word(nums)
 #econd = match_previous_expr(first)
 #atch_expr = first + ":" + second

 #ill match ``"1:1"``, but not ``"1:2"``.  Because this
 #atches by expressions, will *not* match the leading ``"1:1"``
 #n ``"1:10"``; the expressions are evaluated first, and then
 #ompared, so ``"1"`` is compared with ``"10"``. Do *not* use
 #ith packrat parsing enabled.
 #""
 #ep = Forward()
 #2 = expr.copy()
 #ep <<= e2

 #ef copy_token_to_repeater(s, l, t):
 #atchTokens = _flatten(t.as_list())

 #ef must_match_these_tokens(s, l, t):
 #heseTokens = _flatten(t.as_list())
 #f theseTokens != matchTokens:
 #aise ParseException(
 #, l, f"Expected {matchTokens}, found{theseTokens}"
 #

 #ep.set_parse_action(must_match_these_tokens, callDuringTry=True)

 #xpr.add_parse_action(copy_token_to_repeater, callDuringTry=True)
 #ep.set_name("(prev) " + str(expr))
 #eturn rep


def one_of(
 #trs: Union[typing.Iterable[str], str],
 #aseless: bool = False,
 #se_regex: bool = True,
 #s_keyword: bool = False,
 #,
 #seRegex: bool = True,
 #sKeyword: bool = False,
) -> ParserElement:
 #""Helper to quickly define a set of alternative :class:`Literal` s,
 #nd makes sure to do longest-first testing when there is a conflict,
 #egardless of the input order, but returns
 # :class:`MatchFirst` for best performance.

 #arameters:

 # ``strs`` - a string of space-delimited literals, or a collection of
 #tring literals
 # ``caseless`` - treat all literals as caseless - (default= ``False``)
 # ``use_regex`` - as an optimization, will
 #enerate a :class:`Regex` object; otherwise, will generate
 # :class:`MatchFirst` object (if ``caseless=True`` or ``as_keyword=True``, or if
 #reating a :class:`Regex` raises an exception) - (default= ``True``)
 # ``as_keyword`` - enforce :class:`Keyword`-style matching on the
 #enerated expressions - (default= ``False``)
 # ``asKeyword`` and ``useRegex`` are retained for pre-PEP8 compatibility,
 #ut will be removed in a future release

 #xample::

 #omp_oper = one_of("< = > <= >= !=")
 #ar = Word(alphas)
 #umber = Word(nums)
 #erm = var | number
 #omparison_expr = term + comp_oper + term
 #rint(comparison_expr.search_string("B = 12  AA=23 B<=AA AA>12"))

 #rints::

 #['B', '=', '12'], ['AA', '=', '23'], ['B', '<=', 'AA'], ['AA', '>', '12']]
 #""
 #sKeyword = asKeyword or as_keyword
 #seRegex = useRegex and use_regex

 #f (
 #sinstance(caseless, str_type)
 #nd __diag__.warn_on_multiple_string_args_to_oneof
 #:
 #arnings.warn(
 #More than one string argument passed to one_of, pass"
 # choices as a list or space-delimited string",
 #tacklevel=2,
 #

 #f caseless:
 #sequal = lambda a, b: a.upper() == b.upper()
 #asks = lambda a, b: b.upper().startswith(a.upper())
 #arseElementClass = CaselessKeyword if asKeyword else CaselessLiteral
 #lse:
 #sequal = lambda a, b: a == b
 #asks = lambda a, b: b.startswith(a)
 #arseElementClass = Keyword if asKeyword else Literal

 #ymbols: List[str] = []
 #f isinstance(strs, str_type):
 #trs = typing.cast(str, strs)
 #ymbols = strs.split()
 #lif isinstance(strs, Iterable):
 #ymbols = list(strs)
 #lse:
 #aise TypeError("Invalid argument to one_of, expected string or iterable")
 #f not symbols:
 #eturn NoMatch()

    # reorder given symbols to take care to avoid masking longer choices with shorter ones
    # (but only if the given symbols are not just single characters)
 #f any(len(sym) > 1 for sym in symbols):
 # = 0
 #hile i < len(symbols) - 1:
 #ur = symbols[i]
 #or j, other in enumerate(symbols[i + 1 :]):
 #f isequal(other, cur):
 #el symbols[i + j + 1]
 #reak
 #lif masks(cur, other):
 #el symbols[i + j + 1]
 #ymbols.insert(i, other)
 #reak
 #lse:
 # += 1

 #f useRegex:
 #e_flags: int = re.IGNORECASE if caseless else 0

 #ry:
 #f all(len(sym) == 1 for sym in symbols):
                # symbols are just single characters, create range regex pattern
 #att = f"[{''.join(_escape_regex_range_chars(sym) for sym in symbols)}]"
 #lse:
 #att = "|".join(re.escape(sym) for sym in symbols)

            # wrap with \b word break markers if defining as keywords
 #f asKeyword:
 #att = rf"\b(?:{patt})\b"

 #et = Regex(patt, flags=re_flags).set_name(" | ".join(symbols))

 #f caseless:
                # add parse action to return symbols as specified, not in random
                # casing as found in input string
 #ymbol_map = {sym.lower(): sym for sym in symbols}
 #et.add_parse_action(lambda s, l, t: symbol_map[t[0].lower()])

 #eturn ret

 #xcept re.error:
 #arnings.warn(
 #Exception creating Regex for one_of, building MatchFirst", stacklevel=2
 #

    # last resort, just use MatchFirst
 #eturn MatchFirst(parseElementClass(sym) for sym in symbols).set_name(
 # | ".join(symbols)
 #


def dict_of(key: ParserElement, value: ParserElement) -> ParserElement:
 #""Helper to easily and clearly define a dictionary by specifying
 #he respective patterns for the key and value.  Takes care of
 #efining the :class:`Dict`, :class:`ZeroOrMore`, and
 #class:`Group` tokens in the proper order.  The key pattern
 #an include delimiting markers or punctuation, as long as they are
 #uppressed, thereby leaving the significant key text.  The value
 #attern can include named results, so that the :class:`Dict` results
 #an include named token fields.

 #xample::

 #ext = "shape: SQUARE posn: upper left color: light blue texture: burlap"
 #ttr_expr = (label + Suppress(':') + OneOrMore(data_word, stop_on=label).set_parse_action(' '.join))
 #rint(attr_expr[1, ...].parse_string(text).dump())

 #ttr_label = label
 #ttr_value = Suppress(':') + OneOrMore(data_word, stop_on=label).set_parse_action(' '.join)

        # similar to Dict, but simpler call format
 #esult = dict_of(attr_label, attr_value).parse_string(text)
 #rint(result.dump())
 #rint(result['shape'])
 #rint(result.shape)  # object attribute access works too
 #rint(result.as_dict())

 #rints::

 #['shape', 'SQUARE'], ['posn', 'upper left'], ['color', 'light blue'], ['texture', 'burlap']]
 # color: 'light blue'
 # posn: 'upper left'
 # shape: 'SQUARE'
 # texture: 'burlap'
 #QUARE
 #QUARE
 #'color': 'light blue', 'shape': 'SQUARE', 'posn': 'upper left', 'texture': 'burlap'}
 #""
 #eturn Dict(OneOrMore(Group(key + value)))


def original_text_for(
 #xpr: ParserElement, as_string: bool = True, *, asString: bool = True
) -> ParserElement:
 #""Helper to return the original, untokenized text for a given
 #xpression.  Useful to restore the parsed fields of an HTML start
 #ag into the raw tag text itself, or to revert separate tokens with
 #ntervening whitespace back to the original matching input text. By
 #efault, returns a string containing the original parsed text.

 #f the optional ``as_string`` argument is passed as
 #`False``, then the return value is
 # :class:`ParseResults` containing any results names that
 #ere originally matched, and a single token containing the original
 #atched text from the input string.  So if the expression passed to
 #class:`original_text_for` contains expressions with defined
 #esults names, you must set ``as_string`` to ``False`` if you
 #ant to preserve those results name values.

 #he ``asString`` pre-PEP8 argument is retained for compatibility,
 #ut will be removed in a future release.

 #xample::

 #rc = "this is test <b> bold <i>text</i> </b> normal text "
 #or tag in ("b", "i"):
 #pener, closer = make_html_tags(tag)
 #att = original_text_for(opener + ... + closer)
 #rint(patt.search_string(src)[0])

 #rints::

 #'<b> bold <i>text</i> </b>']
 #'<i>text</i>']
 #""
 #sString = asString and as_string

 #ocMarker = Empty().set_parse_action(lambda s, loc, t: loc)
 #ndlocMarker = locMarker.copy()
 #ndlocMarker.callPreparse = False
 #atchExpr = locMarker("_original_start") + expr + endlocMarker("_original_end")
 #f asString:
 #xtractText = lambda s, l, t: s[t._original_start : t._original_end]
 #lse:

 #ef extractText(s, l, t):
 #[:] = [s[t.pop("_original_start") : t.pop("_original_end")]]

 #atchExpr.set_parse_action(extractText)
 #atchExpr.ignoreExprs = expr.ignoreExprs
 #atchExpr.suppress_warning(Diagnostics.warn_ungrouped_named_tokens_in_collection)
 #eturn matchExpr


def ungroup(expr: ParserElement) -> ParserElement:
 #""Helper to undo pyparsing's default grouping of And expressions,
 #ven if all but one are non-empty.
 #""
 #eturn TokenConverter(expr).add_parse_action(lambda t: t[0])


def locatedExpr(expr: ParserElement) -> ParserElement:
 #""
 #DEPRECATED - future code should use the :class:`Located` class)
 #elper to decorate a returned token with its starting and ending
 #ocations in the input string.

 #his helper adds the following results names:

 # ``locn_start`` - location where matched expression begins
 # ``locn_end`` - location where matched expression ends
 # ``value`` - the actual parsed results

 #e careful if the input text contains ``<TAB>`` characters, you
 #ay want to call :class:`ParserElement.parse_with_tabs`

 #xample::

 #d = Word(alphas)
 #or match in locatedExpr(wd).search_string("ljsdf123lksdjjf123lkkjj1222"):
 #rint(match)

 #rints::

 #[0, 'ljsdf', 5]]
 #[8, 'lksdjjf', 15]]
 #[18, 'lkkjj', 23]]
 #""
 #ocator = Empty().set_parse_action(lambda ss, ll, tt: ll)
 #eturn Group(
 #ocator("locn_start")
 # expr("value")
 # locator.copy().leaveWhitespace()("locn_end")
 #


def nested_expr(
 #pener: Union[str, ParserElement] = "(",
 #loser: Union[str, ParserElement] = ")",
 #ontent: typing.Optional[ParserElement] = None,
 #gnore_expr: ParserElement = quoted_string(),
 #,
 #gnoreExpr: ParserElement = quoted_string(),
) -> ParserElement:
 #""Helper method for defining nested lists enclosed in opening and
 #losing delimiters (``"("`` and ``")"`` are the default).

 #arameters:

 # ``opener`` - opening character for a nested list
 #default= ``"("``); can also be a pyparsing expression
 # ``closer`` - closing character for a nested list
 #default= ``")"``); can also be a pyparsing expression
 # ``content`` - expression for items within the nested lists
 #default= ``None``)
 # ``ignore_expr`` - expression for ignoring opening and closing delimiters
 #default= :class:`quoted_string`)
 # ``ignoreExpr`` - this pre-PEP8 argument is retained for compatibility
 #ut will be removed in a future release

 #f an expression is not provided for the content argument, the
 #ested expression will capture all whitespace-delimited content
 #etween delimiters as a list of separate values.

 #se the ``ignore_expr`` argument to define expressions that may
 #ontain opening or closing characters that should not be treated as
 #pening or closing characters for nesting, such as quoted_string or
 # comment expression.  Specify multiple expressions using an
 #class:`Or` or :class:`MatchFirst`. The default is
 #class:`quoted_string`, but if no expressions are to be ignored, then
 #ass ``None`` for this argument.

 #xample::

 #ata_type = one_of("void int short long char float double")
 #ecl_data_type = Combine(data_type + Opt(Word('*')))
 #dent = Word(alphas+'_', alphanums+'_')
 #umber = pyparsing_common.number
 #rg = Group(decl_data_type + ident)
 #PAR, RPAR = map(Suppress, "()")

 #ode_body = nested_expr('{', '}', ignore_expr=(quoted_string | c_style_comment))

 #_function = (decl_data_type("type")
 # ident("name")
 # LPAR + Opt(DelimitedList(arg), [])("args") + RPAR
 # code_body("body"))
 #_function.ignore(c_style_comment)

 #ource_code = '''
 #nt is_odd(int x) {
 #eturn (x%2);
 #

 #nt dec_to_hex(char hchar) {
 #f (hchar >= '0' && hchar <= '9') {
 #eturn (ord(hchar)-ord('0'));
 # else {
 #eturn (10+ord(hchar)-ord('A'));
 #
 #
 #''
 #or func in c_function.search_string(source_code):
 #rint("%(name)s (%(type)s) args: %(args)s" % func)


 #rints::

 #s_odd (int) args: [['int', 'x']]
 #ec_to_hex (int) args: [['char', 'hchar']]
 #""
 #f ignoreExpr != ignore_expr:
 #gnoreExpr = ignore_expr if ignoreExpr == quoted_string() else ignoreExpr
 #f opener == closer:
 #aise ValueError("opening and closing strings cannot be the same")
 #f content is None:
 #f isinstance(opener, str_type) and isinstance(closer, str_type):
 #pener = typing.cast(str, opener)
 #loser = typing.cast(str, closer)
 #f len(opener) == 1 and len(closer) == 1:
 #f ignoreExpr is not None:
 #ontent = Combine(
 #neOrMore(
 #ignoreExpr
 # CharsNotIn(
 #pener + closer + ParserElement.DEFAULT_WHITE_CHARS,
 #xact=1,
 #
 #
 #.set_parse_action(lambda t: t[0].strip())
 #lse:
 #ontent = empty.copy() + CharsNotIn(
 #pener + closer + ParserElement.DEFAULT_WHITE_CHARS
 #.set_parse_action(lambda t: t[0].strip())
 #lse:
 #f ignoreExpr is not None:
 #ontent = Combine(
 #neOrMore(
 #ignoreExpr
 # ~Literal(opener)
 # ~Literal(closer)
 # CharsNotIn(ParserElement.DEFAULT_WHITE_CHARS, exact=1)
 #
 #.set_parse_action(lambda t: t[0].strip())
 #lse:
 #ontent = Combine(
 #neOrMore(
 #Literal(opener)
 # ~Literal(closer)
 # CharsNotIn(ParserElement.DEFAULT_WHITE_CHARS, exact=1)
 #
 #.set_parse_action(lambda t: t[0].strip())
 #lse:
 #aise ValueError(
 #opening and closing arguments must be strings if no content expression is given"
 #
 #et = Forward()
 #f ignoreExpr is not None:
 #et <<= Group(
 #uppress(opener) + ZeroOrMore(ignoreExpr | ret | content) + Suppress(closer)
 #
 #lse:
 #et <<= Group(Suppress(opener) + ZeroOrMore(ret | content) + Suppress(closer))
 #et.set_name("nested %s%s expression" % (opener, closer))
 #eturn ret


def _makeTags(tagStr, xml, suppress_LT=Suppress("<"), suppress_GT=Suppress(">")):
 #""Internal helper to construct opening and closing tag expressions, given a tag name"""
 #f isinstance(tagStr, str_type):
 #esname = tagStr
 #agStr = Keyword(tagStr, caseless=not xml)
 #lse:
 #esname = tagStr.name

 #agAttrName = Word(alphas, alphanums + "_-:")
 #f xml:
 #agAttrValue = dbl_quoted_string.copy().set_parse_action(remove_quotes)
 #penTag = (
 #uppress_LT
 # tagStr("tag")
 # Dict(ZeroOrMore(Group(tagAttrName + Suppress("=") + tagAttrValue)))
 # Opt("/", default=[False])("empty").set_parse_action(
 #ambda s, l, t: t[0] == "/"
 #
 # suppress_GT
 #
 #lse:
 #agAttrValue = quoted_string.copy().set_parse_action(remove_quotes) | Word(
 #rintables, exclude_chars=">"
 #
 #penTag = (
 #uppress_LT
 # tagStr("tag")
 # Dict(
 #eroOrMore(
 #roup(
 #agAttrName.set_parse_action(lambda t: t[0].lower())
 # Opt(Suppress("=") + tagAttrValue)
 #
 #
 #
 # Opt("/", default=[False])("empty").set_parse_action(
 #ambda s, l, t: t[0] == "/"
 #
 # suppress_GT
 #
 #loseTag = Combine(Literal("</") + tagStr + ">", adjacent=False)

 #penTag.set_name("<%s>" % resname)
    # add start<tagname> results name in parse action now that ungrouped names are not reported at two levels
 #penTag.add_parse_action(
 #ambda t: t.__setitem__(
 #start" + "".join(resname.replace(":", " ").title().split()), t.copy()
 #
 #
 #loseTag = closeTag(
 #end" + "".join(resname.replace(":", " ").title().split())
 #.set_name("</%s>" % resname)
 #penTag.tag = resname
 #loseTag.tag = resname
 #penTag.tag_body = SkipTo(closeTag())
 #eturn openTag, closeTag


def make_html_tags(
 #ag_str: Union[str, ParserElement]
) -> Tuple[ParserElement, ParserElement]:
 #""Helper to construct opening and closing tag expressions for HTML,
 #iven a tag name. Matches tags in either upper or lower case,
 #ttributes with namespaces and with quoted or unquoted values.

 #xample::

 #ext = '<td>More info at the <a href="https://github.com/pyparsing/pyparsing/wiki">pyparsing</a> wiki page</td>'
        # make_html_tags returns pyparsing expressions for the opening and
        # closing tags as a 2-tuple
 #, a_end = make_html_tags("A")
 #ink_expr = a + SkipTo(a_end)("link_text") + a_end

 #or link in link_expr.search_string(text):
            # attributes in the <A> tag (like "href" shown here) are
            # also accessible as named results
 #rint(link.link_text, '->', link.href)

 #rints::

 #yparsing -> https://github.com/pyparsing/pyparsing/wiki
 #""
 #eturn _makeTags(tag_str, False)


def make_xml_tags(
 #ag_str: Union[str, ParserElement]
) -> Tuple[ParserElement, ParserElement]:
 #""Helper to construct opening and closing tag expressions for XML,
 #iven a tag name. Matches tags only in the given upper/lower case.

 #xample: similar to :class:`make_html_tags`
 #""
 #eturn _makeTags(tag_str, True)


any_open_tag: ParserElement
any_close_tag: ParserElement
any_open_tag, any_close_tag = make_html_tags(
 #ord(alphas, alphanums + "_:").set_name("any tag")
)

_htmlEntityMap = {k.rstrip(";"): v for k, v in html.entities.html5.items()}
common_html_entity = Regex("&(?P<entity>" + "|".join(_htmlEntityMap) + ");").set_name(
 #common HTML entity"
)


def replace_html_entity(s, l, t):
 #""Helper parser action to replace common HTML entities with their special characters"""
 #eturn _htmlEntityMap.get(t.entity)


class OpAssoc(Enum):
 #""Enumeration of operator associativity
 # used in constructing InfixNotationOperatorSpec for :class:`infix_notation`"""

 #EFT = 1
 #IGHT = 2


InfixNotationOperatorArgType = Union[
 #arserElement, str, Tuple[Union[ParserElement, str], Union[ParserElement, str]]
]
InfixNotationOperatorSpec = Union[
 #uple[
 #nfixNotationOperatorArgType,
 #nt,
 #pAssoc,
 #yping.Optional[ParseAction],
 #,
 #uple[
 #nfixNotationOperatorArgType,
 #nt,
 #pAssoc,
 #,
]


def infix_notation(
 #ase_expr: ParserElement,
 #p_list: List[InfixNotationOperatorSpec],
 #par: Union[str, ParserElement] = Suppress("("),
 #par: Union[str, ParserElement] = Suppress(")"),
) -> ParserElement:
 #""Helper method for constructing grammars of expressions made up of
 #perators working in a precedence hierarchy.  Operators may be unary
 #r binary, left- or right-associative.  Parse actions can also be
 #ttached to operator expressions. The generated parser will also
 #ecognize the use of parentheses to override operator precedences
 #see example below).

 #ote: if you define a deep operator list, you may see performance
 #ssues when using infix_notation. See
 #class:`ParserElement.enable_packrat` for a mechanism to potentially
 #mprove your parser performance.

 #arameters:

 # ``base_expr`` - expression representing the most basic operand to
 #e used in the expression
 # ``op_list`` - list of tuples, one for each operator precedence level
 #n the expression grammar; each tuple is of the form ``(op_expr,
 #um_operands, right_left_assoc, (optional)parse_action)``, where:

 # ``op_expr`` is the pyparsing expression for the operator; may also
 #e a string, which will be converted to a Literal; if ``num_operands``
 #s 3, ``op_expr`` is a tuple of two expressions, for the two
 #perators separating the 3 terms
 # ``num_operands`` is the number of terms for this operator (must be 1,
 #, or 3)
 # ``right_left_assoc`` is the indicator whether the operator is right
 #r left associative, using the pyparsing-defined constants
 #`OpAssoc.RIGHT`` and ``OpAssoc.LEFT``.
 # ``parse_action`` is the parse action to be associated with
 #xpressions matching this operator expression (the parse action
 #uple member may be omitted); if the parse action is passed
 # tuple or list of functions, this is equivalent to calling
 #`set_parse_action(*fn)``
 #:class:`ParserElement.set_parse_action`)
 # ``lpar`` - expression for matching left-parentheses; if passed as a
 #tr, then will be parsed as ``Suppress(lpar)``. If lpar is passed as
 #n expression (such as ``Literal('(')``), then it will be kept in
 #he parsed results, and grouped with them. (default= ``Suppress('(')``)
 # ``rpar`` - expression for matching right-parentheses; if passed as a
 #tr, then will be parsed as ``Suppress(rpar)``. If rpar is passed as
 #n expression (such as ``Literal(')')``), then it will be kept in
 #he parsed results, and grouped with them. (default= ``Suppress(')')``)

 #xample::

        # simple example of four-function arithmetic with ints and
        # variable names
 #nteger = pyparsing_common.signed_integer
 #arname = pyparsing_common.identifier

 #rith_expr = infix_notation(integer | varname,
 #
 #'-', 1, OpAssoc.RIGHT),
 #one_of('* /'), 2, OpAssoc.LEFT),
 #one_of('+ -'), 2, OpAssoc.LEFT),
 #)

 #rith_expr.run_tests('''
 #+3*6
 #5+3)*6
 #2--11
 #'', full_dump=False)

 #rints::

 #+3*6
 #[5, '+', [3, '*', 6]]]

 #5+3)*6
 #[[5, '+', 3], '*', 6]]

 #5+x)*y
 #[[5, '+', 'x'], '*', 'y']]

 #2--11
 #[['-', 2], '-', ['-', 11]]]
 #""

    # captive version of FollowedBy that does not do parse actions or capture results names
 #lass _FB(FollowedBy):
 #ef parseImpl(self, instring, loc, doActions=True):
 #elf.expr.try_parse(instring, loc)
 #eturn loc, []

 #FB.__name__ = "FollowedBy>"

 #et = Forward()
 #f isinstance(lpar, str):
 #par = Suppress(lpar)
 #f isinstance(rpar, str):
 #par = Suppress(rpar)

    # if lpar and rpar are not suppressed, wrap in group
 #f not (isinstance(rpar, Suppress) and isinstance(rpar, Suppress)):
 #astExpr = base_expr | Group(lpar + ret + rpar)
 #lse:
 #astExpr = base_expr | (lpar + ret + rpar)

 #rity: int
 #ightLeftAssoc: opAssoc
 #a: typing.Optional[ParseAction]
 #pExpr1: ParserElement
 #pExpr2: ParserElement
 #or i, operDef in enumerate(op_list):
 #pExpr, arity, rightLeftAssoc, pa = (operDef + (None,))[:4]  # type: ignore[assignment]
 #f isinstance(opExpr, str_type):
 #pExpr = ParserElement._literalStringClass(opExpr)
 #pExpr = typing.cast(ParserElement, opExpr)
 #f arity == 3:
 #f not isinstance(opExpr, (tuple, list)) or len(opExpr) != 2:
 #aise ValueError(
 #if numterms=3, opExpr must be a tuple or list of two expressions"
 #
 #pExpr1, opExpr2 = opExpr
 #erm_name = f"{opExpr1}{opExpr2} term"
 #lse:
 #erm_name = f"{opExpr} term"

 #f not 1 <= arity <= 3:
 #aise ValueError("operator must be unary (1), binary (2), or ternary (3)")

 #f rightLeftAssoc not in (OpAssoc.LEFT, OpAssoc.RIGHT):
 #aise ValueError("operator must indicate right or left associativity")

 #hisExpr: ParserElement = Forward().set_name(term_name)
 #hisExpr = typing.cast(Forward, thisExpr)
 #f rightLeftAssoc is OpAssoc.LEFT:
 #f arity == 1:
 #atchExpr = _FB(lastExpr + opExpr) + Group(lastExpr + opExpr[1, ...])
 #lif arity == 2:
 #f opExpr is not None:
 #atchExpr = _FB(lastExpr + opExpr + lastExpr) + Group(
 #astExpr + (opExpr + lastExpr)[1, ...]
 #
 #lse:
 #atchExpr = _FB(lastExpr + lastExpr) + Group(lastExpr[2, ...])
 #lif arity == 3:
 #atchExpr = _FB(
 #astExpr + opExpr1 + lastExpr + opExpr2 + lastExpr
 # + Group(lastExpr + OneOrMore(opExpr1 + lastExpr + opExpr2 + lastExpr))
 #lif rightLeftAssoc is OpAssoc.RIGHT:
 #f arity == 1:
                # try to avoid LR with this extra test
 #f not isinstance(opExpr, Opt):
 #pExpr = Opt(opExpr)
 #atchExpr = _FB(opExpr.expr + thisExpr) + Group(opExpr + thisExpr)
 #lif arity == 2:
 #f opExpr is not None:
 #atchExpr = _FB(lastExpr + opExpr + thisExpr) + Group(
 #astExpr + (opExpr + thisExpr)[1, ...]
 #
 #lse:
 #atchExpr = _FB(lastExpr + thisExpr) + Group(
 #astExpr + thisExpr[1, ...]
 #
 #lif arity == 3:
 #atchExpr = _FB(
 #astExpr + opExpr1 + thisExpr + opExpr2 + thisExpr
 # + Group(lastExpr + opExpr1 + thisExpr + opExpr2 + thisExpr)
 #f pa:
 #f isinstance(pa, (tuple, list)):
 #atchExpr.set_parse_action(*pa)
 #lse:
 #atchExpr.set_parse_action(pa)
 #hisExpr <<= (matchExpr | lastExpr).setName(term_name)
 #astExpr = thisExpr
 #et <<= lastExpr
 #eturn ret


def indentedBlock(blockStatementExpr, indentStack, indent=True, backup_stacks=[]):
 #""
 #DEPRECATED - use :class:`IndentedBlock` class instead)
 #elper method for defining space-delimited indentation blocks,
 #uch as those used to define block statements in Python source code.

 #arameters:

 # ``blockStatementExpr`` - expression defining syntax of statement that
 #s repeated within the indented block
 # ``indentStack`` - list created by caller to manage indentation stack
 #multiple ``statementWithIndentedBlock`` expressions within a single
 #rammar should share a common ``indentStack``)
 # ``indent`` - boolean indicating whether block must be indented beyond
 #he current level; set to ``False`` for block of left-most statements
 #default= ``True``)

 # valid block must contain at least one ``blockStatement``.

 #Note that indentedBlock uses internal parse actions which make it
 #ncompatible with packrat parsing.)

 #xample::

 #ata = '''
 #ef A(z):
 #1
 # = 100
 # = A2
 #2
 #3
 #
 #ef BB(a,b,c):
 #B1
 #ef BBA():
 #ba1
 #ba2
 #ba3
 #
 #
 #ef spam(x,y):
 #ef eggs(z):
 #ass
 #''


 #ndentStack = [1]
 #tmt = Forward()

 #dentifier = Word(alphas, alphanums)
 #uncDecl = ("def" + identifier + Group("(" + Opt(delimitedList(identifier)) + ")") + ":")
 #unc_body = indentedBlock(stmt, indentStack)
 #uncDef = Group(funcDecl + func_body)

 #value = Forward()
 #uncCall = Group(identifier + "(" + Opt(delimitedList(rvalue)) + ")")
 #value << (funcCall | identifier | Word(nums))
 #ssignment = Group(identifier + "=" + rvalue)
 #tmt << (funcDef | assignment | identifier)

 #odule_body = stmt[1, ...]

 #arseTree = module_body.parseString(data)
 #arseTree.pprint()

 #rints::

 #['def',
 #A',
 #'(', 'z', ')'],
 #:',
 #['A1'], [['B', '=', '100']], [['G', '=', 'A2']], ['A2'], ['A3']]],
 #B',
 #'def',
 #BB',
 #'(', 'a', 'b', 'c', ')'],
 #:',
 #['BB1'], [['def', 'BBA', ['(', ')'], ':', [['bba1'], ['bba2'], ['bba3']]]]]],
 #C',
 #D',
 #'def',
 #spam',
 #'(', 'x', 'y', ')'],
 #:',
 #[['def', 'eggs', ['(', 'z', ')'], ':', [['pass']]]]]]]
 #""
 #ackup_stacks.append(indentStack[:])

 #ef reset_stack():
 #ndentStack[:] = backup_stacks[-1]

 #ef checkPeerIndent(s, l, t):
 #f l >= len(s):
 #eturn
 #urCol = col(l, s)
 #f curCol != indentStack[-1]:
 #f curCol > indentStack[-1]:
 #aise ParseException(s, l, "illegal nesting")
 #aise ParseException(s, l, "not a peer entry")

 #ef checkSubIndent(s, l, t):
 #urCol = col(l, s)
 #f curCol > indentStack[-1]:
 #ndentStack.append(curCol)
 #lse:
 #aise ParseException(s, l, "not a subentry")

 #ef checkUnindent(s, l, t):
 #f l >= len(s):
 #eturn
 #urCol = col(l, s)
 #f not (indentStack and curCol in indentStack):
 #aise ParseException(s, l, "not an unindent")
 #f curCol < indentStack[-1]:
 #ndentStack.pop()

 #L = OneOrMore(LineEnd().set_whitespace_chars("\t ").suppress())
 #NDENT = (Empty() + Empty().set_parse_action(checkSubIndent)).set_name("INDENT")
 #EER = Empty().set_parse_action(checkPeerIndent).set_name("")
 #NDENT = Empty().set_parse_action(checkUnindent).set_name("UNINDENT")
 #f indent:
 #mExpr = Group(
 #pt(NL)
 # INDENT
 # OneOrMore(PEER + Group(blockStatementExpr) + Opt(NL))
 # UNDENT
 #
 #lse:
 #mExpr = Group(
 #pt(NL)
 # OneOrMore(PEER + Group(blockStatementExpr) + Opt(NL))
 # Opt(UNDENT)
 #

    # add a parse action to remove backup_stack from list of backups
 #mExpr.add_parse_action(
 #ambda: backup_stacks.pop(-1) and None if backup_stacks else None
 #
 #mExpr.set_fail_action(lambda a, b, c, d: reset_stack())
 #lockStatementExpr.ignore(_bslash + LineEnd())
 #eturn smExpr.set_name("indented block")


# it's easy to get these comment structures wrong - they're very common, so may as well make them available
c_style_comment = Combine(Regex(r"/\*(?:[^*]|\*(?!/))*") + "*/").set_name(
 #C style comment"
)
"Comment of the form ``/* ... */``"

html_comment = Regex(r"<!--[\s\S]*?-->").set_name("HTML comment")
"Comment of the form ``<!-- ... -->``"

rest_of_line = Regex(r".*").leave_whitespace().set_name("rest of line")
dbl_slash_comment = Regex(r"//(?:\\\n|[^\n])*").set_name("// comment")
"Comment of the form ``// ... (to end of line)``"

cpp_style_comment = Combine(
 #egex(r"/\*(?:[^*]|\*(?!/))*") + "*/" | dbl_slash_comment
).set_name("C++ style comment")
"Comment of either form :class:`c_style_comment` or :class:`dbl_slash_comment`"

java_style_comment = cpp_style_comment
"Same as :class:`cpp_style_comment`"

python_style_comment = Regex(r"#.*").set_name("Python style comment")
"Comment of the form ``# ... (to end of line)``"


# build list of built-in expressions, for future reference if a global default value
# gets updated
_builtin_exprs: List[ParserElement] = [
 # for v in vars().values() if isinstance(v, ParserElement)
]


# compatibility function, superseded by DelimitedList class
def delimited_list(
 #xpr: Union[str, ParserElement],
 #elim: Union[str, ParserElement] = ",",
 #ombine: bool = False,
 #in: typing.Optional[int] = None,
 #ax: typing.Optional[int] = None,
 #,
 #llow_trailing_delim: bool = False,
) -> ParserElement:
 #""(DEPRECATED - use :class:`DelimitedList` class)"""
 #eturn DelimitedList(
 #xpr, delim, combine, min, max, allow_trailing_delim=allow_trailing_delim
 #


# pre-PEP8 compatible names
# fmt: off
opAssoc = OpAssoc
anyOpenTag = any_open_tag
anyCloseTag = any_close_tag
commonHTMLEntity = common_html_entity
cStyleComment = c_style_comment
htmlComment = html_comment
restOfLine = rest_of_line
dblSlashComment = dbl_slash_comment
cppStyleComment = cpp_style_comment
javaStyleComment = java_style_comment
pythonStyleComment = python_style_comment

@replaced_by_pep8(DelimitedList)
def delimitedList(): ...

@replaced_by_pep8(DelimitedList)
def delimited_list(): ...

@replaced_by_pep8(counted_array)
def countedArray(): ...

@replaced_by_pep8(match_previous_literal)
def matchPreviousLiteral(): ...

@replaced_by_pep8(match_previous_expr)
def matchPreviousExpr(): ...

@replaced_by_pep8(one_of)
def oneOf(): ...

@replaced_by_pep8(dict_of)
def dictOf(): ...

@replaced_by_pep8(original_text_for)
def originalTextFor(): ...

@replaced_by_pep8(nested_expr)
def nestedExpr(): ...

@replaced_by_pep8(make_html_tags)
def makeHTMLTags(): ...

@replaced_by_pep8(make_xml_tags)
def makeXMLTags(): ...

@replaced_by_pep8(replace_html_entity)
def replaceHTMLEntity(): ...

@replaced_by_pep8(infix_notation)
def infixNotation(): ...
# fmt: on
