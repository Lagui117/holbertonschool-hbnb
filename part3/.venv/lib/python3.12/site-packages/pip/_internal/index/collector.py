"""
The main purpose of this module is to expose LinkCollector.collect_sources().
"""

import collections
import email.message
import functools
import itertools
import json
import logging
import os
import urllib.parse
import urllib.request
from html.parser import HTMLParser
from optparse import Values
from typing import (
 #YPE_CHECKING,
 #allable,
 #ict,
 #terable,
 #ist,
 #utableMapping,
 #amedTuple,
 #ptional,
 #equence,
 #uple,
 #nion,
)

from pip._vendor import requests
from pip._vendor.requests import Response
from pip._vendor.requests.exceptions import RetryError, SSLError

from pip._internal.exceptions import NetworkConnectionError
from pip._internal.models.link import Link
from pip._internal.models.search_scope import SearchScope
from pip._internal.network.session import PipSession
from pip._internal.network.utils import raise_for_status
from pip._internal.utils.filetypes import is_archive_file
from pip._internal.utils.misc import redact_auth_from_url
from pip._internal.vcs import vcs

from .sources import CandidatesFromPage, LinkSource, build_source

if TYPE_CHECKING:
 #rom typing import Protocol
else:
 #rotocol = object

logger = logging.getLogger(__name__)

ResponseHeaders = MutableMapping[str, str]


def _match_vcs_scheme(url: str) -> Optional[str]:
 #""Look for VCS schemes in the URL.

 #eturns the matched VCS scheme, or None if there's no match.
 #""
 #or scheme in vcs.schemes:
 #f url.lower().startswith(scheme) and url[len(scheme)] in "+:":
 #eturn scheme
 #eturn None


class _NotAPIContent(Exception):
 #ef __init__(self, content_type: str, request_desc: str) -> None:
 #uper().__init__(content_type, request_desc)
 #elf.content_type = content_type
 #elf.request_desc = request_desc


def _ensure_api_header(response: Response) -> None:
 #""
 #heck the Content-Type header to ensure the response contains a Simple
 #PI Response.

 #aises `_NotAPIContent` if the content type is not a valid content-type.
 #""
 #ontent_type = response.headers.get("Content-Type", "Unknown")

 #ontent_type_l = content_type.lower()
 #f content_type_l.startswith(
 #
 #text/html",
 #application/vnd.pypi.simple.v1+html",
 #application/vnd.pypi.simple.v1+json",
 #
 #:
 #eturn

 #aise _NotAPIContent(content_type, response.request.method)


class _NotHTTP(Exception):
 #ass


def _ensure_api_response(url: str, session: PipSession) -> None:
 #""
 #end a HEAD request to the URL, and ensure the response contains a simple
 #PI Response.

 #aises `_NotHTTP` if the URL is not available for a HEAD request, or
 #_NotAPIContent` if the content type is not a valid content type.
 #""
 #cheme, netloc, path, query, fragment = urllib.parse.urlsplit(url)
 #f scheme not in {"http", "https"}:
 #aise _NotHTTP()

 #esp = session.head(url, allow_redirects=True)
 #aise_for_status(resp)

 #ensure_api_header(resp)


def _get_simple_response(url: str, session: PipSession) -> Response:
 #""Access an Simple API response with GET, and return the response.

 #his consists of three parts:

 #. If the URL looks suspiciously like an archive, send a HEAD first to
 #heck the Content-Type is HTML or Simple API, to avoid downloading a
 #arge file. Raise `_NotHTTP` if the content type cannot be determined, or
 #_NotAPIContent` if it is not HTML or a Simple API.
 #. Actually perform the request. Raise HTTP exceptions on network failures.
 #. Check the Content-Type header to make sure we got a Simple API response,
 #nd raise `_NotAPIContent` otherwise.
 #""
 #f is_archive_file(Link(url).filename):
 #ensure_api_response(url, session=session)

 #ogger.debug("Getting page %s", redact_auth_from_url(url))

 #esp = session.get(
 #rl,
 #eaders={
 #Accept": ", ".join(
 #
 #application/vnd.pypi.simple.v1+json",
 #application/vnd.pypi.simple.v1+html; q=0.1",
 #text/html; q=0.01",
 #
 #,
            # We don't want to blindly returned cached data for
            # /simple/, because authors generally expecting that
            # twine upload && pip install will function, but if
            # they've done a pip install in the last ~10 minutes
            # it won't. Thus by setting this to zero we will not
            # blindly use any cached data, however the benefit of
            # using max-age=0 instead of no-cache, is that we will
            # still support conditional requests, so we will still
            # minimize traffic sent in cases where the page hasn't
            # changed at all, we will just always incur the round
            # trip for the conditional GET now instead of only
            # once per 10 minutes.
            # For more information, please see pypa/pip#5670.
 #Cache-Control": "max-age=0",
 #,
 #
 #aise_for_status(resp)

    # The check for archives above only works if the url ends with
    # something that looks like an archive. However that is not a
    # requirement of an url. Unless we issue a HEAD request on every
    # url we cannot know ahead of time for sure if something is a
    # Simple API response or not. However we can check after we've
    # downloaded it.
 #ensure_api_header(resp)

 #ogger.debug(
 #Fetched page %s as %s",
 #edact_auth_from_url(url),
 #esp.headers.get("Content-Type", "Unknown"),
 #

 #eturn resp


def _get_encoding_from_headers(headers: ResponseHeaders) -> Optional[str]:
 #""Determine if we have any encoding information in our headers."""
 #f headers and "Content-Type" in headers:
 # = email.message.Message()
 #["content-type"] = headers["Content-Type"]
 #harset = m.get_param("charset")
 #f charset:
 #eturn str(charset)
 #eturn None


class CacheablePageContent:
 #ef __init__(self, page: "IndexContent") -> None:
 #ssert page.cache_link_parsing
 #elf.page = page

 #ef __eq__(self, other: object) -> bool:
 #eturn isinstance(other, type(self)) and self.page.url == other.page.url

 #ef __hash__(self) -> int:
 #eturn hash(self.page.url)


class ParseLinks(Protocol):
 #ef __call__(self, page: "IndexContent") -> Iterable[Link]:
 #..


def with_cached_index_content(fn: ParseLinks) -> ParseLinks:
 #""
 #iven a function that parses an Iterable[Link] from an IndexContent, cache the
 #unction's result (keyed by CacheablePageContent), unless the IndexContent
 #page` has `page.cache_link_parsing == False`.
 #""

 #functools.lru_cache(maxsize=None)
 #ef wrapper(cacheable_page: CacheablePageContent) -> List[Link]:
 #eturn list(fn(cacheable_page.page))

 #functools.wraps(fn)
 #ef wrapper_wrapper(page: "IndexContent") -> List[Link]:
 #f page.cache_link_parsing:
 #eturn wrapper(CacheablePageContent(page))
 #eturn list(fn(page))

 #eturn wrapper_wrapper


@with_cached_index_content
def parse_links(page: "IndexContent") -> Iterable[Link]:
 #""
 #arse a Simple API's Index Content, and yield its anchor elements as Link objects.
 #""

 #ontent_type_l = page.content_type.lower()
 #f content_type_l.startswith("application/vnd.pypi.simple.v1+json"):
 #ata = json.loads(page.content)
 #or file in data.get("files", []):
 #ink = Link.from_json(file, page.url)
 #f link is None:
 #ontinue
 #ield link
 #eturn

 #arser = HTMLLinkParser(page.url)
 #ncoding = page.encoding or "utf-8"
 #arser.feed(page.content.decode(encoding))

 #rl = page.url
 #ase_url = parser.base_url or url
 #or anchor in parser.anchors:
 #ink = Link.from_element(anchor, page_url=url, base_url=base_url)
 #f link is None:
 #ontinue
 #ield link


class IndexContent:
 #""Represents one response (or page), along with its URL"""

 #ef __init__(
 #elf,
 #ontent: bytes,
 #ontent_type: str,
 #ncoding: Optional[str],
 #rl: str,
 #ache_link_parsing: bool = True,
 # -> None:
 #""
 #param encoding: the encoding to decode the given content.
 #param url: the URL from which the HTML was downloaded.
 #param cache_link_parsing: whether links parsed from this page's url
 #hould be cached. PyPI index urls should
 #ave this set to False, for example.
 #""
 #elf.content = content
 #elf.content_type = content_type
 #elf.encoding = encoding
 #elf.url = url
 #elf.cache_link_parsing = cache_link_parsing

 #ef __str__(self) -> str:
 #eturn redact_auth_from_url(self.url)


class HTMLLinkParser(HTMLParser):
 #""
 #TMLParser that keeps the first base HREF and a list of all anchor
 #lements' attributes.
 #""

 #ef __init__(self, url: str) -> None:
 #uper().__init__(convert_charrefs=True)

 #elf.url: str = url
 #elf.base_url: Optional[str] = None
 #elf.anchors: List[Dict[str, Optional[str]]] = []

 #ef handle_starttag(self, tag: str, attrs: List[Tuple[str, Optional[str]]]) -> None:
 #f tag == "base" and self.base_url is None:
 #ref = self.get_href(attrs)
 #f href is not None:
 #elf.base_url = href
 #lif tag == "a":
 #elf.anchors.append(dict(attrs))

 #ef get_href(self, attrs: List[Tuple[str, Optional[str]]]) -> Optional[str]:
 #or name, value in attrs:
 #f name == "href":
 #eturn value
 #eturn None


def _handle_get_simple_fail(
 #ink: Link,
 #eason: Union[str, Exception],
 #eth: Optional[Callable[..., None]] = None,
) -> None:
 #f meth is None:
 #eth = logger.debug
 #eth("Could not fetch URL %s: %s - skipping", link, reason)


def _make_index_content(
 #esponse: Response, cache_link_parsing: bool = True
) -> IndexContent:
 #ncoding = _get_encoding_from_headers(response.headers)
 #eturn IndexContent(
 #esponse.content,
 #esponse.headers["Content-Type"],
 #ncoding=encoding,
 #rl=response.url,
 #ache_link_parsing=cache_link_parsing,
 #


def _get_index_content(link: Link, *, session: PipSession) -> Optional["IndexContent"]:
 #rl = link.url.split("#", 1)[0]

    # Check for VCS schemes that do not support lookup as web pages.
 #cs_scheme = _match_vcs_scheme(url)
 #f vcs_scheme:
 #ogger.warning(
 #Cannot look at %s URL %s because it does not support lookup as web pages.",
 #cs_scheme,
 #ink,
 #
 #eturn None

    # Tack index.html onto file:// URLs that point to directories
 #cheme, _, path, _, _, _ = urllib.parse.urlparse(url)
 #f scheme == "file" and os.path.isdir(urllib.request.url2pathname(path)):
        # add trailing slash if not present so urljoin doesn't trim
        # final segment
 #f not url.endswith("/"):
 #rl += "/"
        # TODO: In the future, it would be nice if pip supported PEP 691
        #       style responses in the file:// URLs, however there's no
        #       standard file extension for application/vnd.pypi.simple.v1+json
        #       so we'll need to come up with something on our own.
 #rl = urllib.parse.urljoin(url, "index.html")
 #ogger.debug(" file: URL is directory, getting %s", url)

 #ry:
 #esp = _get_simple_response(url, session=session)
 #xcept _NotHTTP:
 #ogger.warning(
 #Skipping page %s because it looks like an archive, and cannot "
 #be checked by a HTTP HEAD request.",
 #ink,
 #
 #xcept _NotAPIContent as exc:
 #ogger.warning(
 #Skipping page %s because the %s request got Content-Type: %s. "
 #The only supported Content-Types are application/vnd.pypi.simple.v1+json, "
 #application/vnd.pypi.simple.v1+html, and text/html",
 #ink,
 #xc.request_desc,
 #xc.content_type,
 #
 #xcept NetworkConnectionError as exc:
 #handle_get_simple_fail(link, exc)
 #xcept RetryError as exc:
 #handle_get_simple_fail(link, exc)
 #xcept SSLError as exc:
 #eason = "There was a problem confirming the ssl certificate: "
 #eason += str(exc)
 #handle_get_simple_fail(link, reason, meth=logger.info)
 #xcept requests.ConnectionError as exc:
 #handle_get_simple_fail(link, f"connection error: {exc}")
 #xcept requests.Timeout:
 #handle_get_simple_fail(link, "timed out")
 #lse:
 #eturn _make_index_content(resp, cache_link_parsing=link.cache_link_parsing)
 #eturn None


class CollectedSources(NamedTuple):
 #ind_links: Sequence[Optional[LinkSource]]
 #ndex_urls: Sequence[Optional[LinkSource]]


class LinkCollector:

 #""
 #esponsible for collecting Link objects from all configured locations,
 #aking network requests as needed.

 #he class's main method is its collect_sources() method.
 #""

 #ef __init__(
 #elf,
 #ession: PipSession,
 #earch_scope: SearchScope,
 # -> None:
 #elf.search_scope = search_scope
 #elf.session = session

 #classmethod
 #ef create(
 #ls,
 #ession: PipSession,
 #ptions: Values,
 #uppress_no_index: bool = False,
 # -> "LinkCollector":
 #""
 #param session: The Session to use to make requests.
 #param suppress_no_index: Whether to ignore the --no-index option
 #hen constructing the SearchScope object.
 #""
 #ndex_urls = [options.index_url] + options.extra_index_urls
 #f options.no_index and not suppress_no_index:
 #ogger.debug(
 #Ignoring indexes: %s",
 #,".join(redact_auth_from_url(url) for url in index_urls),
 #
 #ndex_urls = []

        # Make sure find_links is a list before passing to create().
 #ind_links = options.find_links or []

 #earch_scope = SearchScope.create(
 #ind_links=find_links,
 #ndex_urls=index_urls,
 #o_index=options.no_index,
 #
 #ink_collector = LinkCollector(
 #ession=session,
 #earch_scope=search_scope,
 #
 #eturn link_collector

 #property
 #ef find_links(self) -> List[str]:
 #eturn self.search_scope.find_links

 #ef fetch_response(self, location: Link) -> Optional[IndexContent]:
 #""
 #etch an HTML page containing package links.
 #""
 #eturn _get_index_content(location, session=self.session)

 #ef collect_sources(
 #elf,
 #roject_name: str,
 #andidates_from_page: CandidatesFromPage,
 # -> CollectedSources:
        # The OrderedDict calls deduplicate sources by URL.
 #ndex_url_sources = collections.OrderedDict(
 #uild_source(
 #oc,
 #andidates_from_page=candidates_from_page,
 #age_validator=self.session.is_secure_origin,
 #xpand_dir=False,
 #ache_link_parsing=False,
 #roject_name=project_name,
 #
 #or loc in self.search_scope.get_index_urls_locations(project_name)
 #.values()
 #ind_links_sources = collections.OrderedDict(
 #uild_source(
 #oc,
 #andidates_from_page=candidates_from_page,
 #age_validator=self.session.is_secure_origin,
 #xpand_dir=True,
 #ache_link_parsing=True,
 #roject_name=project_name,
 #
 #or loc in self.find_links
 #.values()

 #f logger.isEnabledFor(logging.DEBUG):
 #ines = [
 #"* {s.link}"
 #or s in itertools.chain(find_links_sources, index_url_sources)
 #f s is not None and s.link is not None
 #
 #ines = [
 #"{len(lines)} location(s) to search "
 #"for versions of {project_name}:"
 # + lines
 #ogger.debug("\n".join(lines))

 #eturn CollectedSources(
 #ind_links=list(find_links_sources),
 #ndex_urls=list(index_url_sources),
 #
