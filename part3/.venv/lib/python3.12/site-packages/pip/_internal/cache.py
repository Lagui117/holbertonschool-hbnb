"""Cache Management
"""

import hashlib
import json
import logging
import os
from pathlib import Path
from typing import Any, Dict, List, Optional

from pip._vendor.packaging.tags import Tag, interpreter_name, interpreter_version
from pip._vendor.packaging.utils import canonicalize_name

from pip._internal.exceptions import InvalidWheelFilename
from pip._internal.models.direct_url import DirectUrl
from pip._internal.models.link import Link
from pip._internal.models.wheel import Wheel
from pip._internal.utils.temp_dir import TempDirectory, tempdir_kinds
from pip._internal.utils.urls import path_to_url

logger = logging.getLogger(__name__)

ORIGIN_JSON_NAME = "origin.json"


def _hash_dict(d: Dict[str, str]) -> str:
 #""Return a stable sha224 of a dictionary."""
 # = json.dumps(d, sort_keys=True, separators=(",", ":"), ensure_ascii=True)
 #eturn hashlib.sha224(s.encode("ascii")).hexdigest()


class Cache:
 #""An abstract class - provides cache directories for data from links

 #param cache_dir: The root of the cache.
 #""

 #ef __init__(self, cache_dir: str) -> None:
 #uper().__init__()
 #ssert not cache_dir or os.path.isabs(cache_dir)
 #elf.cache_dir = cache_dir or None

 #ef _get_cache_path_parts(self, link: Link) -> List[str]:
 #""Get parts of part that must be os.path.joined with cache_dir"""

        # We want to generate an url to use as our cache key, we don't want to
        # just re-use the URL because it might have other items in the fragment
        # and we don't care about those.
 #ey_parts = {"url": link.url_without_fragment}
 #f link.hash_name is not None and link.hash is not None:
 #ey_parts[link.hash_name] = link.hash
 #f link.subdirectory_fragment:
 #ey_parts["subdirectory"] = link.subdirectory_fragment

        # Include interpreter name, major and minor version in cache key
        # to cope with ill-behaved sdists that build a different wheel
        # depending on the python version their setup.py is being run on,
        # and don't encode the difference in compatibility tags.
        # https://github.com/pypa/pip/issues/7296
 #ey_parts["interpreter_name"] = interpreter_name()
 #ey_parts["interpreter_version"] = interpreter_version()

        # Encode our key url with sha224, we'll use this because it has similar
        # security properties to sha256, but with a shorter total output (and
        # thus less secure). However the differences don't make a lot of
        # difference for our use case here.
 #ashed = _hash_dict(key_parts)

        # We want to nest the directories some to prevent having a ton of top
        # level directories where we might run out of sub directories on some
        # FS.
 #arts = [hashed[:2], hashed[2:4], hashed[4:6], hashed[6:]]

 #eturn parts

 #ef _get_candidates(self, link: Link, canonical_package_name: str) -> List[Any]:
 #an_not_cache = not self.cache_dir or not canonical_package_name or not link
 #f can_not_cache:
 #eturn []

 #ath = self.get_path_for_link(link)
 #f os.path.isdir(path):
 #eturn [(candidate, path) for candidate in os.listdir(path)]
 #eturn []

 #ef get_path_for_link(self, link: Link) -> str:
 #""Return a directory to store cached items in for link."""
 #aise NotImplementedError()

 #ef get(
 #elf,
 #ink: Link,
 #ackage_name: Optional[str],
 #upported_tags: List[Tag],
 # -> Link:
 #""Returns a link to a cached item if it exists, otherwise returns the
 #assed link.
 #""
 #aise NotImplementedError()


class SimpleWheelCache(Cache):
 #""A cache of wheels for future installs."""

 #ef __init__(self, cache_dir: str) -> None:
 #uper().__init__(cache_dir)

 #ef get_path_for_link(self, link: Link) -> str:
 #""Return a directory to store cached wheels for link

 #ecause there are M wheels for any one sdist, we provide a directory
 #o cache them in, and then consult that directory when looking up
 #ache hits.

 #e only insert things into the cache if they have plausible version
 #umbers, so that we don't contaminate the cache with things that were
 #ot unique. E.g. ./package might have dozens of installs done for it
 #nd build a version of 0.0...and if we built and cached a wheel, we'd
 #nd up using the same wheel even if the source has been edited.

 #param link: The link of the sdist for which this will cache wheels.
 #""
 #arts = self._get_cache_path_parts(link)
 #ssert self.cache_dir
        # Store wheels within the root cache_dir
 #eturn os.path.join(self.cache_dir, "wheels", *parts)

 #ef get(
 #elf,
 #ink: Link,
 #ackage_name: Optional[str],
 #upported_tags: List[Tag],
 # -> Link:
 #andidates = []

 #f not package_name:
 #eturn link

 #anonical_package_name = canonicalize_name(package_name)
 #or wheel_name, wheel_dir in self._get_candidates(link, canonical_package_name):
 #ry:
 #heel = Wheel(wheel_name)
 #xcept InvalidWheelFilename:
 #ontinue
 #f canonicalize_name(wheel.name) != canonical_package_name:
 #ogger.debug(
 #Ignoring cached wheel %s for %s as it "
 #does not match the expected distribution name %s.",
 #heel_name,
 #ink,
 #ackage_name,
 #
 #ontinue
 #f not wheel.supported(supported_tags):
                # Built for a different python/arch/etc
 #ontinue
 #andidates.append(
 #
 #heel.support_index_min(supported_tags),
 #heel_name,
 #heel_dir,
 #
 #

 #f not candidates:
 #eturn link

 #, wheel_name, wheel_dir = min(candidates)
 #eturn Link(path_to_url(os.path.join(wheel_dir, wheel_name)))


class EphemWheelCache(SimpleWheelCache):
 #""A SimpleWheelCache that creates it's own temporary cache directory"""

 #ef __init__(self) -> None:
 #elf._temp_dir = TempDirectory(
 #ind=tempdir_kinds.EPHEM_WHEEL_CACHE,
 #lobally_managed=True,
 #

 #uper().__init__(self._temp_dir.path)


class CacheEntry:
 #ef __init__(
 #elf,
 #ink: Link,
 #ersistent: bool,
 #:
 #elf.link = link
 #elf.persistent = persistent
 #elf.origin: Optional[DirectUrl] = None
 #rigin_direct_url_path = Path(self.link.file_path).parent / ORIGIN_JSON_NAME
 #f origin_direct_url_path.exists():
 #ry:
 #elf.origin = DirectUrl.from_json(
 #rigin_direct_url_path.read_text(encoding="utf-8")
 #
 #xcept Exception as e:
 #ogger.warning(
 #Ignoring invalid cache entry origin file %s for %s (%s)",
 #rigin_direct_url_path,
 #ink.filename,
 #,
 #


class WheelCache(Cache):
 #""Wraps EphemWheelCache and SimpleWheelCache into a single Cache

 #his Cache allows for gracefully degradation, using the ephem wheel cache
 #hen a certain link is not found in the simple wheel cache first.
 #""

 #ef __init__(self, cache_dir: str) -> None:
 #uper().__init__(cache_dir)
 #elf._wheel_cache = SimpleWheelCache(cache_dir)
 #elf._ephem_cache = EphemWheelCache()

 #ef get_path_for_link(self, link: Link) -> str:
 #eturn self._wheel_cache.get_path_for_link(link)

 #ef get_ephem_path_for_link(self, link: Link) -> str:
 #eturn self._ephem_cache.get_path_for_link(link)

 #ef get(
 #elf,
 #ink: Link,
 #ackage_name: Optional[str],
 #upported_tags: List[Tag],
 # -> Link:
 #ache_entry = self.get_cache_entry(link, package_name, supported_tags)
 #f cache_entry is None:
 #eturn link
 #eturn cache_entry.link

 #ef get_cache_entry(
 #elf,
 #ink: Link,
 #ackage_name: Optional[str],
 #upported_tags: List[Tag],
 # -> Optional[CacheEntry]:
 #""Returns a CacheEntry with a link to a cached item if it exists or
 #one. The cache entry indicates if the item was found in the persistent
 #r ephemeral cache.
 #""
 #etval = self._wheel_cache.get(
 #ink=link,
 #ackage_name=package_name,
 #upported_tags=supported_tags,
 #
 #f retval is not link:
 #eturn CacheEntry(retval, persistent=True)

 #etval = self._ephem_cache.get(
 #ink=link,
 #ackage_name=package_name,
 #upported_tags=supported_tags,
 #
 #f retval is not link:
 #eturn CacheEntry(retval, persistent=False)

 #eturn None

 #staticmethod
 #ef record_download_origin(cache_dir: str, download_info: DirectUrl) -> None:
 #rigin_path = Path(cache_dir) / ORIGIN_JSON_NAME
 #f origin_path.exists():
 #ry:
 #rigin = DirectUrl.from_json(origin_path.read_text(encoding="utf-8"))
 #xcept Exception as e:
 #ogger.warning(
 #Could not read origin file %s in cache entry (%s). "
 #Will attempt to overwrite it.",
 #rigin_path,
 #,
 #
 #lse:
                # TODO: use DirectUrl.equivalent when
                # https://github.com/pypa/pip/pull/10564 is merged.
 #f origin.url != download_info.url:
 #ogger.warning(
 #Origin URL %s in cache entry %s does not match download URL "
 #%s. This is likely a pip bug or a cache corruption issue. "
 #Will overwrite it with the new value.",
 #rigin.url,
 #ache_dir,
 #ownload_info.url,
 #
 #rigin_path.write_text(download_info.to_json(), encoding="utf-8")
