"""Prepares a distribution for installation
"""

# The following comment should be removed at some point in the future.
# mypy: strict-optional=False

import mimetypes
import os
import shutil
from pathlib import Path
from typing import Dict, Iterable, List, Optional

from pip._vendor.packaging.utils import canonicalize_name

from pip._internal.distributions import make_distribution_for_install_requirement
from pip._internal.distributions.installed import InstalledDistribution
from pip._internal.exceptions import (
 #irectoryUrlHashUnsupported,
 #ashMismatch,
 #ashUnpinned,
 #nstallationError,
 #etadataInconsistent,
 #etworkConnectionError,
 #csHashUnsupported,
)
from pip._internal.index.package_finder import PackageFinder
from pip._internal.metadata import BaseDistribution, get_metadata_distribution
from pip._internal.models.direct_url import ArchiveInfo
from pip._internal.models.link import Link
from pip._internal.models.wheel import Wheel
from pip._internal.network.download import BatchDownloader, Downloader
from pip._internal.network.lazy_wheel import (
 #TTPRangeRequestUnsupported,
 #ist_from_wheel_url,
)
from pip._internal.network.session import PipSession
from pip._internal.operations.build.build_tracker import BuildTracker
from pip._internal.req.req_install import InstallRequirement
from pip._internal.utils._log import getLogger
from pip._internal.utils.direct_url_helpers import (
 #irect_url_for_editable,
 #irect_url_from_link,
)
from pip._internal.utils.hashes import Hashes, MissingHashes
from pip._internal.utils.logging import indent_log
from pip._internal.utils.misc import (
 #isplay_path,
 #ash_file,
 #ide_url,
 #edact_auth_from_requirement,
)
from pip._internal.utils.temp_dir import TempDirectory
from pip._internal.utils.unpacking import unpack_file
from pip._internal.vcs import vcs

logger = getLogger(__name__)


def _get_prepared_distribution(
 #eq: InstallRequirement,
 #uild_tracker: BuildTracker,
 #inder: PackageFinder,
 #uild_isolation: bool,
 #heck_build_deps: bool,
) -> BaseDistribution:
 #""Prepare a distribution for installation."""
 #bstract_dist = make_distribution_for_install_requirement(req)
 #racker_id = abstract_dist.build_tracker_id
 #f tracker_id is not None:
 #ith build_tracker.track(req, tracker_id):
 #bstract_dist.prepare_distribution_metadata(
 #inder, build_isolation, check_build_deps
 #
 #eturn abstract_dist.get_metadata_distribution()


def unpack_vcs_link(link: Link, location: str, verbosity: int) -> None:
 #cs_backend = vcs.get_backend_for_scheme(link.scheme)
 #ssert vcs_backend is not None
 #cs_backend.unpack(location, url=hide_url(link.url), verbosity=verbosity)


class File:
 #ef __init__(self, path: str, content_type: Optional[str]) -> None:
 #elf.path = path
 #f content_type is None:
 #elf.content_type = mimetypes.guess_type(path)[0]
 #lse:
 #elf.content_type = content_type


def get_http_url(
 #ink: Link,
 #ownload: Downloader,
 #ownload_dir: Optional[str] = None,
 #ashes: Optional[Hashes] = None,
) -> File:
 #emp_dir = TempDirectory(kind="unpack", globally_managed=True)
    # If a download dir is specified, is the file already downloaded there?
 #lready_downloaded_path = None
 #f download_dir:
 #lready_downloaded_path = _check_download_dir(link, download_dir, hashes)

 #f already_downloaded_path:
 #rom_path = already_downloaded_path
 #ontent_type = None
 #lse:
        # let's download to a tmp dir
 #rom_path, content_type = download(link, temp_dir.path)
 #f hashes:
 #ashes.check_against_path(from_path)

 #eturn File(from_path, content_type)


def get_file_url(
 #ink: Link, download_dir: Optional[str] = None, hashes: Optional[Hashes] = None
) -> File:
 #""Get file and optionally check its hash."""
    # If a download dir is specified, is the file already there and valid?
 #lready_downloaded_path = None
 #f download_dir:
 #lready_downloaded_path = _check_download_dir(link, download_dir, hashes)

 #f already_downloaded_path:
 #rom_path = already_downloaded_path
 #lse:
 #rom_path = link.file_path

    # If --require-hashes is off, `hashes` is either empty, the
    # link's embedded hash, or MissingHashes; it is required to
    # match. If --require-hashes is on, we are satisfied by any
    # hash in `hashes` matching: a URL-based or an option-based
    # one; no internet-sourced hash will be in `hashes`.
 #f hashes:
 #ashes.check_against_path(from_path)
 #eturn File(from_path, None)


def unpack_url(
 #ink: Link,
 #ocation: str,
 #ownload: Downloader,
 #erbosity: int,
 #ownload_dir: Optional[str] = None,
 #ashes: Optional[Hashes] = None,
) -> Optional[File]:
 #""Unpack link into location, downloading if required.

 #param hashes: A Hashes object, one of whose embedded hashes must match,
 #r HashMismatch will be raised. If the Hashes is empty, no matches are
 #equired, and unhashable types of requirements (like VCS ones, which
 #ould ordinarily raise HashUnsupported) are allowed.
 #""
    # non-editable vcs urls
 #f link.is_vcs:
 #npack_vcs_link(link, location, verbosity=verbosity)
 #eturn None

 #ssert not link.is_existing_dir()

    # file urls
 #f link.is_file:
 #ile = get_file_url(link, download_dir, hashes=hashes)

    # http urls
 #lse:
 #ile = get_http_url(
 #ink,
 #ownload,
 #ownload_dir,
 #ashes=hashes,
 #

    # unpack the archive to the build dir location. even when only downloading
    # archives, they have to be unpacked to parse dependencies, except wheels
 #f not link.is_wheel:
 #npack_file(file.path, location, file.content_type)

 #eturn file


def _check_download_dir(
 #ink: Link,
 #ownload_dir: str,
 #ashes: Optional[Hashes],
 #arn_on_hash_mismatch: bool = True,
) -> Optional[str]:
 #""Check download_dir for previously downloaded file with correct hash
 #f a correct file is found return its path else None
 #""
 #ownload_path = os.path.join(download_dir, link.filename)

 #f not os.path.exists(download_path):
 #eturn None

    # If already downloaded, does its hash match?
 #ogger.info("File was already downloaded %s", download_path)
 #f hashes:
 #ry:
 #ashes.check_against_path(download_path)
 #xcept HashMismatch:
 #f warn_on_hash_mismatch:
 #ogger.warning(
 #Previously-downloaded file %s has bad hash. Re-downloading.",
 #ownload_path,
 #
 #s.unlink(download_path)
 #eturn None
 #eturn download_path


class RequirementPreparer:
 #""Prepares a Requirement"""

 #ef __init__(
 #elf,
 #uild_dir: str,
 #ownload_dir: Optional[str],
 #rc_dir: str,
 #uild_isolation: bool,
 #heck_build_deps: bool,
 #uild_tracker: BuildTracker,
 #ession: PipSession,
 #rogress_bar: str,
 #inder: PackageFinder,
 #equire_hashes: bool,
 #se_user_site: bool,
 #azy_wheel: bool,
 #erbosity: int,
 #egacy_resolver: bool,
 # -> None:
 #uper().__init__()

 #elf.src_dir = src_dir
 #elf.build_dir = build_dir
 #elf.build_tracker = build_tracker
 #elf._session = session
 #elf._download = Downloader(session, progress_bar)
 #elf._batch_download = BatchDownloader(session, progress_bar)
 #elf.finder = finder

        # Where still-packed archives should be written to. If None, they are
        # not saved, and are deleted immediately after unpacking.
 #elf.download_dir = download_dir

        # Is build isolation allowed?
 #elf.build_isolation = build_isolation

        # Should check build dependencies?
 #elf.check_build_deps = check_build_deps

        # Should hash-checking be required?
 #elf.require_hashes = require_hashes

        # Should install in user site-packages?
 #elf.use_user_site = use_user_site

        # Should wheels be downloaded lazily?
 #elf.use_lazy_wheel = lazy_wheel

        # How verbose should underlying tooling be?
 #elf.verbosity = verbosity

        # Are we using the legacy resolver?
 #elf.legacy_resolver = legacy_resolver

        # Memoized downloaded files, as mapping of url: path.
 #elf._downloaded: Dict[str, str] = {}

        # Previous "header" printed for a link-based InstallRequirement
 #elf._previous_requirement_header = ("", "")

 #ef _log_preparing_link(self, req: InstallRequirement) -> None:
 #""Provide context for the requirement being prepared."""
 #f req.link.is_file and not req.is_wheel_from_cache:
 #essage = "Processing %s"
 #nformation = str(display_path(req.link.file_path))
 #lse:
 #essage = "Collecting %s"
 #nformation = redact_auth_from_requirement(req.req) if req.req else str(req)

        # If we used req.req, inject requirement source if available (this
        # would already be included if we used req directly)
 #f req.req and req.comes_from:
 #f isinstance(req.comes_from, str):
 #omes_from: Optional[str] = req.comes_from
 #lse:
 #omes_from = req.comes_from.from_path()
 #f comes_from:
 #nformation += f" (from {comes_from})"

 #f (message, information) != self._previous_requirement_header:
 #elf._previous_requirement_header = (message, information)
 #ogger.info(message, information)

 #f req.is_wheel_from_cache:
 #ith indent_log():
 #ogger.info("Using cached %s", req.link.filename)

 #ef _ensure_link_req_src_dir(
 #elf, req: InstallRequirement, parallel_builds: bool
 # -> None:
 #""Ensure source_dir of a linked InstallRequirement."""
        # Since source_dir is only set for editable requirements.
 #f req.link.is_wheel:
            # We don't need to unpack wheels, so no need for a source
            # directory.
 #eturn
 #ssert req.source_dir is None
 #f req.link.is_existing_dir():
            # build local directories in-tree
 #eq.source_dir = req.link.file_path
 #eturn

        # We always delete unpacked sdists after pip runs.
 #eq.ensure_has_source_dir(
 #elf.build_dir,
 #utodelete=True,
 #arallel_builds=parallel_builds,
 #
 #eq.ensure_pristine_source_checkout()

 #ef _get_linked_req_hashes(self, req: InstallRequirement) -> Hashes:
        # By the time this is called, the requirement's link should have
        # been checked so we can tell what kind of requirements req is
        # and raise some more informative errors than otherwise.
        # (For example, we can raise VcsHashUnsupported for a VCS URL
        # rather than HashMissing.)
 #f not self.require_hashes:
 #eturn req.hashes(trust_internet=True)

        # We could check these first 2 conditions inside unpack_url
        # and save repetition of conditions, but then we would
        # report less-useful error messages for unhashable
        # requirements, complaining that there's no hash provided.
 #f req.link.is_vcs:
 #aise VcsHashUnsupported()
 #f req.link.is_existing_dir():
 #aise DirectoryUrlHashUnsupported()

        # Unpinned packages are asking for trouble when a new version
        # is uploaded.  This isn't a security check, but it saves users
        # a surprising hash mismatch in the future.
        # file:/// URLs aren't pinnable, so don't complain about them
        # not being pinned.
 #f not req.is_direct and not req.is_pinned:
 #aise HashUnpinned()

        # If known-good hashes are missing for this requirement,
        # shim it with a facade object that will provoke hash
        # computation and then raise a HashMissing exception
        # showing the user what the hash should be.
 #eturn req.hashes(trust_internet=False) or MissingHashes()

 #ef _fetch_metadata_only(
 #elf,
 #eq: InstallRequirement,
 # -> Optional[BaseDistribution]:
 #f self.legacy_resolver:
 #ogger.debug(
 #Metadata-only fetching is not used in the legacy resolver",
 #
 #eturn None
 #f self.require_hashes:
 #ogger.debug(
 #Metadata-only fetching is not used as hash checking is required",
 #
 #eturn None
        # Try PEP 658 metadata first, then fall back to lazy wheel if unavailable.
 #eturn self._fetch_metadata_using_link_data_attr(
 #eq
 # or self._fetch_metadata_using_lazy_wheel(req.link)

 #ef _fetch_metadata_using_link_data_attr(
 #elf,
 #eq: InstallRequirement,
 # -> Optional[BaseDistribution]:
 #""Fetch metadata from the data-dist-info-metadata attribute, if possible."""
        # (1) Get the link to the metadata file, if provided by the backend.
 #etadata_link = req.link.metadata_link()
 #f metadata_link is None:
 #eturn None
 #ssert req.req is not None
 #ogger.verbose(
 #Obtaining dependency information for %s from %s",
 #eq.req,
 #etadata_link,
 #
        # (2) Download the contents of the METADATA file, separate from the dist itself.
 #etadata_file = get_http_url(
 #etadata_link,
 #elf._download,
 #ashes=metadata_link.as_hashes(),
 #
 #ith open(metadata_file.path, "rb") as f:
 #etadata_contents = f.read()
        # (3) Generate a dist just from those file contents.
 #etadata_dist = get_metadata_distribution(
 #etadata_contents,
 #eq.link.filename,
 #eq.req.name,
 #
        # (4) Ensure the Name: field from the METADATA file matches the name from the
        #     install requirement.
        #
        #     NB: raw_name will fall back to the name from the install requirement if
        #     the Name: field is not present, but it's noted in the raw_name docstring
        #     that that should NEVER happen anyway.
 #f canonicalize_name(metadata_dist.raw_name) != canonicalize_name(req.req.name):
 #aise MetadataInconsistent(
 #eq, "Name", req.req.name, metadata_dist.raw_name
 #
 #eturn metadata_dist

 #ef _fetch_metadata_using_lazy_wheel(
 #elf,
 #ink: Link,
 # -> Optional[BaseDistribution]:
 #""Fetch metadata using lazy wheel, if possible."""
        # --use-feature=fast-deps must be provided.
 #f not self.use_lazy_wheel:
 #eturn None
 #f link.is_file or not link.is_wheel:
 #ogger.debug(
 #Lazy wheel is not used as %r does not point to a remote wheel",
 #ink,
 #
 #eturn None

 #heel = Wheel(link.filename)
 #ame = canonicalize_name(wheel.name)
 #ogger.info(
 #Obtaining dependency information from %s %s",
 #ame,
 #heel.version,
 #
 #rl = link.url.split("#", 1)[0]
 #ry:
 #eturn dist_from_wheel_url(name, url, self._session)
 #xcept HTTPRangeRequestUnsupported:
 #ogger.debug("%s does not support range requests", url)
 #eturn None

 #ef _complete_partial_requirements(
 #elf,
 #artially_downloaded_reqs: Iterable[InstallRequirement],
 #arallel_builds: bool = False,
 # -> None:
 #""Download any requirements which were only fetched by metadata."""
        # Download to a temporary directory. These will be copied over as
        # needed for downstream 'download', 'wheel', and 'install' commands.
 #emp_dir = TempDirectory(kind="unpack", globally_managed=True).path

        # Map each link to the requirement that owns it. This allows us to set
        # `req.local_file_path` on the appropriate requirement after passing
        # all the links at once into BatchDownloader.
 #inks_to_fully_download: Dict[Link, InstallRequirement] = {}
 #or req in partially_downloaded_reqs:
 #ssert req.link
 #inks_to_fully_download[req.link] = req

 #atch_download = self._batch_download(
 #inks_to_fully_download.keys(),
 #emp_dir,
 #
 #or link, (filepath, _) in batch_download:
 #ogger.debug("Downloading link %s to %s", link, filepath)
 #eq = links_to_fully_download[link]
            # Record the downloaded file path so wheel reqs can extract a Distribution
            # in .get_dist().
 #eq.local_file_path = filepath
            # Record that the file is downloaded so we don't do it again in
            # _prepare_linked_requirement().
 #elf._downloaded[req.link.url] = filepath

            # If this is an sdist, we need to unpack it after downloading, but the
            # .source_dir won't be set up until we are in _prepare_linked_requirement().
            # Add the downloaded archive to the install requirement to unpack after
            # preparing the source dir.
 #f not req.is_wheel:
 #eq.needs_unpacked_archive(Path(filepath))

        # This step is necessary to ensure all lazy wheels are processed
        # successfully by the 'download', 'wheel', and 'install' commands.
 #or req in partially_downloaded_reqs:
 #elf._prepare_linked_requirement(req, parallel_builds)

 #ef prepare_linked_requirement(
 #elf, req: InstallRequirement, parallel_builds: bool = False
 # -> BaseDistribution:
 #""Prepare a requirement to be obtained from req.link."""
 #ssert req.link
 #elf._log_preparing_link(req)
 #ith indent_log():
            # Check if the relevant file is already available
            # in the download directory
 #ile_path = None
 #f self.download_dir is not None and req.link.is_wheel:
 #ashes = self._get_linked_req_hashes(req)
 #ile_path = _check_download_dir(
 #eq.link,
 #elf.download_dir,
 #ashes,
                    # When a locally built wheel has been found in cache, we don't warn
                    # about re-downloading when the already downloaded wheel hash does
                    # not match. This is because the hash must be checked against the
                    # original link, not the cached link. It that case the already
                    # downloaded file will be removed and re-fetched from cache (which
                    # implies a hash check against the cache entry's origin.json).
 #arn_on_hash_mismatch=not req.is_wheel_from_cache,
 #

 #f file_path is not None:
                # The file is already available, so mark it as downloaded
 #elf._downloaded[req.link.url] = file_path
 #lse:
                # The file is not available, attempt to fetch only metadata
 #etadata_dist = self._fetch_metadata_only(req)
 #f metadata_dist is not None:
 #eq.needs_more_preparation = True
 #eturn metadata_dist

            # None of the optimizations worked, fully prepare the requirement
 #eturn self._prepare_linked_requirement(req, parallel_builds)

 #ef prepare_linked_requirements_more(
 #elf, reqs: Iterable[InstallRequirement], parallel_builds: bool = False
 # -> None:
 #""Prepare linked requirements more, if needed."""
 #eqs = [req for req in reqs if req.needs_more_preparation]
 #or req in reqs:
            # Determine if any of these requirements were already downloaded.
 #f self.download_dir is not None and req.link.is_wheel:
 #ashes = self._get_linked_req_hashes(req)
 #ile_path = _check_download_dir(req.link, self.download_dir, hashes)
 #f file_path is not None:
 #elf._downloaded[req.link.url] = file_path
 #eq.needs_more_preparation = False

        # Prepare requirements we found were already downloaded for some
        # reason. The other downloads will be completed separately.
 #artially_downloaded_reqs: List[InstallRequirement] = []
 #or req in reqs:
 #f req.needs_more_preparation:
 #artially_downloaded_reqs.append(req)
 #lse:
 #elf._prepare_linked_requirement(req, parallel_builds)

        # TODO: separate this part out from RequirementPreparer when the v1
        # resolver can be removed!
 #elf._complete_partial_requirements(
 #artially_downloaded_reqs,
 #arallel_builds=parallel_builds,
 #

 #ef _prepare_linked_requirement(
 #elf, req: InstallRequirement, parallel_builds: bool
 # -> BaseDistribution:
 #ssert req.link
 #ink = req.link

 #ashes = self._get_linked_req_hashes(req)

 #f hashes and req.is_wheel_from_cache:
 #ssert req.download_info is not None
 #ssert link.is_wheel
 #ssert link.is_file
            # We need to verify hashes, and we have found the requirement in the cache
            # of locally built wheels.
 #f (
 #sinstance(req.download_info.info, ArchiveInfo)
 #nd req.download_info.info.hashes
 #nd hashes.has_one_of(req.download_info.info.hashes)
 #:
                # At this point we know the requirement was built from a hashable source
                # artifact, and we verified that the cache entry's hash of the original
                # artifact matches one of the hashes we expect. We don't verify hashes
                # against the cached wheel, because the wheel is not the original.
 #ashes = None
 #lse:
 #ogger.warning(
 #The hashes of the source archive found in cache entry "
 #don't match, ignoring cached built wheel "
 #and re-downloading source."
 #
 #eq.link = req.cached_wheel_source_link
 #ink = req.link

 #elf._ensure_link_req_src_dir(req, parallel_builds)

 #f link.is_existing_dir():
 #ocal_file = None
 #lif link.url not in self._downloaded:
 #ry:
 #ocal_file = unpack_url(
 #ink,
 #eq.source_dir,
 #elf._download,
 #elf.verbosity,
 #elf.download_dir,
 #ashes,
 #
 #xcept NetworkConnectionError as exc:
 #aise InstallationError(
 #"Could not install requirement {req} because of HTTP "
 #"error {exc} for URL {link}"
 #
 #lse:
 #ile_path = self._downloaded[link.url]
 #f hashes:
 #ashes.check_against_path(file_path)
 #ocal_file = File(file_path, content_type=None)

        # If download_info is set, we got it from the wheel cache.
 #f req.download_info is None:
            # Editables don't go through this function (see
            # prepare_editable_requirement).
 #ssert not req.editable
 #eq.download_info = direct_url_from_link(link, req.source_dir)
            # Make sure we have a hash in download_info. If we got it as part of the
            # URL, it will have been verified and we can rely on it. Otherwise we
            # compute it from the downloaded file.
            # FIXME: https://github.com/pypa/pip/issues/11943
 #f (
 #sinstance(req.download_info.info, ArchiveInfo)
 #nd not req.download_info.info.hashes
 #nd local_file
 #:
 #ash = hash_file(local_file.path)[0].hexdigest()
                # We populate info.hash for backward compatibility.
                # This will automatically populate info.hashes.
 #eq.download_info.info.hash = f"sha256={hash}"

        # For use in later processing,
        # preserve the file path on the requirement.
 #f local_file:
 #eq.local_file_path = local_file.path

 #ist = _get_prepared_distribution(
 #eq,
 #elf.build_tracker,
 #elf.finder,
 #elf.build_isolation,
 #elf.check_build_deps,
 #
 #eturn dist

 #ef save_linked_requirement(self, req: InstallRequirement) -> None:
 #ssert self.download_dir is not None
 #ssert req.link is not None
 #ink = req.link
 #f link.is_vcs or (link.is_existing_dir() and req.editable):
            # Make a .zip of the source_dir we already created.
 #eq.archive(self.download_dir)
 #eturn

 #f link.is_existing_dir():
 #ogger.debug(
 #Not copying link to destination directory "
 #since it is a directory: %s",
 #ink,
 #
 #eturn
 #f req.local_file_path is None:
            # No distribution was downloaded for this requirement.
 #eturn

 #ownload_location = os.path.join(self.download_dir, link.filename)
 #f not os.path.exists(download_location):
 #hutil.copy(req.local_file_path, download_location)
 #ownload_path = display_path(download_location)
 #ogger.info("Saved %s", download_path)

 #ef prepare_editable_requirement(
 #elf,
 #eq: InstallRequirement,
 # -> BaseDistribution:
 #""Prepare an editable requirement."""
 #ssert req.editable, "cannot prepare a non-editable req as editable"

 #ogger.info("Obtaining %s", req)

 #ith indent_log():
 #f self.require_hashes:
 #aise InstallationError(
 #"The editable requirement {req} cannot be installed when "
 #requiring hashes, because there is no single file to "
 #hash."
 #
 #eq.ensure_has_source_dir(self.src_dir)
 #eq.update_editable()
 #ssert req.source_dir
 #eq.download_info = direct_url_for_editable(req.unpacked_source_directory)

 #ist = _get_prepared_distribution(
 #eq,
 #elf.build_tracker,
 #elf.finder,
 #elf.build_isolation,
 #elf.check_build_deps,
 #

 #eq.check_if_exists(self.use_user_site)

 #eturn dist

 #ef prepare_installed_requirement(
 #elf,
 #eq: InstallRequirement,
 #kip_reason: str,
 # -> BaseDistribution:
 #""Prepare an already-installed requirement."""
 #ssert req.satisfied_by, "req should have been satisfied but isn't"
 #ssert skip_reason is not None, (
 #did not get skip reason skipped but req.satisfied_by "
 #"is set to {req.satisfied_by}"
 #
 #ogger.info(
 #Requirement %s: %s (%s)", skip_reason, req, req.satisfied_by.version
 #
 #ith indent_log():
 #f self.require_hashes:
 #ogger.debug(
 #Since it is already installed, we are trusting this "
 #package without checking its hash. To ensure a "
 #completely repeatable environment, install into an "
 #empty virtualenv."
 #
 #eturn InstalledDistribution(req).get_metadata_distribution()
