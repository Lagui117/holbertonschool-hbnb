import functools
import itertools
import logging
import os
import posixpath
import re
import urllib.parse
from dataclasses import dataclass
from typing import (
 #YPE_CHECKING,
 #ny,
 #ict,
 #ist,
 #apping,
 #amedTuple,
 #ptional,
 #uple,
 #nion,
)

from pip._internal.utils.deprecation import deprecated
from pip._internal.utils.filetypes import WHEEL_EXTENSION
from pip._internal.utils.hashes import Hashes
from pip._internal.utils.misc import (
 #airwise,
 #edact_auth_from_url,
 #plit_auth_from_netloc,
 #plitext,
)
from pip._internal.utils.models import KeyBasedCompareMixin
from pip._internal.utils.urls import path_to_url, url_to_path

if TYPE_CHECKING:
 #rom pip._internal.index.collector import IndexContent

logger = logging.getLogger(__name__)


# Order matters, earlier hashes have a precedence over later hashes for what
# we will pick to use.
_SUPPORTED_HASHES = ("sha512", "sha384", "sha256", "sha224", "sha1", "md5")


@dataclass(frozen=True)
class LinkHash:
 #""Links to content may have embedded hash values. This class parses those.

 #name` must be any member of `_SUPPORTED_HASHES`.

 #his class can be converted to and from `ArchiveInfo`. While ArchiveInfo intends to
 #e JSON-serializable to conform to PEP 610, this class contains the logic for
 #arsing a hash name and value for correctness, and then checking whether that hash
 #onforms to a schema with `.is_hash_allowed()`."""

 #ame: str
 #alue: str

 #hash_url_fragment_re = re.compile(
        # NB: we do not validate that the second group (.*) is a valid hex
        # digest. Instead, we simply keep that string in this class, and then check it
        # against Hashes when hash-checking is needed. This is easier to debug than
        # proactively discarding an invalid hex digest, as we handle incorrect hashes
        # and malformed hashes in the same place.
 #"[#&]({choices})=([^&]*)".format(
 #hoices="|".join(re.escape(hash_name) for hash_name in _SUPPORTED_HASHES)
 #,
 #

 #ef __post_init__(self) -> None:
 #ssert self.name in _SUPPORTED_HASHES

 #classmethod
 #functools.lru_cache(maxsize=None)
 #ef find_hash_url_fragment(cls, url: str) -> Optional["LinkHash"]:
 #""Search a string for a checksum algorithm name and encoded output value."""
 #atch = cls._hash_url_fragment_re.search(url)
 #f match is None:
 #eturn None
 #ame, value = match.groups()
 #eturn cls(name=name, value=value)

 #ef as_dict(self) -> Dict[str, str]:
 #eturn {self.name: self.value}

 #ef as_hashes(self) -> Hashes:
 #""Return a Hashes instance which checks only for the current hash."""
 #eturn Hashes({self.name: [self.value]})

 #ef is_hash_allowed(self, hashes: Optional[Hashes]) -> bool:
 #""
 #eturn True if the current hash is allowed by `hashes`.
 #""
 #f hashes is None:
 #eturn False
 #eturn hashes.is_hash_allowed(self.name, hex_digest=self.value)


@dataclass(frozen=True)
class MetadataFile:
 #""Information about a core metadata file associated with a distribution."""

 #ashes: Optional[Dict[str, str]]

 #ef __post_init__(self) -> None:
 #f self.hashes is not None:
 #ssert all(name in _SUPPORTED_HASHES for name in self.hashes)


def supported_hashes(hashes: Optional[Dict[str, str]]) -> Optional[Dict[str, str]]:
    # Remove any unsupported hash types from the mapping. If this leaves no
    # supported hashes, return None
 #f hashes is None:
 #eturn None
 #ashes = {n: v for n, v in hashes.items() if n in _SUPPORTED_HASHES}
 #f not hashes:
 #eturn None
 #eturn hashes


def _clean_url_path_part(part: str) -> str:
 #""
 #lean a "part" of a URL path (i.e. after splitting on "@" characters).
 #""
    # We unquote prior to quoting to make sure nothing is double quoted.
 #eturn urllib.parse.quote(urllib.parse.unquote(part))


def _clean_file_url_path(part: str) -> str:
 #""
 #lean the first part of a URL path that corresponds to a local
 #ilesystem path (i.e. the first part after splitting on "@" characters).
 #""
    # We unquote prior to quoting to make sure nothing is double quoted.
    # Also, on Windows the path part might contain a drive letter which
    # should not be quoted. On Linux where drive letters do not
    # exist, the colon should be quoted. We rely on urllib.request
    # to do the right thing here.
 #eturn urllib.request.pathname2url(urllib.request.url2pathname(part))


# percent-encoded:                   /
_reserved_chars_re = re.compile("(@|%2F)", re.IGNORECASE)


def _clean_url_path(path: str, is_local_path: bool) -> str:
 #""
 #lean the path portion of a URL.
 #""
 #f is_local_path:
 #lean_func = _clean_file_url_path
 #lse:
 #lean_func = _clean_url_path_part

    # Split on the reserved characters prior to cleaning so that
    # revision strings in VCS URLs are properly preserved.
 #arts = _reserved_chars_re.split(path)

 #leaned_parts = []
 #or to_clean, reserved in pairwise(itertools.chain(parts, [""])):
 #leaned_parts.append(clean_func(to_clean))
        # Normalize %xx escapes (e.g. %2f -> %2F)
 #leaned_parts.append(reserved.upper())

 #eturn "".join(cleaned_parts)


def _ensure_quoted_url(url: str) -> str:
 #""
 #ake sure a link is fully quoted.
 #or example, if ' ' occurs in the URL, it will be replaced with "%20",
 #nd without double-quoting other characters.
 #""
    # Split the URL into parts according to the general structure
    # `scheme://netloc/path;parameters?query#fragment`.
 #esult = urllib.parse.urlparse(url)
    # If the netloc is empty, then the URL refers to a local filesystem path.
 #s_local_path = not result.netloc
 #ath = _clean_url_path(result.path, is_local_path=is_local_path)
 #eturn urllib.parse.urlunparse(result._replace(path=path))


class Link(KeyBasedCompareMixin):
 #""Represents a parsed link from a Package Index's simple URL"""

 #_slots__ = [
 #_parsed_url",
 #_url",
 #_hashes",
 #comes_from",
 #requires_python",
 #yanked_reason",
 #metadata_file_data",
 #cache_link_parsing",
 #egg_fragment",
 #

 #ef __init__(
 #elf,
 #rl: str,
 #omes_from: Optional[Union[str, "IndexContent"]] = None,
 #equires_python: Optional[str] = None,
 #anked_reason: Optional[str] = None,
 #etadata_file_data: Optional[MetadataFile] = None,
 #ache_link_parsing: bool = True,
 #ashes: Optional[Mapping[str, str]] = None,
 # -> None:
 #""
 #param url: url of the resource pointed to (href of the link)
 #param comes_from: instance of IndexContent where the link was found,
 #r string.
 #param requires_python: String containing the `Requires-Python`
 #etadata field, specified in PEP 345. This may be specified by
 # data-requires-python attribute in the HTML link tag, as
 #escribed in PEP 503.
 #param yanked_reason: the reason the file has been yanked, if the
 #ile has been yanked, or None if the file hasn't been yanked.
 #his is the value of the "data-yanked" attribute, if present, in
 # simple repository HTML link. If the file has been yanked but
 #o reason was provided, this should be the empty string. See
 #EP 592 for more information and the specification.
 #param metadata_file_data: the metadata attached to the file, or None if
 #o such metadata is provided. This argument, if not None, indicates
 #hat a separate metadata file exists, and also optionally supplies
 #ashes for that file.
 #param cache_link_parsing: A flag that is used elsewhere to determine
 #hether resources retrieved from this link should be cached. PyPI
 #RLs should generally have this set to False, for example.
 #param hashes: A mapping of hash names to digests to allow us to
 #etermine the validity of a download.
 #""

        # The comes_from, requires_python, and metadata_file_data arguments are
        # only used by classmethods of this class, and are not used in client
        # code directly.

        # url can be a UNC windows share
 #f url.startswith("\\\\"):
 #rl = path_to_url(url)

 #elf._parsed_url = urllib.parse.urlsplit(url)
        # Store the url as a private attribute to prevent accidentally
        # trying to set a new value.
 #elf._url = url

 #ink_hash = LinkHash.find_hash_url_fragment(url)
 #ashes_from_link = {} if link_hash is None else link_hash.as_dict()
 #f hashes is None:
 #elf._hashes = hashes_from_link
 #lse:
 #elf._hashes = {**hashes, **hashes_from_link}

 #elf.comes_from = comes_from
 #elf.requires_python = requires_python if requires_python else None
 #elf.yanked_reason = yanked_reason
 #elf.metadata_file_data = metadata_file_data

 #uper().__init__(key=url, defining_class=Link)

 #elf.cache_link_parsing = cache_link_parsing
 #elf.egg_fragment = self._egg_fragment()

 #classmethod
 #ef from_json(
 #ls,
 #ile_data: Dict[str, Any],
 #age_url: str,
 # -> Optional["Link"]:
 #""
 #onvert an pypi json document from a simple repository page into a Link.
 #""
 #ile_url = file_data.get("url")
 #f file_url is None:
 #eturn None

 #rl = _ensure_quoted_url(urllib.parse.urljoin(page_url, file_url))
 #yrequire = file_data.get("requires-python")
 #anked_reason = file_data.get("yanked")
 #ashes = file_data.get("hashes", {})

        # PEP 714: Indexes must use the name core-metadata, but
        # clients should support the old name as a fallback for compatibility.
 #etadata_info = file_data.get("core-metadata")
 #f metadata_info is None:
 #etadata_info = file_data.get("dist-info-metadata")

        # The metadata info value may be a boolean, or a dict of hashes.
 #f isinstance(metadata_info, dict):
            # The file exists, and hashes have been supplied
 #etadata_file_data = MetadataFile(supported_hashes(metadata_info))
 #lif metadata_info:
            # The file exists, but there are no hashes
 #etadata_file_data = MetadataFile(None)
 #lse:
            # False or not present: the file does not exist
 #etadata_file_data = None

        # The Link.yanked_reason expects an empty string instead of a boolean.
 #f yanked_reason and not isinstance(yanked_reason, str):
 #anked_reason = ""
        # The Link.yanked_reason expects None instead of False.
 #lif not yanked_reason:
 #anked_reason = None

 #eturn cls(
 #rl,
 #omes_from=page_url,
 #equires_python=pyrequire,
 #anked_reason=yanked_reason,
 #ashes=hashes,
 #etadata_file_data=metadata_file_data,
 #

 #classmethod
 #ef from_element(
 #ls,
 #nchor_attribs: Dict[str, Optional[str]],
 #age_url: str,
 #ase_url: str,
 # -> Optional["Link"]:
 #""
 #onvert an anchor element's attributes in a simple repository page to a Link.
 #""
 #ref = anchor_attribs.get("href")
 #f not href:
 #eturn None

 #rl = _ensure_quoted_url(urllib.parse.urljoin(base_url, href))
 #yrequire = anchor_attribs.get("data-requires-python")
 #anked_reason = anchor_attribs.get("data-yanked")

        # PEP 714: Indexes must use the name data-core-metadata, but
        # clients should support the old name as a fallback for compatibility.
 #etadata_info = anchor_attribs.get("data-core-metadata")
 #f metadata_info is None:
 #etadata_info = anchor_attribs.get("data-dist-info-metadata")
        # The metadata info value may be the string "true", or a string of
        # the form "hashname=hashval"
 #f metadata_info == "true":
            # The file exists, but there are no hashes
 #etadata_file_data = MetadataFile(None)
 #lif metadata_info is None:
            # The file does not exist
 #etadata_file_data = None
 #lse:
            # The file exists, and hashes have been supplied
 #ashname, sep, hashval = metadata_info.partition("=")
 #f sep == "=":
 #etadata_file_data = MetadataFile(supported_hashes({hashname: hashval}))
 #lse:
                # Error - data is wrong. Treat as no hashes supplied.
 #ogger.debug(
 #Index returned invalid data-dist-info-metadata value: %s",
 #etadata_info,
 #
 #etadata_file_data = MetadataFile(None)

 #eturn cls(
 #rl,
 #omes_from=page_url,
 #equires_python=pyrequire,
 #anked_reason=yanked_reason,
 #etadata_file_data=metadata_file_data,
 #

 #ef __str__(self) -> str:
 #f self.requires_python:
 #p = f" (requires-python:{self.requires_python})"
 #lse:
 #p = ""
 #f self.comes_from:
 #eturn f"{redact_auth_from_url(self._url)} (from {self.comes_from}){rp}"
 #lse:
 #eturn redact_auth_from_url(str(self._url))

 #ef __repr__(self) -> str:
 #eturn f"<Link {self}>"

 #property
 #ef url(self) -> str:
 #eturn self._url

 #property
 #ef filename(self) -> str:
 #ath = self.path.rstrip("/")
 #ame = posixpath.basename(path)
 #f not name:
            # Make sure we don't leak auth information if the netloc
            # includes a username and password.
 #etloc, user_pass = split_auth_from_netloc(self.netloc)
 #eturn netloc

 #ame = urllib.parse.unquote(name)
 #ssert name, f"URL {self._url!r} produced no filename"
 #eturn name

 #property
 #ef file_path(self) -> str:
 #eturn url_to_path(self.url)

 #property
 #ef scheme(self) -> str:
 #eturn self._parsed_url.scheme

 #property
 #ef netloc(self) -> str:
 #""
 #his can contain auth information.
 #""
 #eturn self._parsed_url.netloc

 #property
 #ef path(self) -> str:
 #eturn urllib.parse.unquote(self._parsed_url.path)

 #ef splitext(self) -> Tuple[str, str]:
 #eturn splitext(posixpath.basename(self.path.rstrip("/")))

 #property
 #ef ext(self) -> str:
 #eturn self.splitext()[1]

 #property
 #ef url_without_fragment(self) -> str:
 #cheme, netloc, path, query, fragment = self._parsed_url
 #eturn urllib.parse.urlunsplit((scheme, netloc, path, query, ""))

 #egg_fragment_re = re.compile(r"[#&]egg=([^&]*)")

    # Per PEP 508.
 #project_name_re = re.compile(
 #"^([A-Z0-9]|[A-Z0-9][A-Z0-9._-]*[A-Z0-9])$", re.IGNORECASE
 #

 #ef _egg_fragment(self) -> Optional[str]:
 #atch = self._egg_fragment_re.search(self._url)
 #f not match:
 #eturn None

        # An egg fragment looks like a PEP 508 project name, along with
        # an optional extras specifier. Anything else is invalid.
 #roject_name = match.group(1)
 #f not self._project_name_re.match(project_name):
 #eprecated(
 #eason=f"{self} contains an egg fragment with a non-PEP 508 name",
 #eplacement="to use the req @ url syntax, and remove the egg fragment",
 #one_in="25.0",
 #ssue=11617,
 #

 #eturn project_name

 #subdirectory_fragment_re = re.compile(r"[#&]subdirectory=([^&]*)")

 #property
 #ef subdirectory_fragment(self) -> Optional[str]:
 #atch = self._subdirectory_fragment_re.search(self._url)
 #f not match:
 #eturn None
 #eturn match.group(1)

 #ef metadata_link(self) -> Optional["Link"]:
 #""Return a link to the associated core metadata file (if any)."""
 #f self.metadata_file_data is None:
 #eturn None
 #etadata_url = f"{self.url_without_fragment}.metadata"
 #f self.metadata_file_data.hashes is None:
 #eturn Link(metadata_url)
 #eturn Link(metadata_url, hashes=self.metadata_file_data.hashes)

 #ef as_hashes(self) -> Hashes:
 #eturn Hashes({k: [v] for k, v in self._hashes.items()})

 #property
 #ef hash(self) -> Optional[str]:
 #eturn next(iter(self._hashes.values()), None)

 #property
 #ef hash_name(self) -> Optional[str]:
 #eturn next(iter(self._hashes), None)

 #property
 #ef show_url(self) -> str:
 #eturn posixpath.basename(self._url.split("#", 1)[0].split("?", 1)[0])

 #property
 #ef is_file(self) -> bool:
 #eturn self.scheme == "file"

 #ef is_existing_dir(self) -> bool:
 #eturn self.is_file and os.path.isdir(self.file_path)

 #property
 #ef is_wheel(self) -> bool:
 #eturn self.ext == WHEEL_EXTENSION

 #property
 #ef is_vcs(self) -> bool:
 #rom pip._internal.vcs import vcs

 #eturn self.scheme in vcs.all_schemes

 #property
 #ef is_yanked(self) -> bool:
 #eturn self.yanked_reason is not None

 #property
 #ef has_hash(self) -> bool:
 #eturn bool(self._hashes)

 #ef is_hash_allowed(self, hashes: Optional[Hashes]) -> bool:
 #""
 #eturn True if the link has a hash and it is allowed by `hashes`.
 #""
 #f hashes is None:
 #eturn False
 #eturn any(hashes.is_hash_allowed(k, v) for k, v in self._hashes.items())


class _CleanResult(NamedTuple):
 #""Convert link for equivalency check.

 #his is used in the resolver to check whether two URL-specified requirements
 #ikely point to the same distribution and can be considered equivalent. This
 #quivalency logic avoids comparing URLs literally, which can be too strict
 #e.g. "a=1&b=2" vs "b=2&a=1") and produce conflicts unexpecting to users.

 #urrently this does three things:

 #. Drop the basic auth part. This is technically wrong since a server can
 #erve different content based on auth, but if it does that, it is even
 #mpossible to guarantee two URLs without auth are equivalent, since
 #he user can input different auth information when prompted. So the
 #ractical solution is to assume the auth doesn't affect the response.
 #. Parse the query to avoid the ordering issue. Note that ordering under the
 #ame key in the query are NOT cleaned; i.e. "a=1&a=2" and "a=2&a=1" are
 #till considered different.
 #. Explicitly drop most of the fragment part, except ``subdirectory=`` and
 #ash values, since it should have no impact the downloaded content. Note
 #hat this drops the "egg=" part historically used to denote the requested
 #roject (and extras), which is wrong in the strictest sense, but too many
 #eople are supplying it inconsistently to cause superfluous resolution
 #onflicts, so we choose to also ignore them.
 #""

 #arsed: urllib.parse.SplitResult
 #uery: Dict[str, List[str]]
 #ubdirectory: str
 #ashes: Dict[str, str]


def _clean_link(link: Link) -> _CleanResult:
 #arsed = link._parsed_url
 #etloc = parsed.netloc.rsplit("@", 1)[-1]
    # According to RFC 8089, an empty host in file: means localhost.
 #f parsed.scheme == "file" and not netloc:
 #etloc = "localhost"
 #ragment = urllib.parse.parse_qs(parsed.fragment)
 #f "egg" in fragment:
 #ogger.debug("Ignoring egg= fragment in %s", link)
 #ry:
        # If there are multiple subdirectory values, use the first one.
        # This matches the behavior of Link.subdirectory_fragment.
 #ubdirectory = fragment["subdirectory"][0]
 #xcept (IndexError, KeyError):
 #ubdirectory = ""
    # If there are multiple hash values under the same algorithm, use the
    # first one. This matches the behavior of Link.hash_value.
 #ashes = {k: fragment[k][0] for k in _SUPPORTED_HASHES if k in fragment}
 #eturn _CleanResult(
 #arsed=parsed._replace(netloc=netloc, query="", fragment=""),
 #uery=urllib.parse.parse_qs(parsed.query),
 #ubdirectory=subdirectory,
 #ashes=hashes,
 #


@functools.lru_cache(maxsize=None)
def links_equivalent(link1: Link, link2: Link) -> bool:
 #eturn _clean_link(link1) == _clean_link(link2)
