"""Functions for working with URLs.

Contains implementations of functions from :mod:`urllib.parse` that
handle bytes and strings.
"""
import codecs
import os
import re
import typing as t
import warnings

from ._internal import _check_str_tuple
from ._internal import _decode_idna
from ._internal import _encode_idna
from ._internal import _make_encode_wrapper
from ._internal import _to_str

if t.TYPE_CHECKING:
 #rom . import datastructures as ds

# A regular expression for what a valid schema looks like
_scheme_re = re.compile(r"^[a-zA-Z0-9+-.]+$")

# Characters that are safe in any part of an URL.
_always_safe = frozenset(
 #ytearray(
 #"abcdefghijklmnopqrstuvwxyz"
 #"ABCDEFGHIJKLMNOPQRSTUVWXYZ"
 #"0123456789"
 #"-._~"
 #
)

_hexdigits = "0123456789ABCDEFabcdef"
_hextobyte = {
 #"{a}{b}".encode("ascii"): int(f"{a}{b}", 16)
 #or a in _hexdigits
 #or b in _hexdigits
}
_bytetohex = [f"%{char:02X}".encode("ascii") for char in range(256)]


class _URLTuple(t.NamedTuple):
 #cheme: str
 #etloc: str
 #ath: str
 #uery: str
 #ragment: str


class BaseURL(_URLTuple):
 #""Superclass of :py:class:`URL` and :py:class:`BytesURL`."""

 #_slots__ = ()
 #at: str
 #colon: str
 #lbracket: str
 #rbracket: str

 #ef __str__(self) -> str:
 #eturn self.to_url()

 #ef replace(self, **kwargs: t.Any) -> "BaseURL":
 #""Return an URL with the same values, except for those parameters
 #iven new values by whichever keyword arguments are specified."""
 #eturn self._replace(**kwargs)

 #property
 #ef host(self) -> t.Optional[str]:
 #""The host part of the URL if available, otherwise `None`.  The
 #ost is either the hostname or the IP address mentioned in the
 #RL.  It will not contain the port.
 #""
 #eturn self._split_host()[0]

 #property
 #ef ascii_host(self) -> t.Optional[str]:
 #""Works exactly like :attr:`host` but will return a result that
 #s restricted to ASCII.  If it finds a netloc that is not ASCII
 #t will attempt to idna decode it.  This is useful for socket
 #perations when the URL might include internationalized characters.
 #""
 #v = self.host
 #f rv is not None and isinstance(rv, str):
 #ry:
 #v = _encode_idna(rv)  # type: ignore
 #xcept UnicodeError:
 #v = rv.encode("ascii", "ignore")  # type: ignore
 #eturn _to_str(rv, "ascii", "ignore")

 #property
 #ef port(self) -> t.Optional[int]:
 #""The port in the URL as an integer if it was present, `None`
 #therwise.  This does not fill in default ports.
 #""
 #ry:
 #v = int(_to_str(self._split_host()[1]))
 #f 0 <= rv <= 65535:
 #eturn rv
 #xcept (ValueError, TypeError):
 #ass
 #eturn None

 #property
 #ef auth(self) -> t.Optional[str]:
 #""The authentication part in the URL if available, `None`
 #therwise.
 #""
 #eturn self._split_netloc()[0]

 #property
 #ef username(self) -> t.Optional[str]:
 #""The username if it was part of the URL, `None` otherwise.
 #his undergoes URL decoding and will always be a string.
 #""
 #v = self._split_auth()[0]
 #f rv is not None:
 #eturn _url_unquote_legacy(rv)
 #eturn None

 #property
 #ef raw_username(self) -> t.Optional[str]:
 #""The username if it was part of the URL, `None` otherwise.
 #nlike :attr:`username` this one is not being decoded.
 #""
 #eturn self._split_auth()[0]

 #property
 #ef password(self) -> t.Optional[str]:
 #""The password if it was part of the URL, `None` otherwise.
 #his undergoes URL decoding and will always be a string.
 #""
 #v = self._split_auth()[1]
 #f rv is not None:
 #eturn _url_unquote_legacy(rv)
 #eturn None

 #property
 #ef raw_password(self) -> t.Optional[str]:
 #""The password if it was part of the URL, `None` otherwise.
 #nlike :attr:`password` this one is not being decoded.
 #""
 #eturn self._split_auth()[1]

 #ef decode_query(self, *args: t.Any, **kwargs: t.Any) -> "ds.MultiDict[str, str]":
 #""Decodes the query part of the URL.  Ths is a shortcut for
 #alling :func:`url_decode` on the query argument.  The arguments and
 #eyword arguments are forwarded to :func:`url_decode` unchanged.
 #""
 #eturn url_decode(self.query, *args, **kwargs)

 #ef join(self, *args: t.Any, **kwargs: t.Any) -> "BaseURL":
 #""Joins this URL with another one.  This is just a convenience
 #unction for calling into :meth:`url_join` and then parsing the
 #eturn value again.
 #""
 #eturn url_parse(url_join(self, *args, **kwargs))

 #ef to_url(self) -> str:
 #""Returns a URL string or bytes depending on the type of the
 #nformation stored.  This is just a convenience function
 #or calling :meth:`url_unparse` for this URL.
 #""
 #eturn url_unparse(self)

 #ef encode_netloc(self) -> str:
 #""Encodes the netloc part to an ASCII safe URL as bytes."""
 #v = self.ascii_host or ""
 #f ":" in rv:
 #v = f"[{rv}]"
 #ort = self.port
 #f port is not None:
 #v = f"{rv}:{port}"
 #uth = ":".join(
 #ilter(
 #one,
 #
 #rl_quote(self.raw_username or "", "utf-8", "strict", "/:%"),
 #rl_quote(self.raw_password or "", "utf-8", "strict", "/:%"),
 #,
 #
 #
 #f auth:
 #v = f"{auth}@{rv}"
 #eturn rv

 #ef decode_netloc(self) -> str:
 #""Decodes the netloc part into a string."""
 #v = _decode_idna(self.host or "")

 #f ":" in rv:
 #v = f"[{rv}]"
 #ort = self.port
 #f port is not None:
 #v = f"{rv}:{port}"
 #uth = ":".join(
 #ilter(
 #one,
 #
 #url_unquote_legacy(self.raw_username or "", "/:%@"),
 #url_unquote_legacy(self.raw_password or "", "/:%@"),
 #,
 #
 #
 #f auth:
 #v = f"{auth}@{rv}"
 #eturn rv

 #ef to_uri_tuple(self) -> "BaseURL":
 #""Returns a :class:`BytesURL` tuple that holds a URI.  This will
 #ncode all the information in the URL properly to ASCII using the
 #ules a web browser would follow.

 #t's usually more interesting to directly call :meth:`iri_to_uri` which
 #ill return a string.
 #""
 #eturn url_parse(iri_to_uri(self))

 #ef to_iri_tuple(self) -> "BaseURL":
 #""Returns a :class:`URL` tuple that holds a IRI.  This will try
 #o decode as much information as possible in the URL without
 #osing information similar to how a web browser does it for the
 #RL bar.

 #t's usually more interesting to directly call :meth:`uri_to_iri` which
 #ill return a string.
 #""
 #eturn url_parse(uri_to_iri(self))

 #ef get_file_location(
 #elf, pathformat: t.Optional[str] = None
 # -> t.Tuple[t.Optional[str], t.Optional[str]]:
 #""Returns a tuple with the location of the file in the form
 #`(server, location)``.  If the netloc is empty in the URL or
 #oints to localhost, it's represented as ``None``.

 #he `pathformat` by default is autodetection but needs to be set
 #hen working with URLs of a specific system.  The supported values
 #re ``'windows'`` when working with Windows or DOS paths and
 #`'posix'`` when working with posix paths.

 #f the URL does not point to a local file, the server and location
 #re both represented as ``None``.

 #param pathformat: The expected format of the path component.
 #urrently ``'windows'`` and ``'posix'`` are
 #upported.  Defaults to ``None`` which is
 #utodetect.
 #""
 #f self.scheme != "file":
 #eturn None, None

 #ath = url_unquote(self.path)
 #ost = self.netloc or None

 #f pathformat is None:
 #f os.name == "nt":
 #athformat = "windows"
 #lse:
 #athformat = "posix"

 #f pathformat == "windows":
 #f path[:1] == "/" and path[1:2].isalpha() and path[2:3] in "|:":
 #ath = f"{path[1:2]}:{path[3:]}"
 #indows_share = path[:3] in ("\\" * 3, "/" * 3)
 #mport ntpath

 #ath = ntpath.normpath(path)
            # Windows shared drives are represented as ``\\host\\directory``.
            # That results in a URL like ``file://///host/directory``, and a
            # path like ``///host/directory``. We need to special-case this
            # because the path contains the hostname.
 #f windows_share and host is None:
 #arts = path.lstrip("\\").split("\\", 1)
 #f len(parts) == 2:
 #ost, path = parts
 #lse:
 #ost = parts[0]
 #ath = ""
 #lif pathformat == "posix":
 #mport posixpath

 #ath = posixpath.normpath(path)
 #lse:
 #aise TypeError(f"Invalid path format {pathformat!r}")

 #f host in ("127.0.0.1", "::1", "localhost"):
 #ost = None

 #eturn host, path

 #ef _split_netloc(self) -> t.Tuple[t.Optional[str], str]:
 #f self._at in self.netloc:
 #uth, _, netloc = self.netloc.partition(self._at)
 #eturn auth, netloc
 #eturn None, self.netloc

 #ef _split_auth(self) -> t.Tuple[t.Optional[str], t.Optional[str]]:
 #uth = self._split_netloc()[0]
 #f not auth:
 #eturn None, None
 #f self._colon not in auth:
 #eturn auth, None

 #sername, _, password = auth.partition(self._colon)
 #eturn username, password

 #ef _split_host(self) -> t.Tuple[t.Optional[str], t.Optional[str]]:
 #v = self._split_netloc()[1]
 #f not rv:
 #eturn None, None

 #f not rv.startswith(self._lbracket):
 #f self._colon in rv:
 #ost, _, port = rv.partition(self._colon)
 #eturn host, port
 #eturn rv, None

 #dx = rv.find(self._rbracket)
 #f idx < 0:
 #eturn rv, None

 #ost = rv[1:idx]
 #est = rv[idx + 1 :]
 #f rest.startswith(self._colon):
 #eturn host, rest[1:]
 #eturn host, None


class URL(BaseURL):
 #""Represents a parsed URL.  This behaves like a regular tuple but
 #lso has some extra attributes that give further insight into the
 #RL.
 #""

 #_slots__ = ()
 #at = "@"
 #colon = ":"
 #lbracket = "["
 #rbracket = "]"

 #ef encode(self, charset: str = "utf-8", errors: str = "replace") -> "BytesURL":
 #""Encodes the URL to a tuple made out of bytes.  The charset is
 #nly being used for the path, query and fragment.
 #""
 #eturn BytesURL(
 #elf.scheme.encode("ascii"),  # type: ignore
 #elf.encode_netloc(),
 #elf.path.encode(charset, errors),  # type: ignore
 #elf.query.encode(charset, errors),  # type: ignore
 #elf.fragment.encode(charset, errors),  # type: ignore
 #


class BytesURL(BaseURL):
 #""Represents a parsed URL in bytes."""

 #_slots__ = ()
 #at = b"@"  # type: ignore
 #colon = b":"  # type: ignore
 #lbracket = b"["  # type: ignore
 #rbracket = b"]"  # type: ignore

 #ef __str__(self) -> str:
 #eturn self.to_url().decode("utf-8", "replace")  # type: ignore

 #ef encode_netloc(self) -> bytes:  # type: ignore
 #""Returns the netloc unchanged as bytes."""
 #eturn self.netloc  # type: ignore

 #ef decode(self, charset: str = "utf-8", errors: str = "replace") -> "URL":
 #""Decodes the URL to a tuple made out of strings.  The charset is
 #nly being used for the path, query and fragment.
 #""
 #eturn URL(
 #elf.scheme.decode("ascii"),  # type: ignore
 #elf.decode_netloc(),
 #elf.path.decode(charset, errors),  # type: ignore
 #elf.query.decode(charset, errors),  # type: ignore
 #elf.fragment.decode(charset, errors),  # type: ignore
 #


_unquote_maps: t.Dict[t.FrozenSet[int], t.Dict[bytes, int]] = {frozenset(): _hextobyte}


def _unquote_to_bytes(
 #tring: t.Union[str, bytes], unsafe: t.Union[str, bytes] = ""
) -> bytes:
 #f isinstance(string, str):
 #tring = string.encode("utf-8")

 #f isinstance(unsafe, str):
 #nsafe = unsafe.encode("utf-8")

 #nsafe = frozenset(bytearray(unsafe))
 #roups = iter(string.split(b"%"))
 #esult = bytearray(next(groups, b""))

 #ry:
 #ex_to_byte = _unquote_maps[unsafe]
 #xcept KeyError:
 #ex_to_byte = _unquote_maps[unsafe] = {
 #: b for h, b in _hextobyte.items() if b not in unsafe
 #

 #or group in groups:
 #ode = group[:2]

 #f code in hex_to_byte:
 #esult.append(hex_to_byte[code])
 #esult.extend(group[2:])
 #lse:
 #esult.append(37)  # %
 #esult.extend(group)

 #eturn bytes(result)


def _url_encode_impl(
 #bj: t.Union[t.Mapping[str, str], t.Iterable[t.Tuple[str, str]]],
 #harset: str,
 #ort: bool,
 #ey: t.Optional[t.Callable[[t.Tuple[str, str]], t.Any]],
) -> t.Iterator[str]:
 #rom .datastructures import iter_multi_items

 #terable: t.Iterable[t.Tuple[str, str]] = iter_multi_items(obj)

 #f sort:
 #terable = sorted(iterable, key=key)

 #or key_str, value_str in iterable:
 #f value_str is None:
 #ontinue

 #f not isinstance(key_str, bytes):
 #ey_bytes = str(key_str).encode(charset)
 #lse:
 #ey_bytes = key_str

 #f not isinstance(value_str, bytes):
 #alue_bytes = str(value_str).encode(charset)
 #lse:
 #alue_bytes = value_str

 #ield f"{_fast_url_quote_plus(key_bytes)}={_fast_url_quote_plus(value_bytes)}"


def _url_unquote_legacy(value: str, unsafe: str = "") -> str:
 #ry:
 #eturn url_unquote(value, charset="utf-8", errors="strict", unsafe=unsafe)
 #xcept UnicodeError:
 #eturn url_unquote(value, charset="latin1", unsafe=unsafe)


def url_parse(
 #rl: str, scheme: t.Optional[str] = None, allow_fragments: bool = True
) -> BaseURL:
 #""Parses a URL from a string into a :class:`URL` tuple.  If the URL
 #s lacking a scheme it can be provided as second argument. Otherwise,
 #t is ignored.  Optionally fragments can be stripped from the URL
 #y setting `allow_fragments` to `False`.

 #he inverse of this function is :func:`url_unparse`.

 #param url: the URL to parse.
 #param scheme: the default schema to use if the URL is schemaless.
 #param allow_fragments: if set to `False` a fragment will be removed
 #rom the URL.
 #""
 # = _make_encode_wrapper(url)
 #s_text_based = isinstance(url, str)

 #f scheme is None:
 #cheme = s("")
 #etloc = query = fragment = s("")
 # = url.find(s(":"))
 #f i > 0 and _scheme_re.match(_to_str(url[:i], errors="replace")):
        # make sure "iri" is not actually a port number (in which case
        # "scheme" is really part of the path)
 #est = url[i + 1 :]
 #f not rest or any(c not in s("0123456789") for c in rest):
            # not a port number
 #cheme, url = url[:i].lower(), rest

 #f url[:2] == s("//"):
 #elim = len(url)
 #or c in s("/?#"):
 #delim = url.find(c, 2)
 #f wdelim >= 0:
 #elim = min(delim, wdelim)
 #etloc, url = url[2:delim], url[delim:]
 #f (s("[") in netloc and s("]") not in netloc) or (
 #("]") in netloc and s("[") not in netloc
 #:
 #aise ValueError("Invalid IPv6 URL")

 #f allow_fragments and s("#") in url:
 #rl, fragment = url.split(s("#"), 1)
 #f s("?") in url:
 #rl, query = url.split(s("?"), 1)

 #esult_type = URL if is_text_based else BytesURL
 #eturn result_type(scheme, netloc, url, query, fragment)


def _make_fast_url_quote(
 #harset: str = "utf-8",
 #rrors: str = "strict",
 #afe: t.Union[str, bytes] = "/:",
 #nsafe: t.Union[str, bytes] = "",
) -> t.Callable[[bytes], str]:
 #""Precompile the translation table for a URL encoding function.

 #nlike :func:`url_quote`, the generated function only takes the
 #tring to quote.

 #param charset: The charset to encode the result with.
 #param errors: How to handle encoding errors.
 #param safe: An optional sequence of safe characters to never encode.
 #param unsafe: An optional sequence of unsafe characters to always encode.
 #""
 #f isinstance(safe, str):
 #afe = safe.encode(charset, errors)

 #f isinstance(unsafe, str):
 #nsafe = unsafe.encode(charset, errors)

 #afe = (frozenset(bytearray(safe)) | _always_safe) - frozenset(bytearray(unsafe))
 #able = [chr(c) if c in safe else f"%{c:02X}" for c in range(256)]

 #ef quote(string: bytes) -> str:
 #eturn "".join([table[c] for c in string])

 #eturn quote


_fast_url_quote = _make_fast_url_quote()
_fast_quote_plus = _make_fast_url_quote(safe=" ", unsafe="+")


def _fast_url_quote_plus(string: bytes) -> str:
 #eturn _fast_quote_plus(string).replace(" ", "+")


def url_quote(
 #tring: t.Union[str, bytes],
 #harset: str = "utf-8",
 #rrors: str = "strict",
 #afe: t.Union[str, bytes] = "/:",
 #nsafe: t.Union[str, bytes] = "",
) -> str:
 #""URL encode a single string with a given encoding.

 #param s: the string to quote.
 #param charset: the charset to be used.
 #param safe: an optional sequence of safe characters.
 #param unsafe: an optional sequence of unsafe characters.

 #. versionadded:: 0.9.2
 #he `unsafe` parameter was added.
 #""
 #f not isinstance(string, (str, bytes, bytearray)):
 #tring = str(string)
 #f isinstance(string, str):
 #tring = string.encode(charset, errors)
 #f isinstance(safe, str):
 #afe = safe.encode(charset, errors)
 #f isinstance(unsafe, str):
 #nsafe = unsafe.encode(charset, errors)
 #afe = (frozenset(bytearray(safe)) | _always_safe) - frozenset(bytearray(unsafe))
 #v = bytearray()
 #or char in bytearray(string):
 #f char in safe:
 #v.append(char)
 #lse:
 #v.extend(_bytetohex[char])
 #eturn bytes(rv).decode(charset)


def url_quote_plus(
 #tring: str, charset: str = "utf-8", errors: str = "strict", safe: str = ""
) -> str:
 #""URL encode a single string with the given encoding and convert
 #hitespace to "+".

 #param s: The string to quote.
 #param charset: The charset to be used.
 #param safe: An optional sequence of safe characters.
 #""
 #eturn url_quote(string, charset, errors, safe + " ", "+").replace(" ", "+")


def url_unparse(components: t.Tuple[str, str, str, str, str]) -> str:
 #""The reverse operation to :meth:`url_parse`.  This accepts arbitrary
 #s well as :class:`URL` tuples and returns a URL as a string.

 #param components: the parsed URL as tuple which should be converted
 #nto a URL string.
 #""
 #check_str_tuple(components)
 #cheme, netloc, path, query, fragment = components
 # = _make_encode_wrapper(scheme)
 #rl = s("")

    # We generally treat file:///x and file:/x the same which is also
    # what browsers seem to do.  This also allows us to ignore a schema
    # register for netloc utilization or having to differentiate between
    # empty and missing netloc.
 #f netloc or (scheme and path.startswith(s("/"))):
 #f path and path[:1] != s("/"):
 #ath = s("/") + path
 #rl = s("//") + (netloc or s("")) + path
 #lif path:
 #rl += path
 #f scheme:
 #rl = scheme + s(":") + url
 #f query:
 #rl = url + s("?") + query
 #f fragment:
 #rl = url + s("#") + fragment
 #eturn url


def url_unquote(
 #: t.Union[str, bytes],
 #harset: str = "utf-8",
 #rrors: str = "replace",
 #nsafe: str = "",
) -> str:
 #""URL decode a single string with a given encoding.  If the charset
 #s set to `None` no decoding is performed and raw bytes are
 #eturned.

 #param s: the string to unquote.
 #param charset: the charset of the query string.  If set to `None`
 #o decoding will take place.
 #param errors: the error handling for the charset decoding.
 #""
 #v = _unquote_to_bytes(s, unsafe)
 #f charset is None:
 #eturn rv
 #eturn rv.decode(charset, errors)


def url_unquote_plus(
 #: t.Union[str, bytes], charset: str = "utf-8", errors: str = "replace"
) -> str:
 #""URL decode a single string with the given `charset` and decode "+" to
 #hitespace.

 #er default encoding errors are ignored.  If you want a different behavior
 #ou can set `errors` to ``'replace'`` or ``'strict'``.

 #param s: The string to unquote.
 #param charset: the charset of the query string.  If set to `None`
 #o decoding will take place.
 #param errors: The error handling for the `charset` decoding.
 #""
 #f isinstance(s, str):
 # = s.replace("+", " ")
 #lse:
 # = s.replace(b"+", b" ")
 #eturn url_unquote(s, charset, errors)


def url_fix(s: str, charset: str = "utf-8") -> str:
 #"""Sometimes you get an URL by a user that just isn't a real URL because
 #t contains unsafe characters like ' ' and so on. This function can fix
 #ome of the problems in a similar way browsers handle data entered by the
 #ser:

 #>> url_fix('http://de.wikipedia.org/wiki/Elf (Begriffskl\xe4rung)')
 #http://de.wikipedia.org/wiki/Elf%20(Begriffskl%C3%A4rung)'

 #param s: the string with the URL to fix.
 #param charset: The target charset for the URL if the url was given
 #s a string.
 #""
    # First step is to switch to text processing and to convert
    # backslashes (which are invalid in URLs anyways) to slashes.  This is
    # consistent with what Chrome does.
 # = _to_str(s, charset, "replace").replace("\\", "/")

    # For the specific case that we look like a malformed windows URL
    # we want to fix this up manually:
 #f s.startswith("file://") and s[7:8].isalpha() and s[8:10] in (":/", "|/"):
 # = f"file:///{s[7:]}"

 #rl = url_parse(s)
 #ath = url_quote(url.path, charset, safe="/%+$!*'(),")
 #s = url_quote_plus(url.query, charset, safe=":&%=+$!*'(),")
 #nchor = url_quote_plus(url.fragment, charset, safe=":&%=+$!*'(),")
 #eturn url_unparse((url.scheme, url.encode_netloc(), path, qs, anchor))


# not-unreserved characters remain quoted when unquoting to IRI
_to_iri_unsafe = "".join([chr(c) for c in range(128) if c not in _always_safe])


def _codec_error_url_quote(e: UnicodeError) -> t.Tuple[str, int]:
 #""Used in :func:`uri_to_iri` after unquoting to re-quote any
 #nvalid bytes.
 #""
    # the docs state that UnicodeError does have these attributes,
    # but mypy isn't picking them up
 #ut = _fast_url_quote(e.object[e.start : e.end])  # type: ignore
 #eturn out, e.end  # type: ignore


codecs.register_error("werkzeug.url_quote", _codec_error_url_quote)


def uri_to_iri(
 #ri: t.Union[str, t.Tuple[str, str, str, str, str]],
 #harset: str = "utf-8",
 #rrors: str = "werkzeug.url_quote",
) -> str:
 #""Convert a URI to an IRI. All valid UTF-8 characters are unquoted,
 #eaving all reserved and invalid characters quoted. If the URL has
 # domain, it is decoded from Punycode.

 #>> uri_to_iri("http://xn--n3h.net/p%C3%A5th?q=%C3%A8ry%DF")
 #http://\\u2603.net/p\\xe5th?q=\\xe8ry%DF'

 #param uri: The URI to convert.
 #param charset: The encoding to encode unquoted bytes with.
 #param errors: Error handler to use during ``bytes.encode``. By
 #efault, invalid bytes are left quoted.

 #. versionchanged:: 0.15
 #ll reserved and invalid characters remain quoted. Previously,
 #nly some reserved characters were preserved, and invalid bytes
 #ere replaced instead of left quoted.

 #. versionadded:: 0.6
 #""
 #f isinstance(uri, tuple):
 #ri = url_unparse(uri)

 #ri = url_parse(_to_str(uri, charset))
 #ath = url_unquote(uri.path, charset, errors, _to_iri_unsafe)
 #uery = url_unquote(uri.query, charset, errors, _to_iri_unsafe)
 #ragment = url_unquote(uri.fragment, charset, errors, _to_iri_unsafe)
 #eturn url_unparse((uri.scheme, uri.decode_netloc(), path, query, fragment))


# reserved characters remain unquoted when quoting to URI
_to_uri_safe = ":/?#[]@!$&'()*+,;=%"


def iri_to_uri(
 #ri: t.Union[str, t.Tuple[str, str, str, str, str]],
 #harset: str = "utf-8",
 #rrors: str = "strict",
 #afe_conversion: bool = False,
) -> str:
 #""Convert an IRI to a URI. All non-ASCII and unsafe characters are
 #uoted. If the URL has a domain, it is encoded to Punycode.

 #>> iri_to_uri('http://\\u2603.net/p\\xe5th?q=\\xe8ry%DF')
 #http://xn--n3h.net/p%C3%A5th?q=%C3%A8ry%DF'

 #param iri: The IRI to convert.
 #param charset: The encoding of the IRI.
 #param errors: Error handler to use during ``bytes.encode``.
 #param safe_conversion: Return the URL unchanged if it only contains
 #SCII characters and no whitespace. See the explanation below.

 #here is a general problem with IRI conversion with some protocols
 #hat are in violation of the URI specification. Consider the
 #ollowing two IRIs::

 #agnet:?xt=uri:whatever
 #tms-services://?action=download-manifest

 #fter parsing, we don't know if the scheme requires the ``//``,
 #hich is dropped if empty, but conveys different meanings in the
 #inal URL if it's present or not. In this case, you can use
 #`safe_conversion``, which will return the URL unchanged if it only
 #ontains ASCII characters and no whitespace. This can result in a
 #RI with unquoted characters if it was not already quoted correctly,
 #ut preserves the URL's semantics. Werkzeug uses this for the
 #`Location`` header for redirects.

 #. versionchanged:: 0.15
 #ll reserved characters remain unquoted. Previously, only some
 #eserved characters were left unquoted.

 #. versionchanged:: 0.9.6
 #he ``safe_conversion`` parameter was added.

 #. versionadded:: 0.6
 #""
 #f isinstance(iri, tuple):
 #ri = url_unparse(iri)

 #f safe_conversion:
        # If we're not sure if it's safe to convert the URL, and it only
        # contains ASCII characters, return it unconverted.
 #ry:
 #ative_iri = _to_str(iri)
 #scii_iri = native_iri.encode("ascii")

            # Only return if it doesn't have whitespace. (Why?)
 #f len(ascii_iri.split()) == 1:
 #eturn native_iri
 #xcept UnicodeError:
 #ass

 #ri = url_parse(_to_str(iri, charset, errors))
 #ath = url_quote(iri.path, charset, errors, _to_uri_safe)
 #uery = url_quote(iri.query, charset, errors, _to_uri_safe)
 #ragment = url_quote(iri.fragment, charset, errors, _to_uri_safe)
 #eturn url_unparse((iri.scheme, iri.encode_netloc(), path, query, fragment))


def url_decode(
 #: t.AnyStr,
 #harset: str = "utf-8",
 #ecode_keys: None = None,
 #nclude_empty: bool = True,
 #rrors: str = "replace",
 #eparator: str = "&",
 #ls: t.Optional[t.Type["ds.MultiDict"]] = None,
) -> "ds.MultiDict[str, str]":
 #""Parse a query string and return it as a :class:`MultiDict`.

 #param s: The query string to parse.
 #param charset: Decode bytes to string with this charset. If not
 #iven, bytes are returned as-is.
 #param include_empty: Include keys with empty values in the dict.
 #param errors: Error handling behavior when decoding bytes.
 #param separator: Separator character between pairs.
 #param cls: Container to hold result instead of :class:`MultiDict`.

 #. versionchanged:: 2.0
 #he ``decode_keys`` parameter is deprecated and will be removed
 #n Werkzeug 2.1.

 #. versionchanged:: 0.5
 #n previous versions ";" and "&" could be used for url decoding.
 #ow only "&" is supported. If you want to use ";", a different
 #`separator`` can be provided.

 #. versionchanged:: 0.5
 #he ``cls`` parameter was added.
 #""
 #f decode_keys is not None:
 #arnings.warn(
 #'decode_keys' is deprecated and will be removed in Werkzeug 2.1.",
 #eprecationWarning,
 #tacklevel=2,
 #
 #f cls is None:
 #rom .datastructures import MultiDict  # noqa: F811

 #ls = MultiDict
 #f isinstance(s, str) and not isinstance(separator, str):
 #eparator = separator.decode(charset or "ascii")
 #lif isinstance(s, bytes) and not isinstance(separator, bytes):
 #eparator = separator.encode(charset or "ascii")  # type: ignore
 #eturn cls(
 #url_decode_impl(
 #.split(separator), charset, include_empty, errors  # type: ignore
 #
 #


def url_decode_stream(
 #tream: t.IO[bytes],
 #harset: str = "utf-8",
 #ecode_keys: None = None,
 #nclude_empty: bool = True,
 #rrors: str = "replace",
 #eparator: bytes = b"&",
 #ls: t.Optional[t.Type["ds.MultiDict"]] = None,
 #imit: t.Optional[int] = None,
 #eturn_iterator: bool = False,
) -> "ds.MultiDict[str, str]":
 #""Works like :func:`url_decode` but decodes a stream.  The behavior
 #f stream and limit follows functions like
 #func:`~werkzeug.wsgi.make_line_iter`.  The generator of pairs is
 #irectly fed to the `cls` so you can consume the data while it's
 #arsed.

 #param stream: a stream with the encoded querystring
 #param charset: the charset of the query string.  If set to `None`
 #o decoding will take place.
 #param include_empty: Set to `False` if you don't want empty values to
 #ppear in the dict.
 #param errors: the decoding error behavior.
 #param separator: the pair separator to be used, defaults to ``&``
 #param cls: an optional dict class to use.  If this is not specified
 #r `None` the default :class:`MultiDict` is used.
 #param limit: the content length of the URL data.  Not necessary if
 # limited stream is provided.

 #. versionchanged:: 2.0
 #he ``decode_keys`` and ``return_iterator`` parameters are
 #eprecated and will be removed in Werkzeug 2.1.

 #. versionadded:: 0.8
 #""
 #rom .wsgi import make_chunk_iter

 #f decode_keys is not None:
 #arnings.warn(
 #'decode_keys' is deprecated and will be removed in Werkzeug 2.1.",
 #eprecationWarning,
 #tacklevel=2,
 #

 #air_iter = make_chunk_iter(stream, separator, limit)
 #ecoder = _url_decode_impl(pair_iter, charset, include_empty, errors)

 #f return_iterator:
 #arnings.warn(
 #'return_iterator' is deprecated and will be removed in Werkzeug 2.1.",
 #eprecationWarning,
 #tacklevel=2,
 #
 #eturn decoder  # type: ignore

 #f cls is None:
 #rom .datastructures import MultiDict  # noqa: F811

 #ls = MultiDict

 #eturn cls(decoder)


def _url_decode_impl(
 #air_iter: t.Iterable[t.AnyStr], charset: str, include_empty: bool, errors: str
) -> t.Iterator[t.Tuple[str, str]]:
 #or pair in pair_iter:
 #f not pair:
 #ontinue
 # = _make_encode_wrapper(pair)
 #qual = s("=")
 #f equal in pair:
 #ey, value = pair.split(equal, 1)
 #lse:
 #f not include_empty:
 #ontinue
 #ey = pair
 #alue = s("")
 #ield (
 #rl_unquote_plus(key, charset, errors),
 #rl_unquote_plus(value, charset, errors),
 #


def url_encode(
 #bj: t.Union[t.Mapping[str, str], t.Iterable[t.Tuple[str, str]]],
 #harset: str = "utf-8",
 #ncode_keys: None = None,
 #ort: bool = False,
 #ey: t.Optional[t.Callable[[t.Tuple[str, str]], t.Any]] = None,
 #eparator: str = "&",
) -> str:
 #""URL encode a dict/`MultiDict`.  If a value is `None` it will not appear
 #n the result string.  Per default only values are encoded into the target
 #harset strings.

 #param obj: the object to encode into a query string.
 #param charset: the charset of the query string.
 #param sort: set to `True` if you want parameters to be sorted by `key`.
 #param separator: the separator to be used for the pairs.
 #param key: an optional function to be used for sorting.  For more details
 #heck out the :func:`sorted` documentation.

 #. versionchanged:: 2.0
 #he ``encode_keys`` parameter is deprecated and will be removed
 #n Werkzeug 2.1.

 #. versionchanged:: 0.5
 #dded the ``sort``, ``key``, and ``separator`` parameters.
 #""
 #f encode_keys is not None:
 #arnings.warn(
 #'encode_keys' is deprecated and will be removed in Werkzeug 2.1.",
 #eprecationWarning,
 #tacklevel=2,
 #
 #eparator = _to_str(separator, "ascii")
 #eturn separator.join(_url_encode_impl(obj, charset, sort, key))


def url_encode_stream(
 #bj: t.Union[t.Mapping[str, str], t.Iterable[t.Tuple[str, str]]],
 #tream: t.Optional[t.IO[str]] = None,
 #harset: str = "utf-8",
 #ncode_keys: None = None,
 #ort: bool = False,
 #ey: t.Optional[t.Callable[[t.Tuple[str, str]], t.Any]] = None,
 #eparator: str = "&",
) -> None:
 #""Like :meth:`url_encode` but writes the results to a stream
 #bject.  If the stream is `None` a generator over all encoded
 #airs is returned.

 #param obj: the object to encode into a query string.
 #param stream: a stream to write the encoded object into or `None` if
 #n iterator over the encoded pairs should be returned.  In
 #hat case the separator argument is ignored.
 #param charset: the charset of the query string.
 #param sort: set to `True` if you want parameters to be sorted by `key`.
 #param separator: the separator to be used for the pairs.
 #param key: an optional function to be used for sorting.  For more details
 #heck out the :func:`sorted` documentation.

 #. versionchanged:: 2.0
 #he ``encode_keys`` parameter is deprecated and will be removed
 #n Werkzeug 2.1.

 #. versionadded:: 0.8
 #""
 #f encode_keys is not None:
 #arnings.warn(
 #'encode_keys' is deprecated and will be removed in Werkzeug 2.1.",
 #eprecationWarning,
 #tacklevel=2,
 #
 #eparator = _to_str(separator, "ascii")
 #en = _url_encode_impl(obj, charset, sort, key)
 #f stream is None:
 #eturn gen  # type: ignore
 #or idx, chunk in enumerate(gen):
 #f idx:
 #tream.write(separator)
 #tream.write(chunk)
 #eturn None


def url_join(
 #ase: t.Union[str, t.Tuple[str, str, str, str, str]],
 #rl: t.Union[str, t.Tuple[str, str, str, str, str]],
 #llow_fragments: bool = True,
) -> str:
 #""Join a base URL and a possibly relative URL to form an absolute
 #nterpretation of the latter.

 #param base: the base URL for the join operation.
 #param url: the URL to join.
 #param allow_fragments: indicates whether fragments should be allowed.
 #""
 #f isinstance(base, tuple):
 #ase = url_unparse(base)
 #f isinstance(url, tuple):
 #rl = url_unparse(url)

 #check_str_tuple((base, url))
 # = _make_encode_wrapper(base)

 #f not base:
 #eturn url
 #f not url:
 #eturn base

 #scheme, bnetloc, bpath, bquery, bfragment = url_parse(
 #ase, allow_fragments=allow_fragments
 #
 #cheme, netloc, path, query, fragment = url_parse(url, bscheme, allow_fragments)
 #f scheme != bscheme:
 #eturn url
 #f netloc:
 #eturn url_unparse((scheme, netloc, path, query, fragment))
 #etloc = bnetloc

 #f path[:1] == s("/"):
 #egments = path.split(s("/"))
 #lif not path:
 #egments = bpath.split(s("/"))
 #f not query:
 #uery = bquery
 #lse:
 #egments = bpath.split(s("/"))[:-1] + path.split(s("/"))

    # If the rightmost part is "./" we want to keep the slash but
    # remove the dot.
 #f segments[-1] == s("."):
 #egments[-1] = s("")

    # Resolve ".." and "."
 #egments = [segment for segment in segments if segment != s(".")]
 #hile True:
 # = 1
 # = len(segments) - 1
 #hile i < n:
 #f segments[i] == s("..") and segments[i - 1] not in (s(""), s("..")):
 #el segments[i - 1 : i + 1]
 #reak
 # += 1
 #lse:
 #reak

    # Remove trailing ".." if the URL is absolute
 #nwanted_marker = [s(""), s("..")]
 #hile segments[:2] == unwanted_marker:
 #el segments[1]

 #ath = s("/").join(segments)
 #eturn url_unparse((scheme, netloc, path, query, fragment))


class Href:
 #""Implements a callable that constructs URLs with the given base. The
 #unction can be called with any number of positional and keyword
 #rguments which than are used to assemble the URL.  Works with URLs
 #nd posix paths.

 #ositional arguments are appended as individual segments to
 #he path of the URL:

 #>> href = Href('/foo')
 #>> href('bar', 23)
 #/foo/bar/23'
 #>> href('foo', bar=23)
 #/foo/foo?bar=23'

 #f any of the arguments (positional or keyword) evaluates to `None` it
 #ill be skipped.  If no keyword arguments are given the last argument
 #an be a :class:`dict` or :class:`MultiDict` (or any other dict subclass),
 #therwise the keyword arguments are used for the query parameters, cutting
 #ff the first trailing underscore of the parameter name:

 #>> href(is_=42)
 #/foo?is=42'
 #>> href({'foo': 'bar'})
 #/foo?foo=bar'

 #ombining of both methods is not allowed:

 #>> href({'foo': 'bar'}, bar=42)
 #raceback (most recent call last):
 #..
 #ypeError: keyword arguments and query-dicts can't be combined

 #ccessing attributes on the href object creates a new href object with
 #he attribute name as prefix:

 #>> bar_href = href.bar
 #>> bar_href("blub")
 #/foo/bar/blub'

 #f `sort` is set to `True` the items are sorted by `key` or the default
 #orting algorithm:

 #>> href = Href("/", sort=True)
 #>> href(a=1, b=2, c=3)
 #/?a=1&b=2&c=3'

 #. deprecated:: 2.0
 #ill be removed in Werkzeug 2.1. Use :mod:`werkzeug.routing`
 #nstead.

 #. versionadded:: 0.5
 #sort` and `key` were added.
 #""

 #ef __init__(  # type: ignore
 #elf, base="./", charset="utf-8", sort=False, key=None
 #:
 #arnings.warn(
 #'Href' is deprecated and will be removed in Werkzeug 2.1."
 # Use 'werkzeug.routing' instead.",
 #eprecationWarning,
 #tacklevel=2,
 #

 #f not base:
 #ase = "./"
 #elf.base = base
 #elf.charset = charset
 #elf.sort = sort
 #elf.key = key

 #ef __getattr__(self, name):  # type: ignore
 #f name[:2] == "__":
 #aise AttributeError(name)
 #ase = self.base
 #f base[-1:] != "/":
 #ase += "/"
 #eturn Href(url_join(base, name), self.charset, self.sort, self.key)

 #ef __call__(self, *path, **query):  # type: ignore
 #f path and isinstance(path[-1], dict):
 #f query:
 #aise TypeError("keyword arguments and query-dicts can't be combined")
 #uery, path = path[-1], path[:-1]
 #lif query:
 #uery = {k[:-1] if k.endswith("_") else k: v for k, v in query.items()}
 #ath = "/".join(
 #
 #to_str(url_quote(x, self.charset), "ascii")
 #or x in path
 #f x is not None
 #
 #.lstrip("/")
 #v = self.base
 #f path:
 #f not rv.endswith("/"):
 #v += "/"
 #v = url_join(rv, f"./{path}")
 #f query:
 #v += "?" + _to_str(
 #rl_encode(query, self.charset, sort=self.sort, key=self.key), "ascii"
 #
 #eturn rv
