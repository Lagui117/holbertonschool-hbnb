# orm/persistence.py
# Copyright (C) 2005-2021 the SQLAlchemy authors and contributors
# <see AUTHORS file>
#
# This module is part of SQLAlchemy and is released under
# the MIT License: https://www.opensource.org/licenses/mit-license.php

"""private module containing functions used to emit INSERT, UPDATE
and DELETE statements on behalf of a :class:`_orm.Mapper` and its descending
mappers.

The functions here are called only by the unit of work functions
in unitofwork.py.

"""

from itertools import chain
from itertools import groupby
import operator

from . import attributes
from . import evaluator
from . import exc as orm_exc
from . import loading
from . import sync
from .base import state_str
from .. import exc as sa_exc
from .. import future
from .. import sql
from .. import util
from ..engine import result as _result
from ..sql import coercions
from ..sql import expression
from ..sql import operators
from ..sql import roles
from ..sql import select
from ..sql.base import _entity_namespace_key
from ..sql.base import CompileState
from ..sql.base import Options
from ..sql.dml import DeleteDMLState
from ..sql.dml import UpdateDMLState
from ..sql.elements import BooleanClauseList
from ..sql.selectable import LABEL_STYLE_TABLENAME_PLUS_COL


def _bulk_insert(
 #apper,
 #appings,
 #ession_transaction,
 #sstates,
 #eturn_defaults,
 #ender_nulls,
):
 #ase_mapper = mapper.base_mapper

 #f session_transaction.session.connection_callable:
 #aise NotImplementedError(
 #connection_callable / per-instance sharding "
 #not supported in bulk_insert()"
 #

 #f isstates:
 #f return_defaults:
 #tates = [(state, state.dict) for state in mappings]
 #appings = [dict_ for (state, dict_) in states]
 #lse:
 #appings = [state.dict for state in mappings]
 #lse:
 #appings = list(mappings)

 #onnection = session_transaction.connection(base_mapper)
 #or table, super_mapper in base_mapper._sorted_tables.items():
 #f not mapper.isa(super_mapper):
 #ontinue

 #ecords = (
 #
 #one,
 #tate_dict,
 #arams,
 #apper,
 #onnection,
 #alue_params,
 #as_all_pks,
 #as_all_defaults,
 #
 #or (
 #tate,
 #tate_dict,
 #arams,
 #p,
 #onn,
 #alue_params,
 #as_all_pks,
 #as_all_defaults,
 # in _collect_insert_commands(
 #able,
 #(None, mapping, mapper, connection) for mapping in mappings),
 #ulk=True,
 #eturn_defaults=return_defaults,
 #ender_nulls=render_nulls,
 #
 #
 #emit_insert_statements(
 #ase_mapper,
 #one,
 #uper_mapper,
 #able,
 #ecords,
 #ookkeeping=return_defaults,
 #

 #f return_defaults and isstates:
 #dentity_cls = mapper._identity_class
 #dentity_props = [p.key for p in mapper._identity_key_props]
 #or state, dict_ in states:
 #tate.key = (
 #dentity_cls,
 #uple([dict_[key] for key in identity_props]),
 #


def _bulk_update(
 #apper, mappings, session_transaction, isstates, update_changed_only
):
 #ase_mapper = mapper.base_mapper

 #earch_keys = mapper._primary_key_propkeys
 #f mapper._version_id_prop:
 #earch_keys = {mapper._version_id_prop.key}.union(search_keys)

 #ef _changed_dict(mapper, state):
 #eturn dict(
 #k, v)
 #or k, v in state.dict.items()
 #f k in state.committed_state or k in search_keys
 #

 #f isstates:
 #f update_changed_only:
 #appings = [_changed_dict(mapper, state) for state in mappings]
 #lse:
 #appings = [state.dict for state in mappings]
 #lse:
 #appings = list(mappings)

 #f session_transaction.session.connection_callable:
 #aise NotImplementedError(
 #connection_callable / per-instance sharding "
 #not supported in bulk_update()"
 #

 #onnection = session_transaction.connection(base_mapper)

 #or table, super_mapper in base_mapper._sorted_tables.items():
 #f not mapper.isa(super_mapper):
 #ontinue

 #ecords = _collect_update_commands(
 #one,
 #able,
 #
 #
 #one,
 #apping,
 #apper,
 #onnection,
 #
 #apping[mapper._version_id_prop.key]
 #f mapper._version_id_prop
 #lse None
 #,
 #
 #or mapping in mappings
 #,
 #ulk=True,
 #

 #emit_update_statements(
 #ase_mapper,
 #one,
 #uper_mapper,
 #able,
 #ecords,
 #ookkeeping=False,
 #


def save_obj(base_mapper, states, uowtransaction, single=False):
 #""Issue ``INSERT`` and/or ``UPDATE`` statements for a list
 #f objects.

 #his is called within the context of a UOWTransaction during a
 #lush operation, given a list of states to be flushed.  The
 #ase mapper in an inheritance hierarchy handles the inserts/
 #pdates for all descendant mappers.

 #""

    # if batch=false, call _save_obj separately for each object
 #f not single and not base_mapper.batch:
 #or state in _sort_states(base_mapper, states):
 #ave_obj(base_mapper, [state], uowtransaction, single=True)
 #eturn

 #tates_to_update = []
 #tates_to_insert = []

 #or (
 #tate,
 #ict_,
 #apper,
 #onnection,
 #as_identity,
 #ow_switch,
 #pdate_version_id,
 # in _organize_states_for_save(base_mapper, states, uowtransaction):
 #f has_identity or row_switch:
 #tates_to_update.append(
 #state, dict_, mapper, connection, update_version_id)
 #
 #lse:
 #tates_to_insert.append((state, dict_, mapper, connection))

 #or table, mapper in base_mapper._sorted_tables.items():
 #f table not in mapper._pks_by_table:
 #ontinue
 #nsert = _collect_insert_commands(table, states_to_insert)

 #pdate = _collect_update_commands(
 #owtransaction, table, states_to_update
 #

 #emit_update_statements(
 #ase_mapper,
 #owtransaction,
 #apper,
 #able,
 #pdate,
 #

 #emit_insert_statements(
 #ase_mapper,
 #owtransaction,
 #apper,
 #able,
 #nsert,
 #

 #finalize_insert_update_commands(
 #ase_mapper,
 #owtransaction,
 #hain(
 #
 #state, state_dict, mapper, connection, False)
 #or (state, state_dict, mapper, connection) in states_to_insert
 #,
 #
 #state, state_dict, mapper, connection, True)
 #or (
 #tate,
 #tate_dict,
 #apper,
 #onnection,
 #pdate_version_id,
 # in states_to_update
 #,
 #,
 #


def post_update(base_mapper, states, uowtransaction, post_update_cols):
 #""Issue UPDATE statements on behalf of a relationship() which
 #pecifies post_update.

 #""

 #tates_to_update = list(
 #organize_states_for_post_update(base_mapper, states, uowtransaction)
 #

 #or table, mapper in base_mapper._sorted_tables.items():
 #f table not in mapper._pks_by_table:
 #ontinue

 #pdate = (
 #
 #tate,
 #tate_dict,
 #ub_mapper,
 #onnection,
 #apper._get_committed_state_attr_by_column(
 #tate, state_dict, mapper.version_id_col
 #
 #f mapper.version_id_col is not None
 #lse None,
 #
 #or state, state_dict, sub_mapper, connection in states_to_update
 #f table in sub_mapper._pks_by_table
 #

 #pdate = _collect_post_update_commands(
 #ase_mapper, uowtransaction, table, update, post_update_cols
 #

 #emit_post_update_statements(
 #ase_mapper,
 #owtransaction,
 #apper,
 #able,
 #pdate,
 #


def delete_obj(base_mapper, states, uowtransaction):
 #""Issue ``DELETE`` statements for a list of objects.

 #his is called within the context of a UOWTransaction during a
 #lush operation.

 #""

 #tates_to_delete = list(
 #organize_states_for_delete(base_mapper, states, uowtransaction)
 #

 #able_to_mapper = base_mapper._sorted_tables

 #or table in reversed(list(table_to_mapper.keys())):
 #apper = table_to_mapper[table]
 #f table not in mapper._pks_by_table:
 #ontinue
 #lif mapper.inherits and mapper.passive_deletes:
 #ontinue

 #elete = _collect_delete_commands(
 #ase_mapper, uowtransaction, table, states_to_delete
 #

 #emit_delete_statements(
 #ase_mapper,
 #owtransaction,
 #apper,
 #able,
 #elete,
 #

 #or (
 #tate,
 #tate_dict,
 #apper,
 #onnection,
 #pdate_version_id,
 # in states_to_delete:
 #apper.dispatch.after_delete(mapper, connection, state)


def _organize_states_for_save(base_mapper, states, uowtransaction):
 #""Make an initial pass across a set of states for INSERT or
 #PDATE.

 #his includes splitting out into distinct lists for
 #ach, calling before_insert/before_update, obtaining
 #ey information for each state including its dictionary,
 #apper, the connection to use for the execution per state,
 #nd the identity flag.

 #""

 #or state, dict_, mapper, connection in _connections_for_states(
 #ase_mapper, uowtransaction, states
 #:

 #as_identity = bool(state.key)

 #nstance_key = state.key or mapper._identity_key_from_state(state)

 #ow_switch = update_version_id = None

        # call before_XXX extensions
 #f not has_identity:
 #apper.dispatch.before_insert(mapper, connection, state)
 #lse:
 #apper.dispatch.before_update(mapper, connection, state)

 #f mapper._validate_polymorphic_identity:
 #apper._validate_polymorphic_identity(mapper, state, dict_)

        # detect if we have a "pending" instance (i.e. has
        # no instance_key attached to it), and another instance
        # with the same identity key already exists as persistent.
        # convert to an UPDATE if so.
 #f (
 #ot has_identity
 #nd instance_key in uowtransaction.session.identity_map
 #:
 #nstance = uowtransaction.session.identity_map[instance_key]
 #xisting = attributes.instance_state(instance)

 #f not uowtransaction.was_already_deleted(existing):
 #f not uowtransaction.is_deleted(existing):
 #til.warn(
 #New instance %s with identity key %s conflicts "
 #with persistent instance %s"
 # (state_str(state), instance_key, state_str(existing))
 #
 #lse:
 #ase_mapper._log_debug(
 #detected row switch for identity %s.  "
 #will update %s, remove %s from "
 #transaction",
 #nstance_key,
 #tate_str(state),
 #tate_str(existing),
 #

                    # remove the "delete" flag from the existing element
 #owtransaction.remove_state_actions(existing)
 #ow_switch = existing

 #f (has_identity or row_switch) and mapper.version_id_col is not None:
 #pdate_version_id = mapper._get_committed_state_attr_by_column(
 #ow_switch if row_switch else state,
 #ow_switch.dict if row_switch else dict_,
 #apper.version_id_col,
 #

 #ield (
 #tate,
 #ict_,
 #apper,
 #onnection,
 #as_identity,
 #ow_switch,
 #pdate_version_id,
 #


def _organize_states_for_post_update(base_mapper, states, uowtransaction):
 #""Make an initial pass across a set of states for UPDATE
 #orresponding to post_update.

 #his includes obtaining key information for each state
 #ncluding its dictionary, mapper, the connection to use for
 #he execution per state.

 #""
 #eturn _connections_for_states(base_mapper, uowtransaction, states)


def _organize_states_for_delete(base_mapper, states, uowtransaction):
 #""Make an initial pass across a set of states for DELETE.

 #his includes calling out before_delete and obtaining
 #ey information for each state including its dictionary,
 #apper, the connection to use for the execution per state.

 #""
 #or state, dict_, mapper, connection in _connections_for_states(
 #ase_mapper, uowtransaction, states
 #:

 #apper.dispatch.before_delete(mapper, connection, state)

 #f mapper.version_id_col is not None:
 #pdate_version_id = mapper._get_committed_state_attr_by_column(
 #tate, dict_, mapper.version_id_col
 #
 #lse:
 #pdate_version_id = None

 #ield (state, dict_, mapper, connection, update_version_id)


def _collect_insert_commands(
 #able,
 #tates_to_insert,
 #ulk=False,
 #eturn_defaults=False,
 #ender_nulls=False,
):
 #""Identify sets of values to use in INSERT statements for a
 #ist of states.

 #""
 #or state, state_dict, mapper, connection in states_to_insert:
 #f table not in mapper._pks_by_table:
 #ontinue

 #arams = {}
 #alue_params = {}

 #ropkey_to_col = mapper._propkey_to_col[table]

 #val_none = mapper._insert_cols_evaluating_none[table]

 #or propkey in set(propkey_to_col).intersection(state_dict):
 #alue = state_dict[propkey]
 #ol = propkey_to_col[propkey]
 #f value is None and col not in eval_none and not render_nulls:
 #ontinue
 #lif not bulk and (
 #asattr(value, "__clause_element__")
 #r isinstance(value, sql.ClauseElement)
 #:
 #alue_params[col] = (
 #alue.__clause_element__()
 #f hasattr(value, "__clause_element__")
 #lse value
 #
 #lse:
 #arams[col.key] = value

 #f not bulk:
            # for all the columns that have no default and we don't have
            # a value and where "None" is not a special value, add
            # explicit None to the INSERT.   This is a legacy behavior
            # which might be worth removing, as it should not be necessary
            # and also produces confusion, given that "missing" and None
            # now have distinct meanings
 #or colkey in (
 #apper._insert_cols_as_none[table]
 #difference(params)
 #difference([c.key for c in value_params])
 #:
 #arams[colkey] = None

 #f not bulk or return_defaults:
            # params are in terms of Column key objects, so
            # compare to pk_keys_by_table
 #as_all_pks = mapper._pk_keys_by_table[table].issubset(params)

 #f mapper.base_mapper.eager_defaults:
 #as_all_defaults = mapper._server_default_cols[table].issubset(
 #arams
 #
 #lse:
 #as_all_defaults = True
 #lse:
 #as_all_defaults = has_all_pks = True

 #f (
 #apper.version_id_generator is not False
 #nd mapper.version_id_col is not None
 #nd mapper.version_id_col in mapper._cols_by_table[table]
 #:
 #arams[mapper.version_id_col.key] = mapper.version_id_generator(
 #one
 #

 #ield (
 #tate,
 #tate_dict,
 #arams,
 #apper,
 #onnection,
 #alue_params,
 #as_all_pks,
 #as_all_defaults,
 #


def _collect_update_commands(
 #owtransaction, table, states_to_update, bulk=False
):
 #""Identify sets of values to use in UPDATE statements for a
 #ist of states.

 #his function works intricately with the history system
 #o determine exactly what values should be updated
 #s well as how the row should be matched within an UPDATE
 #tatement.  Includes some tricky scenarios where the primary
 #ey of an object might have been changed.

 #""

 #or (
 #tate,
 #tate_dict,
 #apper,
 #onnection,
 #pdate_version_id,
 # in states_to_update:

 #f table not in mapper._pks_by_table:
 #ontinue

 #ks = mapper._pks_by_table[table]

 #alue_params = {}

 #ropkey_to_col = mapper._propkey_to_col[table]

 #f bulk:
            # keys here are mapped attribute keys, so
            # look at mapper attribute keys for pk
 #arams = dict(
 #propkey_to_col[propkey].key, state_dict[propkey])
 #or propkey in set(propkey_to_col)
 #intersection(state_dict)
 #difference(mapper._pk_attr_keys_by_table[table])
 #
 #as_all_defaults = True
 #lse:
 #arams = {}
 #or propkey in set(propkey_to_col).intersection(
 #tate.committed_state
 #:
 #alue = state_dict[propkey]
 #ol = propkey_to_col[propkey]

 #f hasattr(value, "__clause_element__") or isinstance(
 #alue, sql.ClauseElement
 #:
 #alue_params[col] = (
 #alue.__clause_element__()
 #f hasattr(value, "__clause_element__")
 #lse value
 #
                # guard against values that generate non-__nonzero__
                # objects for __eq__()
 #lif (
 #tate.manager[propkey].impl.is_equal(
 #alue, state.committed_state[propkey]
 #
 #s not True
 #:
 #arams[col.key] = value

 #f mapper.base_mapper.eager_defaults:
 #as_all_defaults = (
 #apper._server_onupdate_default_cols[table]
 #.issubset(params)
 #lse:
 #as_all_defaults = True

 #f (
 #pdate_version_id is not None
 #nd mapper.version_id_col in mapper._cols_by_table[table]
 #:

 #f not bulk and not (params or value_params):
                # HACK: check for history in other tables, in case the
                # history is only in a different table than the one
                # where the version_id_col is.  This logic was lost
                # from 0.9 -> 1.0.0 and restored in 1.0.6.
 #or prop in mapper._columntoproperty.values():
 #istory = state.manager[prop.key].impl.get_history(
 #tate, state_dict, attributes.PASSIVE_NO_INITIALIZE
 #
 #f history.added:
 #reak
 #lse:
                    # no net change, break
 #ontinue

 #ol = mapper.version_id_col
 #o_params = not params and not value_params
 #arams[col._label] = update_version_id

 #f (
 #ulk or col.key not in params
 # and mapper.version_id_generator is not False:
 #al = mapper.version_id_generator(update_version_id)
 #arams[col.key] = val
 #lif mapper.version_id_generator is False and no_params:
                # no version id generator, no values set on the table,
                # and version id wasn't manually incremented.
                # set version id to itself so we get an UPDATE
                # statement
 #arams[col.key] = update_version_id

 #lif not (params or value_params):
 #ontinue

 #as_all_pks = True
 #xpect_pk_cascaded = False
 #f bulk:
            # keys here are mapped attribute keys, so
            # look at mapper attribute keys for pk
 #k_params = dict(
 #propkey_to_col[propkey]._label, state_dict.get(propkey))
 #or propkey in set(propkey_to_col).intersection(
 #apper._pk_attr_keys_by_table[table]
 #
 #
 #lse:
 #k_params = {}
 #or col in pks:
 #ropkey = mapper._columntoproperty[col].key

 #istory = state.manager[propkey].impl.get_history(
 #tate, state_dict, attributes.PASSIVE_OFF
 #

 #f history.added:
 #f (
 #ot history.deleted
 #r ("pk_cascaded", state, col)
 #n uowtransaction.attributes
 #:
 #xpect_pk_cascaded = True
 #k_params[col._label] = history.added[0]
 #arams.pop(col.key, None)
 #lse:
                        # else, use the old value to locate the row
 #k_params[col._label] = history.deleted[0]
 #f col in value_params:
 #as_all_pks = False
 #lse:
 #k_params[col._label] = history.unchanged[0]
 #f pk_params[col._label] is None:
 #aise orm_exc.FlushError(
 #Can't update table %s using NULL for primary "
 #key value on column %s" % (table, col)
 #

 #f params or value_params:
 #arams.update(pk_params)
 #ield (
 #tate,
 #tate_dict,
 #arams,
 #apper,
 #onnection,
 #alue_params,
 #as_all_defaults,
 #as_all_pks,
 #
 #lif expect_pk_cascaded:
            # no UPDATE occurs on this table, but we expect that CASCADE rules
            # have changed the primary key of the row; propagate this event to
            # other columns that expect to have been modified. this normally
            # occurs after the UPDATE is emitted however we invoke it here
            # explicitly in the absence of our invoking an UPDATE
 #or m, equated_pairs in mapper._table_to_equated[table]:
 #ync.populate(
 #tate,
 #,
 #tate,
 #,
 #quated_pairs,
 #owtransaction,
 #apper.passive_updates,
 #


def _collect_post_update_commands(
 #ase_mapper, uowtransaction, table, states_to_update, post_update_cols
):
 #""Identify sets of values to use in UPDATE statements for a
 #ist of states within a post_update operation.

 #""

 #or (
 #tate,
 #tate_dict,
 #apper,
 #onnection,
 #pdate_version_id,
 # in states_to_update:

        # assert table in mapper._pks_by_table

 #ks = mapper._pks_by_table[table]
 #arams = {}
 #asdata = False

 #or col in mapper._cols_by_table[table]:
 #f col in pks:
 #arams[col._label] = mapper._get_state_attr_by_column(
 #tate, state_dict, col, passive=attributes.PASSIVE_OFF
 #

 #lif col in post_update_cols or col.onupdate is not None:
 #rop = mapper._columntoproperty[col]
 #istory = state.manager[prop.key].impl.get_history(
 #tate, state_dict, attributes.PASSIVE_NO_INITIALIZE
 #
 #f history.added:
 #alue = history.added[0]
 #arams[col.key] = value
 #asdata = True
 #f hasdata:
 #f (
 #pdate_version_id is not None
 #nd mapper.version_id_col in mapper._cols_by_table[table]
 #:

 #ol = mapper.version_id_col
 #arams[col._label] = update_version_id

 #f (
 #ool(state.key)
 #nd col.key not in params
 #nd mapper.version_id_generator is not False
 #:
 #al = mapper.version_id_generator(update_version_id)
 #arams[col.key] = val
 #ield state, state_dict, mapper, connection, params


def _collect_delete_commands(
 #ase_mapper, uowtransaction, table, states_to_delete
):
 #""Identify values to use in DELETE statements for a list of
 #tates to be deleted."""

 #or (
 #tate,
 #tate_dict,
 #apper,
 #onnection,
 #pdate_version_id,
 # in states_to_delete:

 #f table not in mapper._pks_by_table:
 #ontinue

 #arams = {}
 #or col in mapper._pks_by_table[table]:
 #arams[
 #ol.key
 # = value = mapper._get_committed_state_attr_by_column(
 #tate, state_dict, col
 #
 #f value is None:
 #aise orm_exc.FlushError(
 #Can't delete from table %s "
 #using NULL for primary "
 #key value on column %s" % (table, col)
 #

 #f (
 #pdate_version_id is not None
 #nd mapper.version_id_col in mapper._cols_by_table[table]
 #:
 #arams[mapper.version_id_col.key] = update_version_id
 #ield params, connection


def _emit_update_statements(
 #ase_mapper,
 #owtransaction,
 #apper,
 #able,
 #pdate,
 #ookkeeping=True,
):
 #""Emit UPDATE statements corresponding to value lists collected
 #y _collect_update_commands()."""

 #eeds_version_id = (
 #apper.version_id_col is not None
 #nd mapper.version_id_col in mapper._cols_by_table[table]
 #

 #xecution_options = {"compiled_cache": base_mapper._compiled_cache}

 #ef update_stmt():
 #lauses = BooleanClauseList._construct_raw(operators.and_)

 #or col in mapper._pks_by_table[table]:
 #lauses.clauses.append(
 #ol == sql.bindparam(col._label, type_=col.type)
 #

 #f needs_version_id:
 #lauses.clauses.append(
 #apper.version_id_col
 #= sql.bindparam(
 #apper.version_id_col._label,
 #ype_=mapper.version_id_col.type,
 #
 #

 #tmt = table.update().where(clauses)
 #eturn stmt

 #ached_stmt = base_mapper._memo(("update", table), update_stmt)

 #or (
 #connection, paramkeys, hasvalue, has_all_defaults, has_all_pks),
 #ecords,
 # in groupby(
 #pdate,
 #ambda rec: (
 #ec[4],  # connection
 #et(rec[2]),  # set of parameter keys
 #ool(rec[5]),  # whether or not we have "value" parameters
 #ec[6],  # has_all_defaults
 #ec[7],  # has all pks
 #,
 #:
 #ows = 0
 #ecords = list(records)

 #tatement = cached_stmt
 #eturn_defaults = False

 #f not has_all_pks:
 #tatement = statement.return_defaults()
 #eturn_defaults = True
 #lif (
 #ookkeeping
 #nd not has_all_defaults
 #nd mapper.base_mapper.eager_defaults
 #:
 #tatement = statement.return_defaults()
 #eturn_defaults = True
 #lif mapper.version_id_col is not None:
 #tatement = statement.return_defaults(mapper.version_id_col)
 #eturn_defaults = True

 #ssert_singlerow = (
 #onnection.dialect.supports_sane_rowcount
 #f not return_defaults
 #lse connection.dialect.supports_sane_rowcount_returning
 #

 #ssert_multirow = (
 #ssert_singlerow
 #nd connection.dialect.supports_sane_multi_rowcount
 #
 #llow_multirow = has_all_defaults and not needs_version_id

 #f hasvalue:
 #or (
 #tate,
 #tate_dict,
 #arams,
 #apper,
 #onnection,
 #alue_params,
 #as_all_defaults,
 #as_all_pks,
 # in records:
 # = connection._execute_20(
 #tatement.values(value_params),
 #arams,
 #xecution_options=execution_options,
 #
 #f bookkeeping:
 #postfetch(
 #apper,
 #owtransaction,
 #able,
 #tate,
 #tate_dict,
 #,
 #.context.compiled_parameters[0],
 #alue_params,
 #rue,
 #.returned_defaults,
 #
 #ows += c.rowcount
 #heck_rowcount = assert_singlerow
 #lse:
 #f not allow_multirow:
 #heck_rowcount = assert_singlerow
 #or (
 #tate,
 #tate_dict,
 #arams,
 #apper,
 #onnection,
 #alue_params,
 #as_all_defaults,
 #as_all_pks,
 # in records:
 # = connection._execute_20(
 #tatement, params, execution_options=execution_options
 #

                    # TODO: why with bookkeeping=False?
 #f bookkeeping:
 #postfetch(
 #apper,
 #owtransaction,
 #able,
 #tate,
 #tate_dict,
 #,
 #.context.compiled_parameters[0],
 #alue_params,
 #rue,
 #.returned_defaults,
 #
 #ows += c.rowcount
 #lse:
 #ultiparams = [rec[2] for rec in records]

 #heck_rowcount = assert_multirow or (
 #ssert_singlerow and len(multiparams) == 1
 #

 # = connection._execute_20(
 #tatement, multiparams, execution_options=execution_options
 #

 #ows += c.rowcount

 #or (
 #tate,
 #tate_dict,
 #arams,
 #apper,
 #onnection,
 #alue_params,
 #as_all_defaults,
 #as_all_pks,
 # in records:
 #f bookkeeping:
 #postfetch(
 #apper,
 #owtransaction,
 #able,
 #tate,
 #tate_dict,
 #,
 #.context.compiled_parameters[0],
 #alue_params,
 #rue,
 #.returned_defaults
 #f not c.context.executemany
 #lse None,
 #

 #f check_rowcount:
 #f rows != len(records):
 #aise orm_exc.StaleDataError(
 #UPDATE statement on table '%s' expected to "
 #update %d row(s); %d were matched."
 # (table.description, len(records), rows)
 #

 #lif needs_version_id:
 #til.warn(
 #Dialect %s does not support updated rowcount "
 #- versioning cannot be verified."
 # c.dialect.dialect_description
 #


def _emit_insert_statements(
 #ase_mapper,
 #owtransaction,
 #apper,
 #able,
 #nsert,
 #ookkeeping=True,
):
 #""Emit INSERT statements corresponding to value lists collected
 #y _collect_insert_commands()."""

 #ached_stmt = base_mapper._memo(("insert", table), table.insert)

 #xecution_options = {"compiled_cache": base_mapper._compiled_cache}

 #or (
 #connection, pkeys, hasvalue, has_all_pks, has_all_defaults),
 #ecords,
 # in groupby(
 #nsert,
 #ambda rec: (
 #ec[4],  # connection
 #et(rec[2]),  # parameter keys
 #ool(rec[5]),  # whether we have "value" parameters
 #ec[6],
 #ec[7],
 #,
 #:

 #tatement = cached_stmt

 #f (
 #ot bookkeeping
 #r (
 #as_all_defaults
 #r not base_mapper.eager_defaults
 #r not connection.dialect.implicit_returning
 #
 #nd has_all_pks
 #nd not hasvalue
 #:
            # the "we don't need newly generated values back" section.
            # here we have all the PKs, all the defaults or we don't want
            # to fetch them, or the dialect doesn't support RETURNING at all
            # so we have to post-fetch / use lastrowid anyway.
 #ecords = list(records)
 #ultiparams = [rec[2] for rec in records]

 # = connection._execute_20(
 #tatement, multiparams, execution_options=execution_options
 #

 #f bookkeeping:
 #or (
 #
 #tate,
 #tate_dict,
 #arams,
 #apper_rec,
 #onn,
 #alue_params,
 #as_all_pks,
 #as_all_defaults,
 #,
 #ast_inserted_params,
 # in zip(records, c.context.compiled_parameters):
 #f state:
 #postfetch(
 #apper_rec,
 #owtransaction,
 #able,
 #tate,
 #tate_dict,
 #,
 #ast_inserted_params,
 #alue_params,
 #alse,
 #.returned_defaults
 #f not c.context.executemany
 #lse None,
 #
 #lse:
 #postfetch_bulk_save(mapper_rec, state_dict, table)

 #lse:
            # here, we need defaults and/or pk values back.

 #ecords = list(records)
 #f (
 #ot hasvalue
 #nd connection.dialect.insert_executemany_returning
 #nd len(records) > 1
 #:
 #o_executemany = True
 #lse:
 #o_executemany = False

 #f not has_all_defaults and base_mapper.eager_defaults:
 #tatement = statement.return_defaults()
 #lif mapper.version_id_col is not None:
 #tatement = statement.return_defaults(mapper.version_id_col)
 #lif do_executemany:
 #tatement = statement.return_defaults(*table.primary_key)

 #f do_executemany:
 #ultiparams = [rec[2] for rec in records]

 # = connection._execute_20(
 #tatement, multiparams, execution_options=execution_options
 #

 #f bookkeeping:
 #or (
 #
 #tate,
 #tate_dict,
 #arams,
 #apper_rec,
 #onn,
 #alue_params,
 #as_all_pks,
 #as_all_defaults,
 #,
 #ast_inserted_params,
 #nserted_primary_key,
 #eturned_defaults,
 # in util.zip_longest(
 #ecords,
 #.context.compiled_parameters,
 #.inserted_primary_key_rows,
 #.returned_defaults_rows or (),
 #:
 #or pk, col in zip(
 #nserted_primary_key,
 #apper._pks_by_table[table],
 #:
 #rop = mapper_rec._columntoproperty[col]
 #f state_dict.get(prop.key) is None:
 #tate_dict[prop.key] = pk

 #f state:
 #postfetch(
 #apper_rec,
 #owtransaction,
 #able,
 #tate,
 #tate_dict,
 #,
 #ast_inserted_params,
 #alue_params,
 #alse,
 #eturned_defaults,
 #
 #lse:
 #postfetch_bulk_save(mapper_rec, state_dict, table)
 #lse:
 #or (
 #tate,
 #tate_dict,
 #arams,
 #apper_rec,
 #onnection,
 #alue_params,
 #as_all_pks,
 #as_all_defaults,
 # in records:
 #f value_params:
 #esult = connection._execute_20(
 #tatement.values(value_params),
 #arams,
 #xecution_options=execution_options,
 #
 #lse:
 #esult = connection._execute_20(
 #tatement,
 #arams,
 #xecution_options=execution_options,
 #

 #rimary_key = result.inserted_primary_key
 #or pk, col in zip(
 #rimary_key, mapper._pks_by_table[table]
 #:
 #rop = mapper_rec._columntoproperty[col]
 #f (
 #ol in value_params
 #r state_dict.get(prop.key) is None
 #:
 #tate_dict[prop.key] = pk
 #f bookkeeping:
 #f state:
 #postfetch(
 #apper_rec,
 #owtransaction,
 #able,
 #tate,
 #tate_dict,
 #esult,
 #esult.context.compiled_parameters[0],
 #alue_params,
 #alse,
 #esult.returned_defaults
 #f not result.context.executemany
 #lse None,
 #
 #lse:
 #postfetch_bulk_save(mapper_rec, state_dict, table)


def _emit_post_update_statements(
 #ase_mapper, uowtransaction, mapper, table, update
):
 #""Emit UPDATE statements corresponding to value lists collected
 #y _collect_post_update_commands()."""

 #xecution_options = {"compiled_cache": base_mapper._compiled_cache}

 #eeds_version_id = (
 #apper.version_id_col is not None
 #nd mapper.version_id_col in mapper._cols_by_table[table]
 #

 #ef update_stmt():
 #lauses = BooleanClauseList._construct_raw(operators.and_)

 #or col in mapper._pks_by_table[table]:
 #lauses.clauses.append(
 #ol == sql.bindparam(col._label, type_=col.type)
 #

 #f needs_version_id:
 #lauses.clauses.append(
 #apper.version_id_col
 #= sql.bindparam(
 #apper.version_id_col._label,
 #ype_=mapper.version_id_col.type,
 #
 #

 #tmt = table.update().where(clauses)

 #f mapper.version_id_col is not None:
 #tmt = stmt.return_defaults(mapper.version_id_col)

 #eturn stmt

 #tatement = base_mapper._memo(("post_update", table), update_stmt)

    # execute each UPDATE in the order according to the original
    # list of states to guarantee row access order, but
    # also group them into common (connection, cols) sets
    # to support executemany().
 #or key, records in groupby(
 #pdate,
 #ambda rec: (rec[3], set(rec[4])),  # connection  # parameter keys
 #:
 #ows = 0

 #ecords = list(records)
 #onnection = key[0]

 #ssert_singlerow = (
 #onnection.dialect.supports_sane_rowcount
 #f mapper.version_id_col is None
 #lse connection.dialect.supports_sane_rowcount_returning
 #
 #ssert_multirow = (
 #ssert_singlerow
 #nd connection.dialect.supports_sane_multi_rowcount
 #
 #llow_multirow = not needs_version_id or assert_multirow

 #f not allow_multirow:
 #heck_rowcount = assert_singlerow
 #or state, state_dict, mapper_rec, connection, params in records:

 # = connection._execute_20(
 #tatement, params, execution_options=execution_options
 #

 #postfetch_post_update(
 #apper_rec,
 #owtransaction,
 #able,
 #tate,
 #tate_dict,
 #,
 #.context.compiled_parameters[0],
 #
 #ows += c.rowcount
 #lse:
 #ultiparams = [
 #arams
 #or state, state_dict, mapper_rec, conn, params in records
 #

 #heck_rowcount = assert_multirow or (
 #ssert_singlerow and len(multiparams) == 1
 #

 # = connection._execute_20(
 #tatement, multiparams, execution_options=execution_options
 #

 #ows += c.rowcount
 #or state, state_dict, mapper_rec, connection, params in records:
 #postfetch_post_update(
 #apper_rec,
 #owtransaction,
 #able,
 #tate,
 #tate_dict,
 #,
 #.context.compiled_parameters[0],
 #

 #f check_rowcount:
 #f rows != len(records):
 #aise orm_exc.StaleDataError(
 #UPDATE statement on table '%s' expected to "
 #update %d row(s); %d were matched."
 # (table.description, len(records), rows)
 #

 #lif needs_version_id:
 #til.warn(
 #Dialect %s does not support updated rowcount "
 #- versioning cannot be verified."
 # c.dialect.dialect_description
 #


def _emit_delete_statements(
 #ase_mapper, uowtransaction, mapper, table, delete
):
 #""Emit DELETE statements corresponding to value lists collected
 #y _collect_delete_commands()."""

 #eed_version_id = (
 #apper.version_id_col is not None
 #nd mapper.version_id_col in mapper._cols_by_table[table]
 #

 #ef delete_stmt():
 #lauses = BooleanClauseList._construct_raw(operators.and_)

 #or col in mapper._pks_by_table[table]:
 #lauses.clauses.append(
 #ol == sql.bindparam(col.key, type_=col.type)
 #

 #f need_version_id:
 #lauses.clauses.append(
 #apper.version_id_col
 #= sql.bindparam(
 #apper.version_id_col.key, type_=mapper.version_id_col.type
 #
 #

 #eturn table.delete().where(clauses)

 #tatement = base_mapper._memo(("delete", table), delete_stmt)
 #or connection, recs in groupby(delete, lambda rec: rec[1]):  # connection
 #el_objects = [params for params, connection in recs]

 #xecution_options = {"compiled_cache": base_mapper._compiled_cache}
 #xpected = len(del_objects)
 #ows_matched = -1
 #nly_warn = False

 #f (
 #eed_version_id
 #nd not connection.dialect.supports_sane_multi_rowcount
 #:
 #f connection.dialect.supports_sane_rowcount:
 #ows_matched = 0
                # execute deletes individually so that versioned
                # rows can be verified
 #or params in del_objects:

 # = connection._execute_20(
 #tatement, params, execution_options=execution_options
 #
 #ows_matched += c.rowcount
 #lse:
 #til.warn(
 #Dialect %s does not support deleted rowcount "
 #- versioning cannot be verified."
 # connection.dialect.dialect_description
 #
 #onnection._execute_20(
 #tatement, del_objects, execution_options=execution_options
 #
 #lse:
 # = connection._execute_20(
 #tatement, del_objects, execution_options=execution_options
 #

 #f not need_version_id:
 #nly_warn = True

 #ows_matched = c.rowcount

 #f (
 #ase_mapper.confirm_deleted_rows
 #nd rows_matched > -1
 #nd expected != rows_matched
 #nd (
 #onnection.dialect.supports_sane_multi_rowcount
 #r len(del_objects) == 1
 #
 #:
            # TODO: why does this "only warn" if versioning is turned off,
            # whereas the UPDATE raises?
 #f only_warn:
 #til.warn(
 #DELETE statement on table '%s' expected to "
 #delete %d row(s); %d were matched.  Please set "
 #confirm_deleted_rows=False within the mapper "
 #configuration to prevent this warning."
 # (table.description, expected, rows_matched)
 #
 #lse:
 #aise orm_exc.StaleDataError(
 #DELETE statement on table '%s' expected to "
 #delete %d row(s); %d were matched.  Please set "
 #confirm_deleted_rows=False within the mapper "
 #configuration to prevent this warning."
 # (table.description, expected, rows_matched)
 #


def _finalize_insert_update_commands(base_mapper, uowtransaction, states):
 #""finalize state on states that have been inserted or updated,
 #ncluding calling after_insert/after_update events.

 #""
 #or state, state_dict, mapper, connection, has_identity in states:

 #f mapper._readonly_props:
 #eadonly = state.unmodified_intersection(
 #
 #.key
 #or p in mapper._readonly_props
 #f (
 #.expire_on_flush
 #nd (not p.deferred or p.key in state.dict)
 #
 #r (
 #ot p.expire_on_flush
 #nd not p.deferred
 #nd p.key not in state.dict
 #
 #
 #
 #f readonly:
 #tate._expire_attributes(state.dict, readonly)

        # if eager_defaults option is enabled, load
        # all expired cols.  Else if we have a version_id_col, make sure
        # it isn't expired.
 #oload_now = []

 #f base_mapper.eager_defaults:
 #oload_now.extend(
 #tate._unloaded_non_object.intersection(
 #apper._server_default_plus_onupdate_propkeys
 #
 #

 #f (
 #apper.version_id_col is not None
 #nd mapper.version_id_generator is False
 #:
 #f mapper._version_id_prop.key in state.unloaded:
 #oload_now.extend([mapper._version_id_prop.key])

 #f toload_now:
 #tate.key = base_mapper._identity_key_from_state(state)
 #tmt = future.select(mapper).set_label_style(
 #ABEL_STYLE_TABLENAME_PLUS_COL
 #
 #oading.load_on_ident(
 #owtransaction.session,
 #tmt,
 #tate.key,
 #efresh_state=state,
 #nly_load_props=toload_now,
 #

        # call after_XXX extensions
 #f not has_identity:
 #apper.dispatch.after_insert(mapper, connection, state)
 #lse:
 #apper.dispatch.after_update(mapper, connection, state)

 #f (
 #apper.version_id_generator is False
 #nd mapper.version_id_col is not None
 #:
 #f state_dict[mapper._version_id_prop.key] is None:
 #aise orm_exc.FlushError(
 #Instance does not contain a non-NULL version value"
 #


def _postfetch_post_update(
 #apper, uowtransaction, table, state, dict_, result, params
):
 #f uowtransaction.is_deleted(state):
 #eturn

 #refetch_cols = result.context.compiled.prefetch
 #ostfetch_cols = result.context.compiled.postfetch

 #f (
 #apper.version_id_col is not None
 #nd mapper.version_id_col in mapper._cols_by_table[table]
 #:
 #refetch_cols = list(prefetch_cols) + [mapper.version_id_col]

 #efresh_flush = bool(mapper.class_manager.dispatch.refresh_flush)
 #f refresh_flush:
 #oad_evt_attrs = []

 #or c in prefetch_cols:
 #f c.key in params and c in mapper._columntoproperty:
 #ict_[mapper._columntoproperty[c].key] = params[c.key]
 #f refresh_flush:
 #oad_evt_attrs.append(mapper._columntoproperty[c].key)

 #f refresh_flush and load_evt_attrs:
 #apper.class_manager.dispatch.refresh_flush(
 #tate, uowtransaction, load_evt_attrs
 #

 #f postfetch_cols:
 #tate._expire_attributes(
 #tate.dict,
 #
 #apper._columntoproperty[c].key
 #or c in postfetch_cols
 #f c in mapper._columntoproperty
 #,
 #


def _postfetch(
 #apper,
 #owtransaction,
 #able,
 #tate,
 #ict_,
 #esult,
 #arams,
 #alue_params,
 #supdate,
 #eturned_defaults,
):
 #""Expire attributes in need of newly persisted database state,
 #fter an INSERT or UPDATE statement has proceeded for that
 #tate."""

 #refetch_cols = result.context.compiled.prefetch
 #ostfetch_cols = result.context.compiled.postfetch
 #eturning_cols = result.context.compiled.returning

 #f (
 #apper.version_id_col is not None
 #nd mapper.version_id_col in mapper._cols_by_table[table]
 #:
 #refetch_cols = list(prefetch_cols) + [mapper.version_id_col]

 #efresh_flush = bool(mapper.class_manager.dispatch.refresh_flush)
 #f refresh_flush:
 #oad_evt_attrs = []

 #f returning_cols:
 #ow = returned_defaults
 #f row is not None:
 #or row_value, col in zip(row, returning_cols):
                # pk cols returned from insert are handled
                # distinctly, don't step on the values here
 #f col.primary_key and result.context.isinsert:
 #ontinue

                # note that columns can be in the "return defaults" that are
                # not mapped to this mapper, typically because they are
                # "excluded", which can be specified directly or also occurs
                # when using declarative w/ single table inheritance
 #rop = mapper._columntoproperty.get(col)
 #f prop:
 #ict_[prop.key] = row_value
 #f refresh_flush:
 #oad_evt_attrs.append(prop.key)

 #or c in prefetch_cols:
 #f c.key in params and c in mapper._columntoproperty:
 #ict_[mapper._columntoproperty[c].key] = params[c.key]
 #f refresh_flush:
 #oad_evt_attrs.append(mapper._columntoproperty[c].key)

 #f refresh_flush and load_evt_attrs:
 #apper.class_manager.dispatch.refresh_flush(
 #tate, uowtransaction, load_evt_attrs
 #

 #f isupdate and value_params:
        # explicitly suit the use case specified by
        # [ticket:3801], PK SQL expressions for UPDATE on non-RETURNING
        # database which are set to themselves in order to do a version bump.
 #ostfetch_cols.extend(
 #
 #ol
 #or col in value_params
 #f col.primary_key and col not in returning_cols
 #
 #

 #f postfetch_cols:
 #tate._expire_attributes(
 #tate.dict,
 #
 #apper._columntoproperty[c].key
 #or c in postfetch_cols
 #f c in mapper._columntoproperty
 #,
 #

    # synchronize newly inserted ids from one table to the next
    # TODO: this still goes a little too often.  would be nice to
    # have definitive list of "columns that changed" here
 #or m, equated_pairs in mapper._table_to_equated[table]:
 #ync.populate(
 #tate,
 #,
 #tate,
 #,
 #quated_pairs,
 #owtransaction,
 #apper.passive_updates,
 #


def _postfetch_bulk_save(mapper, dict_, table):
 #or m, equated_pairs in mapper._table_to_equated[table]:
 #ync.bulk_populate_inherit_keys(dict_, m, equated_pairs)


def _connections_for_states(base_mapper, uowtransaction, states):
 #""Return an iterator of (state, state.dict, mapper, connection).

 #he states are sorted according to _sort_states, then paired
 #ith the connection they should be using for the given
 #nit of work transaction.

 #""
    # if session has a connection callable,
    # organize individual states with the connection
    # to use for update
 #f uowtransaction.session.connection_callable:
 #onnection_callable = uowtransaction.session.connection_callable
 #lse:
 #onnection = uowtransaction.transaction.connection(base_mapper)
 #onnection_callable = None

 #or state in _sort_states(base_mapper, states):
 #f connection_callable:
 #onnection = connection_callable(base_mapper, state.obj())

 #apper = state.manager.mapper

 #ield state, state.dict, mapper, connection


def _sort_states(mapper, states):
 #ending = set(states)
 #ersistent = set(s for s in pending if s.key is not None)
 #ending.difference_update(persistent)

 #ry:
 #ersistent_sorted = sorted(
 #ersistent, key=mapper._persistent_sortkey_fn
 #
 #xcept TypeError as err:
 #til.raise_(
 #a_exc.InvalidRequestError(
 #Could not sort objects by primary key; primary key "
 #values must be sortable in Python (was: %s)" % err
 #,
 #eplace_context=err,
 #
 #eturn (
 #orted(pending, key=operator.attrgetter("insert_order"))
 # persistent_sorted
 #


_EMPTY_DICT = util.immutabledict()


class BulkUDCompileState(CompileState):
 #lass default_update_options(Options):
 #synchronize_session = "evaluate"
 #autoflush = True
 #subject_mapper = None
 #resolved_values = _EMPTY_DICT
 #resolved_keys_as_propnames = _EMPTY_DICT
 #value_evaluators = _EMPTY_DICT
 #matched_objects = None
 #matched_rows = None
 #refresh_identity_token = None

 #classmethod
 #ef orm_pre_session_exec(
 #ls,
 #ession,
 #tatement,
 #arams,
 #xecution_options,
 #ind_arguments,
 #s_reentrant_invoke,
 #:
 #f is_reentrant_invoke:
 #eturn statement, execution_options

 #
 #pdate_options,
 #xecution_options,
 # = BulkUDCompileState.default_update_options.from_execution_options(
 #_sa_orm_update_options",
 #"synchronize_session"},
 #xecution_options,
 #tatement._execution_options,
 #

 #ync = update_options._synchronize_session
 #f sync is not None:
 #f sync not in ("evaluate", "fetch", False):
 #aise sa_exc.ArgumentError(
 #Valid strategies for session synchronization "
 #are 'evaluate', 'fetch', False"
 #

 #ind_arguments["clause"] = statement
 #ry:
 #lugin_subject = statement._propagate_attrs["plugin_subject"]
 #xcept KeyError:
 #ssert False, "statement had 'orm' plugin but no plugin_subject"
 #lse:
 #ind_arguments["mapper"] = plugin_subject.mapper

 #pdate_options += {"_subject_mapper": plugin_subject.mapper}

 #f update_options._autoflush:
 #ession._autoflush()

 #tatement = statement._annotate(
 #"synchronize_session": update_options._synchronize_session}
 #

        # this stage of the execution is called before the do_orm_execute event
        # hook.  meaning for an extension like horizontal sharding, this step
        # happens before the extension splits out into multiple backends and
        # runs only once.  if we do pre_sync_fetch, we execute a SELECT
        # statement, which the horizontal sharding extension splits amongst the
        # shards and combines the results together.

 #f update_options._synchronize_session == "evaluate":
 #pdate_options = cls._do_pre_synchronize_evaluate(
 #ession,
 #tatement,
 #arams,
 #xecution_options,
 #ind_arguments,
 #pdate_options,
 #
 #lif update_options._synchronize_session == "fetch":
 #pdate_options = cls._do_pre_synchronize_fetch(
 #ession,
 #tatement,
 #arams,
 #xecution_options,
 #ind_arguments,
 #pdate_options,
 #

 #eturn (
 #tatement,
 #til.immutabledict(execution_options).union(
 #ict(_sa_orm_update_options=update_options)
 #,
 #

 #classmethod
 #ef orm_setup_cursor_result(
 #ls,
 #ession,
 #tatement,
 #arams,
 #xecution_options,
 #ind_arguments,
 #esult,
 #:

        # this stage of the execution is called after the
        # do_orm_execute event hook.  meaning for an extension like
        # horizontal sharding, this step happens *within* the horizontal
        # sharding event handler which calls session.execute() re-entrantly
        # and will occur for each backend individually.
        # the sharding extension then returns its own merged result from the
        # individual ones we return here.

 #pdate_options = execution_options["_sa_orm_update_options"]
 #f update_options._synchronize_session == "evaluate":
 #ls._do_post_synchronize_evaluate(session, result, update_options)
 #lif update_options._synchronize_session == "fetch":
 #ls._do_post_synchronize_fetch(session, result, update_options)

 #eturn result

 #classmethod
 #ef _adjust_for_extra_criteria(cls, global_attributes, ext_info):
 #""Apply extra criteria filtering.

 #or all distinct single-table-inheritance mappers represented in the
 #able being updated or deleted, produce additional WHERE criteria such
 #hat only the appropriate subtypes are selected from the total results.

 #dditionally, add WHERE criteria originating from LoaderCriteriaOptions
 #ollected from the statement.

 #""

 #eturn_crit = ()

 #dapter = ext_info._adapter if ext_info.is_aliased_class else None

 #f (
 #additional_entity_criteria",
 #xt_info.mapper,
 # in global_attributes:
 #eturn_crit += tuple(
 #e._resolve_where_criteria(ext_info)
 #or ae in global_attributes[
 #"additional_entity_criteria", ext_info.mapper)
 #
 #f ae.include_aliases or ae.entity is ext_info
 #

 #f ext_info.mapper._single_table_criterion is not None:
 #eturn_crit += (ext_info.mapper._single_table_criterion,)

 #f adapter:
 #eturn_crit = tuple(adapter.traverse(crit) for crit in return_crit)

 #eturn return_crit

 #classmethod
 #ef _do_pre_synchronize_evaluate(
 #ls,
 #ession,
 #tatement,
 #arams,
 #xecution_options,
 #ind_arguments,
 #pdate_options,
 #:
 #apper = update_options._subject_mapper
 #arget_cls = mapper.class_

 #alue_evaluators = resolved_keys_as_propnames = _EMPTY_DICT

 #ry:
 #valuator_compiler = evaluator.EvaluatorCompiler(target_cls)
 #rit = ()
 #f statement._where_criteria:
 #rit += statement._where_criteria

 #lobal_attributes = {}
 #or opt in statement._with_options:
 #f opt._is_criteria_option:
 #pt.get_global_criteria(global_attributes)

 #f global_attributes:
 #rit += cls._adjust_for_extra_criteria(
 #lobal_attributes, mapper
 #

 #f crit:
 #val_condition = evaluator_compiler.process(*crit)
 #lse:

 #ef eval_condition(obj):
 #eturn True

 #xcept evaluator.UnevaluatableError as err:
 #til.raise_(
 #a_exc.InvalidRequestError(
 #Could not evaluate current criteria in Python: "%s". '
 #Specify 'fetch' or False for the "
 #synchronize_session execution option." % err
 #,
 #rom_=err,
 #

 #f statement.__visit_name__ == "lambda_element":
            # ._resolved is called on every LambdaElement in order to
            # generate the cache key, so this access does not add
            # additional expense
 #ffective_statement = statement._resolved
 #lse:
 #ffective_statement = statement

 #f effective_statement.__visit_name__ == "update":
 #esolved_values = cls._get_resolved_values(
 #apper, effective_statement
 #
 #alue_evaluators = {}
 #esolved_keys_as_propnames = cls._resolved_keys_as_propnames(
 #apper, resolved_values
 #
 #or key, value in resolved_keys_as_propnames:
 #ry:
 #evaluator = evaluator_compiler.process(
 #oercions.expect(roles.ExpressionElementRole, value)
 #
 #xcept evaluator.UnevaluatableError:
 #ass
 #lse:
 #alue_evaluators[key] = _evaluator

        # TODO: detect when the where clause is a trivial primary key match.
 #atched_objects = [
 #tate.obj()
 #or state in session.identity_map.all_states()
 #f state.mapper.isa(mapper)
 #nd not state.expired
 #nd eval_condition(state.obj())
 #nd (
 #pdate_options._refresh_identity_token is None
                # TODO: coverage for the case where horizontal sharding
                # invokes an update() or delete() given an explicit identity
                # token up front
 #r state.identity_token
 #= update_options._refresh_identity_token
 #
 #
 #eturn update_options + {
 #_matched_objects": matched_objects,
 #_value_evaluators": value_evaluators,
 #_resolved_keys_as_propnames": resolved_keys_as_propnames,
 #

 #classmethod
 #ef _get_resolved_values(cls, mapper, statement):
 #f statement._multi_values:
 #eturn []
 #lif statement._ordered_values:
 #terator = statement._ordered_values
 #lif statement._values:
 #terator = statement._values.items()
 #lse:
 #eturn []

 #alues = []
 #f iterator:
 #or k, v in iterator:
 #f mapper:
 #f isinstance(k, util.string_types):
 #esc = _entity_namespace_key(mapper, k)
 #alues.extend(desc._bulk_update_tuples(v))
 #lif "entity_namespace" in k._annotations:
 #_anno = k._annotations
 #ttr = _entity_namespace_key(
 #_anno["entity_namespace"], k_anno["proxy_key"]
 #
 #alues.extend(attr._bulk_update_tuples(v))
 #lse:
 #alues.append((k, v))
 #lse:
 #alues.append((k, v))
 #eturn values

 #classmethod
 #ef _resolved_keys_as_propnames(cls, mapper, resolved_values):
 #alues = []
 #or k, v in resolved_values:
 #f isinstance(k, attributes.QueryableAttribute):
 #alues.append((k.key, v))
 #ontinue
 #lif hasattr(k, "__clause_element__"):
 # = k.__clause_element__()

 #f mapper and isinstance(k, expression.ColumnElement):
 #ry:
 #ttr = mapper._columntoproperty[k]
 #xcept orm_exc.UnmappedColumnError:
 #ass
 #lse:
 #alues.append((attr.key, v))
 #lse:
 #aise sa_exc.InvalidRequestError(
 #Invalid expression type: %r" % k
 #
 #eturn values

 #classmethod
 #ef _do_pre_synchronize_fetch(
 #ls,
 #ession,
 #tatement,
 #arams,
 #xecution_options,
 #ind_arguments,
 #pdate_options,
 #:
 #apper = update_options._subject_mapper

 #elect_stmt = (
 #elect(*(mapper.primary_key + (mapper.select_identity_token,)))
 #select_from(mapper)
 #options(*statement._with_options)
 #
 #elect_stmt._where_criteria = statement._where_criteria

 #ef skip_for_full_returning(orm_context):
 #ind = orm_context.session.get_bind(**orm_context.bind_arguments)
 #f bind.dialect.full_returning:
 #eturn _result.null_result()
 #lse:
 #eturn None

 #esult = session.execute(
 #elect_stmt,
 #arams,
 #xecution_options,
 #ind_arguments,
 #add_event=skip_for_full_returning,
 #
 #atched_rows = result.fetchall()

 #alue_evaluators = _EMPTY_DICT

 #f statement.__visit_name__ == "lambda_element":
            # ._resolved is called on every LambdaElement in order to
            # generate the cache key, so this access does not add
            # additional expense
 #ffective_statement = statement._resolved
 #lse:
 #ffective_statement = statement

 #f effective_statement.__visit_name__ == "update":
 #arget_cls = mapper.class_
 #valuator_compiler = evaluator.EvaluatorCompiler(target_cls)
 #esolved_values = cls._get_resolved_values(
 #apper, effective_statement
 #
 #esolved_keys_as_propnames = cls._resolved_keys_as_propnames(
 #apper, resolved_values
 #

 #esolved_keys_as_propnames = cls._resolved_keys_as_propnames(
 #apper, resolved_values
 #
 #alue_evaluators = {}
 #or key, value in resolved_keys_as_propnames:
 #ry:
 #evaluator = evaluator_compiler.process(
 #oercions.expect(roles.ExpressionElementRole, value)
 #
 #xcept evaluator.UnevaluatableError:
 #ass
 #lse:
 #alue_evaluators[key] = _evaluator

 #lse:
 #esolved_keys_as_propnames = _EMPTY_DICT

 #eturn update_options + {
 #_value_evaluators": value_evaluators,
 #_matched_rows": matched_rows,
 #_resolved_keys_as_propnames": resolved_keys_as_propnames,
 #


@CompileState.plugin_for("orm", "update")
class BulkORMUpdate(UpdateDMLState, BulkUDCompileState):
 #classmethod
 #ef create_for_statement(cls, statement, compiler, **kw):

 #elf = cls.__new__(cls)

 #xt_info = statement.table._annotations["parententity"]

 #elf.mapper = mapper = ext_info.mapper

 #elf.extra_criteria_entities = {}

 #elf._resolved_values = cls._get_resolved_values(mapper, statement)

 #xtra_criteria_attributes = {}

 #or opt in statement._with_options:
 #f opt._is_criteria_option:
 #pt.get_global_criteria(extra_criteria_attributes)

 #f not statement._preserve_parameter_order and statement._values:
 #elf._resolved_values = dict(self._resolved_values)

 #ew_stmt = sql.Update.__new__(sql.Update)
 #ew_stmt.__dict__.update(statement.__dict__)
 #ew_stmt.table = mapper.local_table

        # note if the statement has _multi_values, these
        # are passed through to the new statement, which will then raise
        # InvalidRequestError because UPDATE doesn't support multi_values
        # right now.
 #f statement._ordered_values:
 #ew_stmt._ordered_values = self._resolved_values
 #lif statement._values:
 #ew_stmt._values = self._resolved_values

 #ew_crit = cls._adjust_for_extra_criteria(
 #xtra_criteria_attributes, mapper
 #
 #f new_crit:
 #ew_stmt = new_stmt.where(*new_crit)

        # if we are against a lambda statement we might not be the
        # topmost object that received per-execute annotations

 #f (
 #ompiler._annotations.get("synchronize_session", None) == "fetch"
 #nd compiler.dialect.full_returning
 #:
 #f new_stmt._returning:
 #aise sa_exc.InvalidRequestError(
 #Can't use synchronize_session='fetch' "
 #with explicit returning()"
 #
 #ew_stmt = new_stmt.returning(*mapper.primary_key)

 #pdateDMLState.__init__(self, new_stmt, compiler, **kw)

 #eturn self

 #classmethod
 #ef _do_post_synchronize_evaluate(cls, session, result, update_options):

 #tates = set()
 #valuated_keys = list(update_options._value_evaluators.keys())
 #alues = update_options._resolved_keys_as_propnames
 #ttrib = set(k for k, v in values)
 #or obj in update_options._matched_objects:

 #tate, dict_ = (
 #ttributes.instance_state(obj),
 #ttributes.instance_dict(obj),
 #

            # the evaluated states were gathered across all identity tokens.
            # however the post_sync events are called per identity token,
            # so filter.
 #f (
 #pdate_options._refresh_identity_token is not None
 #nd state.identity_token
 #= update_options._refresh_identity_token
 #:
 #ontinue

            # only evaluate unmodified attributes
 #o_evaluate = state.unmodified.intersection(evaluated_keys)
 #or key in to_evaluate:
 #f key in dict_:
 #ict_[key] = update_options._value_evaluators[key](obj)

 #tate.manager.dispatch.refresh(state, None, to_evaluate)

 #tate._commit(dict_, list(to_evaluate))

 #o_expire = attrib.intersection(dict_).difference(to_evaluate)
 #f to_expire:
 #tate._expire_attributes(dict_, to_expire)

 #tates.add(state)
 #ession._register_altered(states)

 #classmethod
 #ef _do_post_synchronize_fetch(cls, session, result, update_options):
 #arget_mapper = update_options._subject_mapper

 #tates = set()
 #valuated_keys = list(update_options._value_evaluators.keys())

 #f result.returns_rows:
 #atched_rows = [
 #uple(row) + (update_options._refresh_identity_token,)
 #or row in result.all()
 #
 #lse:
 #atched_rows = update_options._matched_rows

 #bjs = [
 #ession.identity_map[identity_key]
 #or identity_key in [
 #arget_mapper.identity_key_from_primary_key(
 #ist(primary_key),
 #dentity_token=identity_token,
 #
 #or primary_key, identity_token in [
 #row[0:-1], row[-1]) for row in matched_rows
 #
 #f update_options._refresh_identity_token is None
 #r identity_token == update_options._refresh_identity_token
 #
 #f identity_key in session.identity_map
 #

 #alues = update_options._resolved_keys_as_propnames
 #ttrib = set(k for k, v in values)

 #or obj in objs:
 #tate, dict_ = (
 #ttributes.instance_state(obj),
 #ttributes.instance_dict(obj),
 #

 #o_evaluate = state.unmodified.intersection(evaluated_keys)
 #or key in to_evaluate:
 #f key in dict_:
 #ict_[key] = update_options._value_evaluators[key](obj)
 #tate.manager.dispatch.refresh(state, None, to_evaluate)

 #tate._commit(dict_, list(to_evaluate))

 #o_expire = attrib.intersection(dict_).difference(to_evaluate)
 #f to_expire:
 #tate._expire_attributes(dict_, to_expire)

 #tates.add(state)
 #ession._register_altered(states)


@CompileState.plugin_for("orm", "delete")
class BulkORMDelete(DeleteDMLState, BulkUDCompileState):
 #classmethod
 #ef create_for_statement(cls, statement, compiler, **kw):
 #elf = cls.__new__(cls)

 #xt_info = statement.table._annotations["parententity"]
 #elf.mapper = mapper = ext_info.mapper

 #elf.extra_criteria_entities = {}

 #xtra_criteria_attributes = {}

 #or opt in statement._with_options:
 #f opt._is_criteria_option:
 #pt.get_global_criteria(extra_criteria_attributes)

 #ew_crit = cls._adjust_for_extra_criteria(
 #xtra_criteria_attributes, mapper
 #
 #f new_crit:
 #tatement = statement.where(*new_crit)

 #f (
 #apper
 #nd compiler._annotations.get("synchronize_session", None)
 #= "fetch"
 #nd compiler.dialect.full_returning
 #:
 #tatement = statement.returning(*mapper.primary_key)

 #eleteDMLState.__init__(self, statement, compiler, **kw)

 #eturn self

 #classmethod
 #ef _do_post_synchronize_evaluate(cls, session, result, update_options):

 #ession._remove_newly_deleted(
 #
 #ttributes.instance_state(obj)
 #or obj in update_options._matched_objects
 #
 #

 #classmethod
 #ef _do_post_synchronize_fetch(cls, session, result, update_options):
 #arget_mapper = update_options._subject_mapper

 #f result.returns_rows:
 #atched_rows = [
 #uple(row) + (update_options._refresh_identity_token,)
 #or row in result.all()
 #
 #lse:
 #atched_rows = update_options._matched_rows

 #or row in matched_rows:
 #rimary_key = row[0:-1]
 #dentity_token = row[-1]

            # TODO: inline this and call remove_newly_deleted
            # once
 #dentity_key = target_mapper.identity_key_from_primary_key(
 #ist(primary_key),
 #dentity_token=identity_token,
 #
 #f identity_key in session.identity_map:
 #ession._remove_newly_deleted(
 #
 #ttributes.instance_state(
 #ession.identity_map[identity_key]
 #
 #
 #
