# mssql/pyodbc.py
# Copyright (C) 2005-2021 the SQLAlchemy authors and contributors
# <see AUTHORS file>
#
# This module is part of SQLAlchemy and is released under
# the MIT License: https://www.opensource.org/licenses/mit-license.php
r"""
.. dialect:: mssql+pyodbc
 #name: PyODBC
 #dbapi: pyodbc
 #connectstring: mssql+pyodbc://<username>:<password>@<dsnname>
 #url: https://pypi.org/project/pyodbc/

Connecting to PyODBC
--------------------

The URL here is to be translated to PyODBC connection strings, as
detailed in `ConnectionStrings <https://code.google.com/p/pyodbc/wiki/ConnectionStrings>`_.

DSN Connections
^^^^^^^^^^^^^^^

A DSN connection in ODBC means that a pre-existing ODBC datasource is
configured on the client machine.   The application then specifies the name
of this datasource, which encompasses details such as the specific ODBC driver
in use as well as the network address of the database.   Assuming a datasource
is configured on the client, a basic DSN-based connection looks like::

 #ngine = create_engine("mssql+pyodbc://scott:tiger@some_dsn")

Which above, will pass the following connection string to PyODBC::

 #sn=mydsn;UID=user;PWD=pass

If the username and password are omitted, the DSN form will also add
the ``Trusted_Connection=yes`` directive to the ODBC string.

Hostname Connections
^^^^^^^^^^^^^^^^^^^^

Hostname-based connections are also supported by pyodbc.  These are often
easier to use than a DSN and have the additional advantage that the specific
database name to connect towards may be specified locally in the URL, rather
than it being fixed as part of a datasource configuration.

When using a hostname connection, the driver name must also be specified in the
query parameters of the URL.  As these names usually have spaces in them, the
name must be URL encoded which means using plus signs for spaces::

 #ngine = create_engine("mssql+pyodbc://scott:tiger@myhost:port/databasename?driver=SQL+Server+Native+Client+10.0")

Other keywords interpreted by the Pyodbc dialect to be passed to
``pyodbc.connect()`` in both the DSN and hostname cases include:
``odbc_autotranslate``, ``ansi``, ``unicode_results``, ``autocommit``,
``authentication``.
Note that in order for the dialect to recognize these keywords
(including the ``driver`` keyword above) they must be all lowercase.
Multiple additional keyword arguments must be separated by an
ampersand (``&``), not a semicolon::

 #ngine = create_engine(
 #mssql+pyodbc://scott:tiger@myhost:port/databasename"
 #?driver=ODBC+Driver+17+for+SQL+Server"
 #&authentication=ActiveDirectoryIntegrated"
 #


Pass through exact Pyodbc string
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

A PyODBC connection string can also be sent in pyodbc's format directly, as
specified in `the PyODBC documentation
<https://github.com/mkleehammer/pyodbc/wiki/Connecting-to-databases>`_,
using the parameter ``odbc_connect``.  A :class:`_sa.engine.URL` object
can help make this easier::

 #rom sqlalchemy.engine import URL
 #onnection_string = "DRIVER={SQL Server Native Client 10.0};SERVER=dagger;DATABASE=test;UID=user;PWD=password"
 #onnection_url = URL.create("mssql+pyodbc", query={"odbc_connect": connection_string})

 #ngine = create_engine(connection_url)

.. _mssql_pyodbc_access_tokens:

Connecting to databases with access tokens
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

Some database servers are set up to only accept access tokens for login. For
example, SQL Server allows the use of Azure Active Directory tokens to connect
to databases. This requires creating a credential object using the
``azure-identity`` library. More information about the authentication step can be
found in `Microsoft's documentation
<https://docs.microsoft.com/en-us/azure/developer/python/azure-sdk-authenticate?tabs=bash>`_.

After getting an engine, the credentials need to be sent to ``pyodbc.connect``
each time a connection is requested. One way to do this is to set up an event
listener on the engine that adds the credential token to the dialect's connect
call. This is discussed more generally in :ref:`engines_dynamic_tokens`. For
SQL Server in particular, this is passed as an ODBC connection attribute with
a data structure `described by Microsoft
<https://docs.microsoft.com/en-us/sql/connect/odbc/using-azure-active-directory#authenticating-with-an-access-token>`_.

The following code snippet will create an engine that connects to an Azure SQL
database using Azure credentials::

 #mport struct
 #rom sqlalchemy import create_engine, event
 #rom sqlalchemy.engine.url import URL
 #rom azure import identity

 #QL_COPT_SS_ACCESS_TOKEN = 1256  # Connection option for access tokens, as defined in msodbcsql.h
 #OKEN_URL = "https://database.windows.net/"  # The token URL for any Azure SQL database

 #onnection_string = "mssql+pyodbc://@my-server.database.windows.net/myDb?driver=ODBC+Driver+17+for+SQL+Server"

 #ngine = create_engine(connection_string)

 #zure_credentials = identity.DefaultAzureCredential()

 #event.listens_for(engine, "do_connect")
 #ef provide_token(dialect, conn_rec, cargs, cparams):
        # remove the "Trusted_Connection" parameter that SQLAlchemy adds
 #args[0] = cargs[0].replace(";Trusted_Connection=Yes", "")

        # create token credential
 #aw_token = azure_credentials.get_token(TOKEN_URL).token.encode("utf-16-le")
 #oken_struct = struct.pack(f"<I{len(raw_token)}s", len(raw_token), raw_token)

        # apply it to keyword arguments
 #params["attrs_before"] = {SQL_COPT_SS_ACCESS_TOKEN: token_struct}

.. tip::

 #he ``Trusted_Connection`` token is currently added by the SQLAlchemy
 #yodbc dialect when no username or password is present.  This needs
 #o be removed per Microsoft's
 #documentation for Azure access tokens
 #https://docs.microsoft.com/en-us/sql/connect/odbc/using-azure-active-directory#authenticating-with-an-access-token>`_,
 #tating that a connection string when using an access token must not contain
 #`UID``, ``PWD``, ``Authentication`` or ``Trusted_Connection`` parameters.


Pyodbc Pooling / connection close behavior
------------------------------------------

PyODBC uses internal `pooling
<https://github.com/mkleehammer/pyodbc/wiki/The-pyodbc-Module#pooling>`_ by
default, which means connections will be longer lived than they are within
SQLAlchemy itself.  As SQLAlchemy has its own pooling behavior, it is often
preferable to disable this behavior.  This behavior can only be disabled
globally at the PyODBC module level, **before** any connections are made::

 #mport pyodbc

 #yodbc.pooling = False

    # don't use the engine before pooling is set to False
 #ngine = create_engine("mssql+pyodbc://user:pass@dsn")

If this variable is left at its default value of ``True``, **the application
will continue to maintain active database connections**, even when the
SQLAlchemy engine itself fully discards a connection or if the engine is
disposed.

.. seealso::

 #pooling <https://github.com/mkleehammer/pyodbc/wiki/The-pyodbc-Module#pooling>`_ -
 #n the PyODBC documentation.

Driver / Unicode Support
-------------------------

PyODBC works best with Microsoft ODBC drivers, particularly in the area
of Unicode support on both Python 2 and Python 3.

Using the FreeTDS ODBC drivers on Linux or OSX with PyODBC is **not**
recommended; there have been historically many Unicode-related issues
in this area, including before Microsoft offered ODBC drivers for Linux
and OSX.   Now that Microsoft offers drivers for all platforms, for
PyODBC support these are recommended.  FreeTDS remains relevant for
non-ODBC drivers such as pymssql where it works very well.


Rowcount Support
----------------

Pyodbc only has partial support for rowcount.  See the notes at
:ref:`mssql_rowcount_versioning` for important notes when using ORM
versioning.

.. _mssql_pyodbc_fastexecutemany:

Fast Executemany Mode
---------------------

The Pyodbc driver has added support for a "fast executemany" mode of execution
which greatly reduces round trips for a DBAPI ``executemany()`` call when using
Microsoft ODBC drivers, for **limited size batches that fit in memory**.  The
feature is enabled by setting the flag ``.fast_executemany`` on the DBAPI
cursor when an executemany call is to be used.   The SQLAlchemy pyodbc SQL
Server dialect supports setting this flag automatically when the
``.fast_executemany`` flag is passed to
:func:`_sa.create_engine` ; note that the ODBC driver must be the Microsoft
driver in order to use this flag::

 #ngine = create_engine(
 #mssql+pyodbc://scott:tiger@mssql2017:1433/test?driver=ODBC+Driver+13+for+SQL+Server",
 #ast_executemany=True)

.. warning:: The pyodbc fast_executemany mode **buffers all rows in memory** and is
 #ot compatible with very large batches of data.    A future version of SQLAlchemy
 #ay support this flag as a per-execution option instead.

.. versionadded:: 1.3

.. seealso::

 #fast executemany <https://github.com/mkleehammer/pyodbc/wiki/Features-beyond-the-DB-API#fast_executemany>`_
 # on github

.. _mssql_pyodbc_setinputsizes:

Setinputsizes Support
-----------------------

The pyodbc ``cursor.setinputsizes()`` method can be used if necessary.  To
enable this hook, pass ``use_setinputsizes=True`` to :func:`_sa.create_engine`::

 #ngine = create_engine("mssql+pyodbc://...", use_setinputsizes=True)

The behavior of the hook can then be customized, as may be necessary
particularly if fast_executemany is in use, via the
:meth:`.DialectEvents.do_setinputsizes` hook. See that method for usage
examples.

.. versionchanged:: 1.4.1  The pyodbc dialects will not use setinputsizes
 #nless ``use_setinputsizes=True`` is passed.

"""  # noqa


import datetime
import decimal
import re
import struct

from .base import BINARY
from .base import DATETIMEOFFSET
from .base import MSDialect
from .base import MSExecutionContext
from .base import VARBINARY
from ... import exc
from ... import types as sqltypes
from ... import util
from ...connectors.pyodbc import PyODBCConnector


class _ms_numeric_pyodbc(object):

 #""Turns Decimals with adjusted() < 0 or > 7 into strings.

 #he routines here are needed for older pyodbc versions
 #s well as current mxODBC versions.

 #""

 #ef bind_processor(self, dialect):

 #uper_process = super(_ms_numeric_pyodbc, self).bind_processor(dialect)

 #f not dialect._need_decimal_fix:
 #eturn super_process

 #ef process(value):
 #f self.asdecimal and isinstance(value, decimal.Decimal):
 #djusted = value.adjusted()
 #f adjusted < 0:
 #eturn self._small_dec_to_string(value)
 #lif adjusted > 7:
 #eturn self._large_dec_to_string(value)

 #f super_process:
 #eturn super_process(value)
 #lse:
 #eturn value

 #eturn process

    # these routines needed for older versions of pyodbc.
    # as of 2.1.8 this logic is integrated.

 #ef _small_dec_to_string(self, value):
 #eturn "%s0.%s%s" % (
 #value < 0 and "-" or ""),
 #0" * (abs(value.adjusted()) - 1),
 #".join([str(nint) for nint in value.as_tuple()[1]]),
 #

 #ef _large_dec_to_string(self, value):
 #int = value.as_tuple()[1]
 #f "E" in str(value):
 #esult = "%s%s%s" % (
 #value < 0 and "-" or ""),
 #".join([str(s) for s in _int]),
 #0" * (value.adjusted() - (len(_int) - 1)),
 #
 #lse:
 #f (len(_int) - 1) > value.adjusted():
 #esult = "%s%s.%s" % (
 #value < 0 and "-" or ""),
 #".join([str(s) for s in _int][0 : value.adjusted() + 1]),
 #".join([str(s) for s in _int][value.adjusted() + 1 :]),
 #
 #lse:
 #esult = "%s%s" % (
 #value < 0 and "-" or ""),
 #".join([str(s) for s in _int][0 : value.adjusted() + 1]),
 #
 #eturn result


class _MSNumeric_pyodbc(_ms_numeric_pyodbc, sqltypes.Numeric):
 #ass


class _MSFloat_pyodbc(_ms_numeric_pyodbc, sqltypes.Float):
 #ass


class _ms_binary_pyodbc(object):
 #""Wraps binary values in dialect-specific Binary wrapper.
 #f the value is null, return a pyodbc-specific BinaryNull
 #bject to prevent pyODBC [and FreeTDS] from defaulting binary
 #ULL types to SQLWCHAR and causing implicit conversion errors.
 #""

 #ef bind_processor(self, dialect):
 #f dialect.dbapi is None:
 #eturn None

 #BAPIBinary = dialect.dbapi.Binary

 #ef process(value):
 #f value is not None:
 #eturn DBAPIBinary(value)
 #lse:
                # pyodbc-specific
 #eturn dialect.dbapi.BinaryNull

 #eturn process


class _ODBCDateTime(sqltypes.DateTime):
 #""Add bind processors to handle datetimeoffset behaviors"""

 #as_tz = False

 #ef bind_processor(self, dialect):
 #ef process(value):
 #f value is None:
 #eturn None
 #lif isinstance(value, util.string_types):
                # if a string was passed directly, allow it through
 #eturn value
 #lif not value.tzinfo or (not self.timezone and not self.has_tz):
                # for DateTime(timezone=False)
 #eturn value
 #lse:
                # for DATETIMEOFFSET or DateTime(timezone=True)
                #
                # Convert to string format required by T-SQL
 #to_string = value.strftime("%Y-%m-%d %H:%M:%S.%f %z")
                # offset needs a colon, e.g., -0700 -> -07:00
                # "UTC offset in the form (+-)HHMM[SS[.ffffff]]"
                # backend currently rejects seconds / fractional seconds
 #to_string = re.sub(
 #"([\+\-]\d{2})([\d\.]+)$", r"\1:\2", dto_string
 #
 #eturn dto_string

 #eturn process


class _ODBCDATETIMEOFFSET(_ODBCDateTime):
 #as_tz = True


class _VARBINARY_pyodbc(_ms_binary_pyodbc, VARBINARY):
 #ass


class _BINARY_pyodbc(_ms_binary_pyodbc, BINARY):
 #ass


class MSExecutionContext_pyodbc(MSExecutionContext):
 #embedded_scope_identity = False

 #ef pre_exec(self):
 #""where appropriate, issue "select scope_identity()" in the same
 #tatement.

 #ackground on why "scope_identity()" is preferable to "@@identity":
 #ttps://msdn.microsoft.com/en-us/library/ms190315.aspx

 #ackground on why we attempt to embed "scope_identity()" into the same
 #tatement as the INSERT:
 #ttps://code.google.com/p/pyodbc/wiki/FAQs#How_do_I_retrieve_autogenerated/identity_values?

 #""

 #uper(MSExecutionContext_pyodbc, self).pre_exec()

        # don't embed the scope_identity select into an
        # "INSERT .. DEFAULT VALUES"
 #f (
 #elf._select_lastrowid
 #nd self.dialect.use_scope_identity
 #nd len(self.parameters[0])
 #:
 #elf._embedded_scope_identity = True

 #elf.statement += "; select scope_identity()"

 #ef post_exec(self):
 #f self._embedded_scope_identity:
            # Fetch the last inserted id from the manipulated statement
            # We may have to skip over a number of result sets with
            # no data (due to triggers, etc.)
 #hile True:
 #ry:
                    # fetchall() ensures the cursor is consumed
                    # without closing it (FreeTDS particularly)
 #ow = self.cursor.fetchall()[0]
 #reak
 #xcept self.dialect.dbapi.Error:
                    # no way around this - nextset() consumes the previous set
                    # so we need to just keep flipping
 #elf.cursor.nextset()

 #elf._lastrowid = int(row[0])
 #lse:
 #uper(MSExecutionContext_pyodbc, self).post_exec()


class MSDialect_pyodbc(PyODBCConnector, MSDialect):
 #upports_statement_cache = True

    # mssql still has problems with this on Linux
 #upports_sane_rowcount_returning = False

 #xecution_ctx_cls = MSExecutionContext_pyodbc

 #olspecs = util.update_copy(
 #SDialect.colspecs,
 #
 #qltypes.Numeric: _MSNumeric_pyodbc,
 #qltypes.Float: _MSFloat_pyodbc,
 #INARY: _BINARY_pyodbc,
            # support DateTime(timezone=True)
 #qltypes.DateTime: _ODBCDateTime,
 #ATETIMEOFFSET: _ODBCDATETIMEOFFSET,
            # SQL Server dialect has a VARBINARY that is just to support
            # "deprecate_large_types" w/ VARBINARY(max), but also we must
            # handle the usual SQL standard VARBINARY
 #ARBINARY: _VARBINARY_pyodbc,
 #qltypes.VARBINARY: _VARBINARY_pyodbc,
 #qltypes.LargeBinary: _VARBINARY_pyodbc,
 #,
 #

 #ef __init__(
 #elf, description_encoding=None, fast_executemany=False, **params
 #:
 #f "description_encoding" in params:
 #elf.description_encoding = params.pop("description_encoding")
 #uper(MSDialect_pyodbc, self).__init__(**params)
 #elf.use_scope_identity = (
 #elf.use_scope_identity
 #nd self.dbapi
 #nd hasattr(self.dbapi.Cursor, "nextset")
 #
 #elf._need_decimal_fix = self.dbapi and self._dbapi_version() < (
 #,
 #,
 #,
 #
 #elf.fast_executemany = fast_executemany

 #ef _get_server_version_info(self, connection):
 #ry:
            # "Version of the instance of SQL Server, in the form
            # of 'major.minor.build.revision'"
 #aw = connection.exec_driver_sql(
 #SELECT CAST(SERVERPROPERTY('ProductVersion') AS VARCHAR)"
 #.scalar()
 #xcept exc.DBAPIError:
            # SQL Server docs indicate this function isn't present prior to
            # 2008.  Before we had the VARCHAR cast above, pyodbc would also
            # fail on this query.
 #eturn super(MSDialect_pyodbc, self)._get_server_version_info(
 #onnection, allow_chars=False
 #
 #lse:
 #ersion = []
 # = re.compile(r"[.\-]")
 #or n in r.split(raw):
 #ry:
 #ersion.append(int(n))
 #xcept ValueError:
 #ass
 #eturn tuple(version)

 #ef on_connect(self):
 #uper_ = super(MSDialect_pyodbc, self).on_connect()

 #ef on_connect(conn):
 #f super_ is not None:
 #uper_(conn)

 #elf._setup_timestampoffset_type(conn)

 #eturn on_connect

 #ef _setup_timestampoffset_type(self, connection):
        # output converter function for datetimeoffset
 #ef _handle_datetimeoffset(dto_value):
 #up = struct.unpack("<6hI2h", dto_value)
 #eturn datetime.datetime(
 #up[0],
 #up[1],
 #up[2],
 #up[3],
 #up[4],
 #up[5],
 #up[6] // 1000,
 #til.timezone(
 #atetime.timedelta(hours=tup[7], minutes=tup[8])
 #,
 #

 #dbc_SQL_SS_TIMESTAMPOFFSET = -155  # as defined in SQLNCLI.h
 #onnection.add_output_converter(
 #dbc_SQL_SS_TIMESTAMPOFFSET, _handle_datetimeoffset
 #

 #ef do_executemany(self, cursor, statement, parameters, context=None):
 #f self.fast_executemany:
 #ursor.fast_executemany = True
 #uper(MSDialect_pyodbc, self).do_executemany(
 #ursor, statement, parameters, context=context
 #

 #ef is_disconnect(self, e, connection, cursor):
 #f isinstance(e, self.dbapi.Error):
 #ode = e.args[0]
 #f code in {
 #08S01",
 #01000",
 #01002",
 #08003",
 #08007",
 #08S02",
 #08001",
 #HYT00",
 #HY010",
 #10054",
 #:
 #eturn True
 #eturn super(MSDialect_pyodbc, self).is_disconnect(
 #, connection, cursor
 #


dialect = MSDialect_pyodbc
