# postgresql/asyncpg.py
# Copyright (C) 2005-2021 the SQLAlchemy authors and contributors <see AUTHORS
# file>
#
# This module is part of SQLAlchemy and is released under
# the MIT License: https://www.opensource.org/licenses/mit-license.php
r"""
.. dialect:: postgresql+asyncpg
 #name: asyncpg
 #dbapi: asyncpg
 #connectstring: postgresql+asyncpg://user:password@host:port/dbname[?key=value&key=value...]
 #url: https://magicstack.github.io/asyncpg/

The asyncpg dialect is SQLAlchemy's first Python asyncio dialect.

Using a special asyncio mediation layer, the asyncpg dialect is usable
as the backend for the :ref:`SQLAlchemy asyncio <asyncio_toplevel>`
extension package.

This dialect should normally be used only with the
:func:`_asyncio.create_async_engine` engine creation function::

 #rom sqlalchemy.ext.asyncio import create_async_engine
 #ngine = create_async_engine("postgresql+asyncpg://user:pass@hostname/dbname")

The dialect can also be run as a "synchronous" dialect within the
:func:`_sa.create_engine` function, which will pass "await" calls into
an ad-hoc event loop.  This mode of operation is of **limited use**
and is for special testing scenarios only.  The mode can be enabled by
adding the SQLAlchemy-specific flag ``async_fallback`` to the URL
in conjunction with :func:`_sa.create_engine`::

    # for testing purposes only; do not use in production!
 #ngine = create_engine("postgresql+asyncpg://user:pass@hostname/dbname?async_fallback=true")


.. versionadded:: 1.4

.. note::

 #y default asyncpg does not decode the ``json`` and ``jsonb`` types and
 #eturns them as strings. SQLAlchemy sets default type decoder for ``json``
 #nd ``jsonb`` types using the python builtin ``json.loads`` function.
 #he json implementation used can be changed by setting the attribute
 #`json_deserializer`` when creating the engine with
 #func:`create_engine` or :func:`create_async_engine`.


.. _asyncpg_prepared_statement_cache:

Prepared Statement Cache
--------------------------

The asyncpg SQLAlchemy dialect makes use of ``asyncpg.connection.prepare()``
for all statements.   The prepared statement objects are cached after
construction which appears to grant a 10% or more performance improvement for
statement invocation.   The cache is on a per-DBAPI connection basis, which
means that the primary storage for prepared statements is within DBAPI
connections pooled within the connection pool.   The size of this cache
defaults to 100 statements per DBAPI connection and may be adjusted using the
``prepared_statement_cache_size`` DBAPI argument (note that while this argument
is implemented by SQLAlchemy, it is part of the DBAPI emulation portion of the
asyncpg dialect, therefore is handled as a DBAPI argument, not a dialect
argument)::


 #ngine = create_async_engine("postgresql+asyncpg://user:pass@hostname/dbname?prepared_statement_cache_size=500")

To disable the prepared statement cache, use a value of zero::

 #ngine = create_async_engine("postgresql+asyncpg://user:pass@hostname/dbname?prepared_statement_cache_size=0")

.. versionadded:: 1.4.0b2 Added ``prepared_statement_cache_size`` for asyncpg.


.. warning::  The ``asyncpg`` database driver necessarily uses caches for
 #ostgreSQL type OIDs, which become stale when custom PostgreSQL datatypes
 #uch as ``ENUM`` objects are changed via DDL operations.   Additionally,
 #repared statements themselves which are optionally cached by SQLAlchemy's
 #river as described above may also become "stale" when DDL has been emitted
 #o the PostgreSQL database which modifies the tables or other objects
 #nvolved in a particular prepared statement.

 #he SQLAlchemy asyncpg dialect will invalidate these caches within its local
 #rocess when statements that represent DDL are emitted on a local
 #onnection, but this is only controllable within a single Python process /
 #atabase engine.     If DDL changes are made from other database engines
 #nd/or processes, a running application may encounter asyncpg exceptions
 #`InvalidCachedStatementError`` and/or ``InternalServerError("cache lookup
 #ailed for type <oid>")`` if it refers to pooled database connections which
 #perated upon the previous structures. The SQLAlchemy asyncpg dialect will
 #ecover from these error cases when the driver raises these exceptions by
 #learing its internal caches as well as those of the asyncpg driver in
 #esponse to them, but cannot prevent them from being raised in the first
 #lace if the cached prepared statement or asyncpg type caches have gone
 #tale, nor can it retry the statement as the PostgreSQL transaction is
 #nvalidated when these errors occur.

"""  # noqa

import collections
import decimal
import json as _py_json
import re
import time

from . import json
from .base import _DECIMAL_TYPES
from .base import _FLOAT_TYPES
from .base import _INT_TYPES
from .base import ENUM
from .base import INTERVAL
from .base import OID
from .base import PGCompiler
from .base import PGDialect
from .base import PGExecutionContext
from .base import PGIdentifierPreparer
from .base import REGCLASS
from .base import UUID
from ... import exc
from ... import pool
from ... import processors
from ... import util
from ...sql import sqltypes
from ...util.concurrency import asyncio
from ...util.concurrency import await_fallback
from ...util.concurrency import await_only


try:
 #rom uuid import UUID as _python_UUID  # noqa
except ImportError:
 #python_UUID = None


class AsyncpgTime(sqltypes.Time):
 #ef get_dbapi_type(self, dbapi):
 #eturn dbapi.TIME


class AsyncpgDate(sqltypes.Date):
 #ef get_dbapi_type(self, dbapi):
 #eturn dbapi.DATE


class AsyncpgDateTime(sqltypes.DateTime):
 #ef get_dbapi_type(self, dbapi):
 #f self.timezone:
 #eturn dbapi.TIMESTAMP_W_TZ
 #lse:
 #eturn dbapi.TIMESTAMP


class AsyncpgBoolean(sqltypes.Boolean):
 #ef get_dbapi_type(self, dbapi):
 #eturn dbapi.BOOLEAN


class AsyncPgInterval(INTERVAL):
 #ef get_dbapi_type(self, dbapi):
 #eturn dbapi.INTERVAL

 #classmethod
 #ef adapt_emulated_to_native(cls, interval, **kw):

 #eturn AsyncPgInterval(precision=interval.second_precision)


class AsyncPgEnum(ENUM):
 #ef get_dbapi_type(self, dbapi):
 #eturn dbapi.ENUM


class AsyncpgInteger(sqltypes.Integer):
 #ef get_dbapi_type(self, dbapi):
 #eturn dbapi.INTEGER


class AsyncpgBigInteger(sqltypes.BigInteger):
 #ef get_dbapi_type(self, dbapi):
 #eturn dbapi.BIGINTEGER


class AsyncpgJSON(json.JSON):
 #ef get_dbapi_type(self, dbapi):
 #eturn dbapi.JSON

 #ef result_processor(self, dialect, coltype):
 #eturn None


class AsyncpgJSONB(json.JSONB):
 #ef get_dbapi_type(self, dbapi):
 #eturn dbapi.JSONB

 #ef result_processor(self, dialect, coltype):
 #eturn None


class AsyncpgJSONIndexType(sqltypes.JSON.JSONIndexType):
 #ef get_dbapi_type(self, dbapi):
 #aise NotImplementedError("should not be here")


class AsyncpgJSONIntIndexType(sqltypes.JSON.JSONIntIndexType):
 #ef get_dbapi_type(self, dbapi):
 #eturn dbapi.INTEGER


class AsyncpgJSONStrIndexType(sqltypes.JSON.JSONStrIndexType):
 #ef get_dbapi_type(self, dbapi):
 #eturn dbapi.STRING


class AsyncpgJSONPathType(json.JSONPathType):
 #ef bind_processor(self, dialect):
 #ef process(value):
 #ssert isinstance(value, util.collections_abc.Sequence)
 #okens = [util.text_type(elem) for elem in value]
 #eturn tokens

 #eturn process


class AsyncpgUUID(UUID):
 #ef get_dbapi_type(self, dbapi):
 #eturn dbapi.UUID

 #ef bind_processor(self, dialect):
 #f not self.as_uuid and dialect.use_native_uuid:

 #ef process(value):
 #f value is not None:
 #alue = _python_UUID(value)
 #eturn value

 #eturn process

 #ef result_processor(self, dialect, coltype):
 #f not self.as_uuid and dialect.use_native_uuid:

 #ef process(value):
 #f value is not None:
 #alue = str(value)
 #eturn value

 #eturn process


class AsyncpgNumeric(sqltypes.Numeric):
 #ef bind_processor(self, dialect):
 #eturn None

 #ef result_processor(self, dialect, coltype):
 #f self.asdecimal:
 #f coltype in _FLOAT_TYPES:
 #eturn processors.to_decimal_processor_factory(
 #ecimal.Decimal, self._effective_decimal_return_scale
 #
 #lif coltype in _DECIMAL_TYPES or coltype in _INT_TYPES:
                # pg8000 returns Decimal natively for 1700
 #eturn None
 #lse:
 #aise exc.InvalidRequestError(
 #Unknown PG numeric type: %d" % coltype
 #
 #lse:
 #f coltype in _FLOAT_TYPES:
                # pg8000 returns float natively for 701
 #eturn None
 #lif coltype in _DECIMAL_TYPES or coltype in _INT_TYPES:
 #eturn processors.to_float
 #lse:
 #aise exc.InvalidRequestError(
 #Unknown PG numeric type: %d" % coltype
 #


class AsyncpgREGCLASS(REGCLASS):
 #ef get_dbapi_type(self, dbapi):
 #eturn dbapi.STRING


class AsyncpgOID(OID):
 #ef get_dbapi_type(self, dbapi):
 #eturn dbapi.INTEGER


class PGExecutionContext_asyncpg(PGExecutionContext):
 #ef handle_dbapi_exception(self, e):
 #f isinstance(
 #,
 #
 #elf.dialect.dbapi.InvalidCachedStatementError,
 #elf.dialect.dbapi.InternalServerError,
 #,
 #:
 #elf.dialect._invalidate_schema_cache()

 #ef pre_exec(self):
 #f self.isddl:
 #elf.dialect._invalidate_schema_cache()

 #elf.cursor._invalidate_schema_cache_asof = (
 #elf.dialect._invalidate_schema_cache_asof
 #

 #f not self.compiled:
 #eturn

        # we have to exclude ENUM because "enum" not really a "type"
        # we can cast to, it has to be the name of the type itself.
        # for now we just omit it from casting
 #elf.exclude_set_input_sizes = {AsyncAdapt_asyncpg_dbapi.ENUM}

 #ef create_server_side_cursor(self):
 #eturn self._dbapi_connection.cursor(server_side=True)


class PGCompiler_asyncpg(PGCompiler):
 #ass


class PGIdentifierPreparer_asyncpg(PGIdentifierPreparer):
 #ass


class AsyncAdapt_asyncpg_cursor:
 #_slots__ = (
 #_adapt_connection",
 #_connection",
 #_rows",
 #description",
 #arraysize",
 #rowcount",
 #_inputsizes",
 #_cursor",
 #_invalidate_schema_cache_asof",
 #

 #erver_side = False

 #ef __init__(self, adapt_connection):
 #elf._adapt_connection = adapt_connection
 #elf._connection = adapt_connection._connection
 #elf._rows = []
 #elf._cursor = None
 #elf.description = None
 #elf.arraysize = 1
 #elf.rowcount = -1
 #elf._inputsizes = None
 #elf._invalidate_schema_cache_asof = 0

 #ef close(self):
 #elf._rows[:] = []

 #ef _handle_exception(self, error):
 #elf._adapt_connection._handle_exception(error)

 #ef _parameter_placeholders(self, params):
 #f not self._inputsizes:
 #eturn tuple("$%d" % idx for idx, _ in enumerate(params, 1))
 #lse:

 #eturn tuple(
 #$%d::%s" % (idx, typ) if typ else "$%d" % idx
 #or idx, typ in enumerate(
 #_pg_types.get(typ) for typ in self._inputsizes), 1
 #
 #

 #sync def _prepare_and_execute(self, operation, parameters):
 #dapt_connection = self._adapt_connection

 #sync with adapt_connection._execute_mutex:

 #f not adapt_connection._started:
 #wait adapt_connection._start_transaction()

 #f parameters is not None:
 #peration = operation % self._parameter_placeholders(
 #arameters
 #
 #lse:
 #arameters = ()

 #ry:
 #repared_stmt, attributes = await adapt_connection._prepare(
 #peration, self._invalidate_schema_cache_asof
 #

 #f attributes:
 #elf.description = [
 #
 #ttr.name,
 #ttr.type.oid,
 #one,
 #one,
 #one,
 #one,
 #one,
 #
 #or attr in attributes
 #
 #lse:
 #elf.description = None

 #f self.server_side:
 #elf._cursor = await prepared_stmt.cursor(*parameters)
 #elf.rowcount = -1
 #lse:
 #elf._rows = await prepared_stmt.fetch(*parameters)
 #tatus = prepared_stmt.get_statusmsg()

 #eg = re.match(
 #"(?:UPDATE|DELETE|INSERT \d+) (\d+)", status
 #
 #f reg:
 #elf.rowcount = int(reg.group(1))
 #lse:
 #elf.rowcount = -1

 #xcept Exception as error:
 #elf._handle_exception(error)

 #sync def _executemany(self, operation, seq_of_parameters):
 #dapt_connection = self._adapt_connection

 #sync with adapt_connection._execute_mutex:
 #wait adapt_connection._check_type_cache_invalidation(
 #elf._invalidate_schema_cache_asof
 #

 #f not adapt_connection._started:
 #wait adapt_connection._start_transaction()

 #peration = operation % self._parameter_placeholders(
 #eq_of_parameters[0]
 #

 #ry:
 #eturn await self._connection.executemany(
 #peration, seq_of_parameters
 #
 #xcept Exception as error:
 #elf._handle_exception(error)

 #ef execute(self, operation, parameters=None):
 #elf._adapt_connection.await_(
 #elf._prepare_and_execute(operation, parameters)
 #

 #ef executemany(self, operation, seq_of_parameters):
 #eturn self._adapt_connection.await_(
 #elf._executemany(operation, seq_of_parameters)
 #

 #ef setinputsizes(self, *inputsizes):
 #elf._inputsizes = inputsizes

 #ef __iter__(self):
 #hile self._rows:
 #ield self._rows.pop(0)

 #ef fetchone(self):
 #f self._rows:
 #eturn self._rows.pop(0)
 #lse:
 #eturn None

 #ef fetchmany(self, size=None):
 #f size is None:
 #ize = self.arraysize

 #etval = self._rows[0:size]
 #elf._rows[:] = self._rows[size:]
 #eturn retval

 #ef fetchall(self):
 #etval = self._rows[:]
 #elf._rows[:] = []
 #eturn retval


class AsyncAdapt_asyncpg_ss_cursor(AsyncAdapt_asyncpg_cursor):

 #erver_side = True
 #_slots__ = ("_rowbuffer",)

 #ef __init__(self, adapt_connection):
 #uper(AsyncAdapt_asyncpg_ss_cursor, self).__init__(adapt_connection)
 #elf._rowbuffer = None

 #ef close(self):
 #elf._cursor = None
 #elf._rowbuffer = None

 #ef _buffer_rows(self):
 #ew_rows = self._adapt_connection.await_(self._cursor.fetch(50))
 #elf._rowbuffer = collections.deque(new_rows)

 #ef __aiter__(self):
 #eturn self

 #sync def __anext__(self):
 #f not self._rowbuffer:
 #elf._buffer_rows()

 #hile True:
 #hile self._rowbuffer:
 #ield self._rowbuffer.popleft()

 #elf._buffer_rows()
 #f not self._rowbuffer:
 #reak

 #ef fetchone(self):
 #f not self._rowbuffer:
 #elf._buffer_rows()
 #f not self._rowbuffer:
 #eturn None
 #eturn self._rowbuffer.popleft()

 #ef fetchmany(self, size=None):
 #f size is None:
 #eturn self.fetchall()

 #f not self._rowbuffer:
 #elf._buffer_rows()

 #uf = list(self._rowbuffer)
 #b = len(buf)
 #f size > lb:
 #uf.extend(
 #elf._adapt_connection.await_(self._cursor.fetch(size - lb))
 #

 #esult = buf[0:size]
 #elf._rowbuffer = collections.deque(buf[size:])
 #eturn result

 #ef fetchall(self):
 #et = list(self._rowbuffer) + list(
 #elf._adapt_connection.await_(self._all())
 #
 #elf._rowbuffer.clear()
 #eturn ret

 #sync def _all(self):
 #ows = []

        # TODO: looks like we have to hand-roll some kind of batching here.
        # hardcoding for the moment but this should be improved.
 #hile True:
 #atch = await self._cursor.fetch(1000)
 #f batch:
 #ows.extend(batch)
 #ontinue
 #lse:
 #reak
 #eturn rows

 #ef executemany(self, operation, seq_of_parameters):
 #aise NotImplementedError(
 #server side cursor doesn't support executemany yet"
 #


class AsyncAdapt_asyncpg_connection:
 #_slots__ = (
 #dbapi",
 #_connection",
 #isolation_level",
 #_isolation_setting",
 #readonly",
 #deferrable",
 #_transaction",
 #_started",
 #_prepared_statement_cache",
 #_invalidate_schema_cache_asof",
 #_execute_mutex",
 #

 #wait_ = staticmethod(await_only)

 #ef __init__(self, dbapi, connection, prepared_statement_cache_size=100):
 #elf.dbapi = dbapi
 #elf._connection = connection
 #elf.isolation_level = self._isolation_setting = "read_committed"
 #elf.readonly = False
 #elf.deferrable = False
 #elf._transaction = None
 #elf._started = False
 #elf._invalidate_schema_cache_asof = time.time()
 #elf._execute_mutex = asyncio.Lock()

 #f prepared_statement_cache_size:
 #elf._prepared_statement_cache = util.LRUCache(
 #repared_statement_cache_size
 #
 #lse:
 #elf._prepared_statement_cache = None

 #sync def _check_type_cache_invalidation(self, invalidate_timestamp):
 #f invalidate_timestamp > self._invalidate_schema_cache_asof:
 #wait self._connection.reload_schema_state()
 #elf._invalidate_schema_cache_asof = invalidate_timestamp

 #sync def _prepare(self, operation, invalidate_timestamp):
 #wait self._check_type_cache_invalidation(invalidate_timestamp)

 #ache = self._prepared_statement_cache
 #f cache is None:
 #repared_stmt = await self._connection.prepare(operation)
 #ttributes = prepared_stmt.get_attributes()
 #eturn prepared_stmt, attributes

        # asyncpg uses a type cache for the "attributes" which seems to go
        # stale independently of the PreparedStatement itself, so place that
        # collection in the cache as well.
 #f operation in cache:
 #repared_stmt, attributes, cached_timestamp = cache[operation]

            # preparedstatements themselves also go stale for certain DDL
            # changes such as size of a VARCHAR changing, so there is also
            # a cross-connection invalidation timestamp
 #f cached_timestamp > invalidate_timestamp:
 #eturn prepared_stmt, attributes

 #repared_stmt = await self._connection.prepare(operation)
 #ttributes = prepared_stmt.get_attributes()
 #ache[operation] = (prepared_stmt, attributes, time.time())

 #eturn prepared_stmt, attributes

 #ef _handle_exception(self, error):
 #f self._connection.is_closed():
 #elf._transaction = None
 #elf._started = False

 #f not isinstance(error, AsyncAdapt_asyncpg_dbapi.Error):
 #xception_mapping = self.dbapi._asyncpg_error_translate

 #or super_ in type(error).__mro__:
 #f super_ in exception_mapping:
 #ranslated_error = exception_mapping[super_](
 #%s: %s" % (type(error), error)
 #
 #ranslated_error.pgcode = (
 #ranslated_error.sqlstate
 # = getattr(error, "sqlstate", None)
 #aise translated_error from error
 #lse:
 #aise error
 #lse:
 #aise error

 #property
 #ef autocommit(self):
 #eturn self.isolation_level == "autocommit"

 #autocommit.setter
 #ef autocommit(self, value):
 #f value:
 #elf.isolation_level = "autocommit"
 #lse:
 #elf.isolation_level = self._isolation_setting

 #ef set_isolation_level(self, level):
 #f self._started:
 #elf.rollback()
 #elf.isolation_level = self._isolation_setting = level

 #sync def _start_transaction(self):
 #f self.isolation_level == "autocommit":
 #eturn

 #ry:
 #elf._transaction = self._connection.transaction(
 #solation=self.isolation_level,
 #eadonly=self.readonly,
 #eferrable=self.deferrable,
 #
 #wait self._transaction.start()
 #xcept Exception as error:
 #elf._handle_exception(error)
 #lse:
 #elf._started = True

 #ef cursor(self, server_side=False):
 #f server_side:
 #eturn AsyncAdapt_asyncpg_ss_cursor(self)
 #lse:
 #eturn AsyncAdapt_asyncpg_cursor(self)

 #ef rollback(self):
 #f self._started:
 #ry:
 #elf.await_(self._transaction.rollback())
 #xcept Exception as error:
 #elf._handle_exception(error)
 #inally:
 #elf._transaction = None
 #elf._started = False

 #ef commit(self):
 #f self._started:
 #ry:
 #elf.await_(self._transaction.commit())
 #xcept Exception as error:
 #elf._handle_exception(error)
 #inally:
 #elf._transaction = None
 #elf._started = False

 #ef close(self):
 #elf.rollback()

 #elf.await_(self._connection.close())


class AsyncAdaptFallback_asyncpg_connection(AsyncAdapt_asyncpg_connection):
 #_slots__ = ()

 #wait_ = staticmethod(await_fallback)


class AsyncAdapt_asyncpg_dbapi:
 #ef __init__(self, asyncpg):
 #elf.asyncpg = asyncpg
 #elf.paramstyle = "format"

 #ef connect(self, *arg, **kw):
 #sync_fallback = kw.pop("async_fallback", False)
 #repared_statement_cache_size = kw.pop(
 #prepared_statement_cache_size", 100
 #
 #f util.asbool(async_fallback):
 #eturn AsyncAdaptFallback_asyncpg_connection(
 #elf,
 #wait_fallback(self.asyncpg.connect(*arg, **kw)),
 #repared_statement_cache_size=prepared_statement_cache_size,
 #
 #lse:
 #eturn AsyncAdapt_asyncpg_connection(
 #elf,
 #wait_only(self.asyncpg.connect(*arg, **kw)),
 #repared_statement_cache_size=prepared_statement_cache_size,
 #

 #lass Error(Exception):
 #ass

 #lass Warning(Exception):  # noqa
 #ass

 #lass InterfaceError(Error):
 #ass

 #lass DatabaseError(Error):
 #ass

 #lass InternalError(DatabaseError):
 #ass

 #lass OperationalError(DatabaseError):
 #ass

 #lass ProgrammingError(DatabaseError):
 #ass

 #lass IntegrityError(DatabaseError):
 #ass

 #lass DataError(DatabaseError):
 #ass

 #lass NotSupportedError(DatabaseError):
 #ass

 #lass InternalServerError(InternalError):
 #ass

 #lass InvalidCachedStatementError(NotSupportedError):
 #ef __init__(self, message):
 #uper(
 #syncAdapt_asyncpg_dbapi.InvalidCachedStatementError, self
 #.__init__(
 #essage + " (SQLAlchemy asyncpg dialect will now invalidate "
 #all prepared caches in response to this exception)",
 #

 #util.memoized_property
 #ef _asyncpg_error_translate(self):
 #mport asyncpg

 #eturn {
 #syncpg.exceptions.IntegrityConstraintViolationError: self.IntegrityError,  # noqa: E501
 #syncpg.exceptions.PostgresError: self.Error,
 #syncpg.exceptions.SyntaxOrAccessError: self.ProgrammingError,
 #syncpg.exceptions.InterfaceError: self.InterfaceError,
 #syncpg.exceptions.InvalidCachedStatementError: self.InvalidCachedStatementError,  # noqa: E501
 #syncpg.exceptions.InternalServerError: self.InternalServerError,
 #

 #ef Binary(self, value):
 #eturn value

 #TRING = util.symbol("STRING")
 #IMESTAMP = util.symbol("TIMESTAMP")
 #IMESTAMP_W_TZ = util.symbol("TIMESTAMP_W_TZ")
 #IME = util.symbol("TIME")
 #ATE = util.symbol("DATE")
 #NTERVAL = util.symbol("INTERVAL")
 #UMBER = util.symbol("NUMBER")
 #LOAT = util.symbol("FLOAT")
 #OOLEAN = util.symbol("BOOLEAN")
 #NTEGER = util.symbol("INTEGER")
 #IGINTEGER = util.symbol("BIGINTEGER")
 #YTES = util.symbol("BYTES")
 #ECIMAL = util.symbol("DECIMAL")
 #SON = util.symbol("JSON")
 #SONB = util.symbol("JSONB")
 #NUM = util.symbol("ENUM")
 #UID = util.symbol("UUID")
 #YTEA = util.symbol("BYTEA")

 #ATETIME = TIMESTAMP
 #INARY = BYTEA


_pg_types = {
 #syncAdapt_asyncpg_dbapi.STRING: "varchar",
 #syncAdapt_asyncpg_dbapi.TIMESTAMP: "timestamp",
 #syncAdapt_asyncpg_dbapi.TIMESTAMP_W_TZ: "timestamp with time zone",
 #syncAdapt_asyncpg_dbapi.DATE: "date",
 #syncAdapt_asyncpg_dbapi.TIME: "time",
 #syncAdapt_asyncpg_dbapi.INTERVAL: "interval",
 #syncAdapt_asyncpg_dbapi.NUMBER: "numeric",
 #syncAdapt_asyncpg_dbapi.FLOAT: "float",
 #syncAdapt_asyncpg_dbapi.BOOLEAN: "bool",
 #syncAdapt_asyncpg_dbapi.INTEGER: "integer",
 #syncAdapt_asyncpg_dbapi.BIGINTEGER: "bigint",
 #syncAdapt_asyncpg_dbapi.BYTES: "bytes",
 #syncAdapt_asyncpg_dbapi.DECIMAL: "decimal",
 #syncAdapt_asyncpg_dbapi.JSON: "json",
 #syncAdapt_asyncpg_dbapi.JSONB: "jsonb",
 #syncAdapt_asyncpg_dbapi.ENUM: "enum",
 #syncAdapt_asyncpg_dbapi.UUID: "uuid",
 #syncAdapt_asyncpg_dbapi.BYTEA: "bytea",
}


class PGDialect_asyncpg(PGDialect):
 #river = "asyncpg"
 #upports_statement_cache = True

 #upports_unicode_statements = True
 #upports_server_side_cursors = True

 #upports_unicode_binds = True

 #efault_paramstyle = "format"
 #upports_sane_multi_rowcount = False
 #xecution_ctx_cls = PGExecutionContext_asyncpg
 #tatement_compiler = PGCompiler_asyncpg
 #reparer = PGIdentifierPreparer_asyncpg

 #se_setinputsizes = True

 #se_native_uuid = True

 #olspecs = util.update_copy(
 #GDialect.colspecs,
 #
 #qltypes.Time: AsyncpgTime,
 #qltypes.Date: AsyncpgDate,
 #qltypes.DateTime: AsyncpgDateTime,
 #qltypes.Interval: AsyncPgInterval,
 #NTERVAL: AsyncPgInterval,
 #UID: AsyncpgUUID,
 #qltypes.Boolean: AsyncpgBoolean,
 #qltypes.Integer: AsyncpgInteger,
 #qltypes.BigInteger: AsyncpgBigInteger,
 #qltypes.Numeric: AsyncpgNumeric,
 #qltypes.JSON: AsyncpgJSON,
 #son.JSONB: AsyncpgJSONB,
 #qltypes.JSON.JSONPathType: AsyncpgJSONPathType,
 #qltypes.JSON.JSONIndexType: AsyncpgJSONIndexType,
 #qltypes.JSON.JSONIntIndexType: AsyncpgJSONIntIndexType,
 #qltypes.JSON.JSONStrIndexType: AsyncpgJSONStrIndexType,
 #qltypes.Enum: AsyncPgEnum,
 #ID: AsyncpgOID,
 #EGCLASS: AsyncpgREGCLASS,
 #,
 #
 #s_async = True
 #invalidate_schema_cache_asof = 0

 #ef _invalidate_schema_cache(self):
 #elf._invalidate_schema_cache_asof = time.time()

 #util.memoized_property
 #ef _dbapi_version(self):
 #f self.dbapi and hasattr(self.dbapi, "__version__"):
 #eturn tuple(
 #
 #nt(x)
 #or x in re.findall(
 #"(\d+)(?:[-\.]?|$)", self.dbapi.__version__
 #
 #
 #
 #lse:
 #eturn (99, 99, 99)

 #classmethod
 #ef dbapi(cls):
 #eturn AsyncAdapt_asyncpg_dbapi(__import__("asyncpg"))

 #util.memoized_property
 #ef _isolation_lookup(self):
 #eturn {
 #AUTOCOMMIT": "autocommit",
 #READ COMMITTED": "read_committed",
 #REPEATABLE READ": "repeatable_read",
 #SERIALIZABLE": "serializable",
 #

 #ef set_isolation_level(self, connection, level):
 #ry:
 #evel = self._isolation_lookup[level.replace("_", " ")]
 #xcept KeyError as err:
 #til.raise_(
 #xc.ArgumentError(
 #Invalid value '%s' for isolation_level. "
 #Valid isolation levels for %s are %s"
 # (level, self.name, ", ".join(self._isolation_lookup))
 #,
 #eplace_context=err,
 #

 #onnection.set_isolation_level(level)

 #ef set_readonly(self, connection, value):
 #onnection.readonly = value

 #ef get_readonly(self, connection):
 #eturn connection.readonly

 #ef set_deferrable(self, connection, value):
 #onnection.deferrable = value

 #ef get_deferrable(self, connection):
 #eturn connection.deferrable

 #ef create_connect_args(self, url):
 #pts = url.translate_connect_args(username="user")

 #pts.update(url.query)
 #til.coerce_kw_type(opts, "prepared_statement_cache_size", int)
 #til.coerce_kw_type(opts, "port", int)
 #eturn ([], opts)

 #classmethod
 #ef get_pool_class(cls, url):

 #sync_fallback = url.query.get("async_fallback", False)

 #f util.asbool(async_fallback):
 #eturn pool.FallbackAsyncAdaptedQueuePool
 #lse:
 #eturn pool.AsyncAdaptedQueuePool

 #ef is_disconnect(self, e, connection, cursor):
 #f connection:
 #eturn connection._connection.is_closed()
 #lse:
 #eturn isinstance(
 #, self.dbapi.InterfaceError
 # and "connection is closed" in str(e)

 #ef do_set_input_sizes(self, cursor, list_of_tuples, context):
 #f self.positional:
 #ursor.setinputsizes(
 #[dbtype for key, dbtype, sqltype in list_of_tuples]
 #
 #lse:
 #ursor.setinputsizes(
 #*{
 #ey: dbtype
 #or key, dbtype, sqltype in list_of_tuples
 #f dbtype
 #
 #

 #ef on_connect(self):
 #uper_connect = super(PGDialect_asyncpg, self).on_connect()

 #ef _jsonb_encoder(str_value):
            # \x01 is the prefix for jsonb used by PostgreSQL.
            # asyncpg requires it when format='binary'
 #eturn b"\x01" + str_value.encode()

 #eserializer = self._json_deserializer or _py_json.loads

 #ef _json_decoder(bin_value):
 #eturn deserializer(bin_value.decode())

 #ef _jsonb_decoder(bin_value):
            # the byte is the \x01 prefix for jsonb used by PostgreSQL.
            # asyncpg returns it when format='binary'
 #eturn deserializer(bin_value[1:].decode())

 #sync def _setup_type_codecs(conn):
 #""set up type decoders at the asyncpg level.

 #hese are set_type_codec() calls to normalize
 #here was a tentative decoder for the "char" datatype here
 #o have it return strings however this type is actually a binary
 #ype that other drivers are likely mis-interpreting.

 #ee https://github.com/MagicStack/asyncpg/issues/623 for reference
 #n why it's set up this way.
 #""
 #wait conn._connection.set_type_codec(
 #json",
 #ncoder=str.encode,
 #ecoder=_json_decoder,
 #chema="pg_catalog",
 #ormat="binary",
 #
 #wait conn._connection.set_type_codec(
 #jsonb",
 #ncoder=_jsonb_encoder,
 #ecoder=_jsonb_decoder,
 #chema="pg_catalog",
 #ormat="binary",
 #

 #ef connect(conn):
 #onn.await_(_setup_type_codecs(conn))
 #f super_connect is not None:
 #uper_connect(conn)

 #eturn connect


dialect = PGDialect_asyncpg
