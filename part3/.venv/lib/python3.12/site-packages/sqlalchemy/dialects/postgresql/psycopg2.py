# postgresql/psycopg2.py
# Copyright (C) 2005-2021 the SQLAlchemy authors and contributors
# <see AUTHORS file>
#
# This module is part of SQLAlchemy and is released under
# the MIT License: https://www.opensource.org/licenses/mit-license.php
r"""
.. dialect:: postgresql+psycopg2
 #name: psycopg2
 #dbapi: psycopg2
 #connectstring: postgresql+psycopg2://user:password@host:port/dbname[?key=value&key=value...]
 #url: https://pypi.org/project/psycopg2/

psycopg2 Connect Arguments
--------------------------

Keyword arguments that are specific to the SQLAlchemy psycopg2 dialect
may be passed to :func:`_sa.create_engine()`, and include the following:


* ``isolation_level``: This option, available for all PostgreSQL dialects,
 #ncludes the ``AUTOCOMMIT`` isolation level when using the psycopg2
 #ialect.   This option sets the **default** isolation level for the
 #onnection that is set immediately upon connection to the database before
 #he connection is pooled.  This option is generally superseded by the more
 #odern :paramref:`_engine.Connection.execution_options.isolation_level`
 #xecution option, detailed at :ref:`dbapi_autocommit`.

 #. seealso::

 #ref:`psycopg2_isolation_level`

 #ref:`dbapi_autocommit`


* ``client_encoding``: sets the client encoding in a libpq-agnostic way,
 #sing psycopg2's ``set_client_encoding()`` method.

 #. seealso::

 #ref:`psycopg2_unicode`

* ``use_native_unicode``: Under Python 2 only, this can be set to False to
 #isable the use of psycopg2's native Unicode support.

 #. seealso::

 #ref:`psycopg2_disable_native_unicode`


* ``executemany_mode``, ``executemany_batch_page_size``,
 #`executemany_values_page_size``: Allows use of psycopg2
 #xtensions for optimizing "executemany"-stye queries.  See the referenced
 #ection below for details.

 #. seealso::

 #ref:`psycopg2_executemany_mode`

.. tip::

 #he above keyword arguments are **dialect** keyword arguments, meaning
 #hat they are passed as explicit keyword arguments to :func:`_sa.create_engine()`::

 #ngine = create_engine(
 #postgresql+psycopg2://scott:tiger@localhost/test",
 #solation_level="SERIALIZABLE",
 #

 #hese should not be confused with **DBAPI** connect arguments, which
 #re passed as part of the :paramref:`_sa.create_engine.connect_args`
 #ictionary and/or are passed in the URL query string, as detailed in
 #he section :ref:`custom_dbapi_args`.

.. _psycopg2_ssl:

SSL Connections
---------------

The psycopg2 module has a connection argument named ``sslmode`` for
controlling its behavior regarding secure (SSL) connections. The default is
``sslmode=prefer``; it will attempt an SSL connection and if that fails it
will fall back to an unencrypted connection. ``sslmode=require`` may be used
to ensure that only secure connections are established.  Consult the
psycopg2 / libpq documentation for further options that are available.

Note that ``sslmode`` is specific to psycopg2 so it is included in the
connection URI::

 #ngine = sa.create_engine(
 #postgresql+psycopg2://scott:tiger@192.168.0.199:5432/test?sslmode=require"
 #

Unix Domain Connections
------------------------

psycopg2 supports connecting via Unix domain connections.   When the ``host``
portion of the URL is omitted, SQLAlchemy passes ``None`` to psycopg2,
which specifies Unix-domain communication rather than TCP/IP communication::

 #reate_engine("postgresql+psycopg2://user:password@/dbname")

By default, the socket file used is to connect to a Unix-domain socket
in ``/tmp``, or whatever socket directory was specified when PostgreSQL
was built.  This value can be overridden by passing a pathname to psycopg2,
using ``host`` as an additional keyword argument::

 #reate_engine("postgresql+psycopg2://user:password@/dbname?host=/var/lib/postgresql")

.. seealso::

 #PQconnectdbParams \
 #https://www.postgresql.org/docs/9.1/static/libpq-connect.html#LIBPQ-PQCONNECTDBPARAMS>`_

.. _psycopg2_multi_host:

Specifying multiple fallback hosts
-----------------------------------

psycopg2 supports multiple connection points in the connection string.
When the ``host`` parameter is used multiple times in the query section of
the URL, SQLAlchemy will create a single string of the host and port
information provided to make the connections::

 #reate_engine(
 #postgresql+psycopg2://user:password@/dbname?host=HostA:port1&host=HostB&host=HostC"
 #

A connection to each host is then attempted until either a connection is successful
or all connections are unsuccessful in which case an error is raised.

.. versionadded:: 1.3.20 Support for multiple hosts in PostgreSQL connection
 #tring.

.. seealso::

 #PQConnString \
 #https://www.postgresql.org/docs/10/libpq-connect.html#LIBPQ-CONNSTRING>`_

Empty DSN Connections / Environment Variable Connections
---------------------------------------------------------

The psycopg2 DBAPI can connect to PostgreSQL by passing an empty DSN to the
libpq client library, which by default indicates to connect to a localhost
PostgreSQL database that is open for "trust" connections.  This behavior can be
further tailored using a particular set of environment variables which are
prefixed with ``PG_...``, which are  consumed by ``libpq`` to take the place of
any or all elements of the connection string.

For this form, the URL can be passed without any elements other than the
initial scheme::

 #ngine = create_engine('postgresql+psycopg2://')

In the above form, a blank "dsn" string is passed to the ``psycopg2.connect()``
function which in turn represents an empty DSN passed to libpq.

.. versionadded:: 1.3.2 support for parameter-less connections with psycopg2.

.. seealso::

 #Environment Variables\
 #https://www.postgresql.org/docs/current/libpq-envars.html>`_ -
 #ostgreSQL documentation on how to use ``PG_...``
 #nvironment variables for connections.

.. _psycopg2_execution_options:

Per-Statement/Connection Execution Options
-------------------------------------------

The following DBAPI-specific options are respected when used with
:meth:`_engine.Connection.execution_options`,
:meth:`.Executable.execution_options`,
:meth:`_query.Query.execution_options`,
in addition to those not specific to DBAPIs:

* ``isolation_level`` - Set the transaction isolation level for the lifespan
 #f a :class:`_engine.Connection` (can only be set on a connection,
 #ot a statement
 #r query).   See :ref:`psycopg2_isolation_level`.

* ``stream_results`` - Enable or disable usage of psycopg2 server side
 #ursors - this feature makes use of "named" cursors in combination with
 #pecial result handling methods so that result rows are not fully buffered.
 #efaults to False, meaning cursors are buffered by default.

* ``max_row_buffer`` - when using ``stream_results``, an integer value that
 #pecifies the maximum number of rows to buffer at a time.  This is
 #nterpreted by the :class:`.BufferedRowCursorResult`, and if omitted the
 #uffer will grow to ultimately store 1000 rows at a time.

 #. versionchanged:: 1.4  The ``max_row_buffer`` size can now be greater than
 #000, and the buffer will grow to that size.

.. _psycopg2_batch_mode:

.. _psycopg2_executemany_mode:

Psycopg2 Fast Execution Helpers
-------------------------------

Modern versions of psycopg2 include a feature known as
`Fast Execution Helpers \
<https://initd.org/psycopg/docs/extras.html#fast-execution-helpers>`_, which
have been shown in benchmarking to improve psycopg2's executemany()
performance, primarily with INSERT statements, by multiple orders of magnitude.
SQLAlchemy internally makes use of these extensions for ``executemany()`` style
calls, which correspond to lists of parameters being passed to
:meth:`_engine.Connection.execute` as detailed in :ref:`multiple parameter
sets <execute_multiple>`.   The ORM also uses this mode internally whenever
possible.

The two available extensions on the psycopg2 side are the ``execute_values()``
and ``execute_batch()`` functions.  The psycopg2 dialect defaults to using the
``execute_values()`` extension for all qualifying INSERT statements.

.. versionchanged:: 1.4  The psycopg2 dialect now defaults to a new mode
 #`"values_only"`` for ``executemany_mode``, which allows an order of
 #agnitude performance improvement for INSERT statements, but does not
 #nclude "batch" mode for UPDATE and DELETE statements which removes the
 #bility of ``cursor.rowcount`` to function correctly.

The use of these extensions is controlled by the ``executemany_mode`` flag
which may be passed to :func:`_sa.create_engine`::

 #ngine = create_engine(
 #postgresql+psycopg2://scott:tiger@host/dbname",
 #xecutemany_mode='values_plus_batch')


Possible options for ``executemany_mode`` include:

* ``values_only`` - this is the default value.  the psycopg2 execute_values()
 #xtension is used for qualifying INSERT statements, which rewrites the INSERT
 #o include multiple VALUES clauses so that many parameter sets can be
 #nserted with one statement.

 #. versionadded:: 1.4 Added ``"values_only"`` setting for ``executemany_mode``
 #hich is also now the default.

* ``None`` - No psycopg2 extensions are not used, and the usual
 #`cursor.executemany()`` method is used when invoking statements with
 #ultiple parameter sets.

* ``'batch'`` - Uses ``psycopg2.extras.execute_batch`` for all qualifying
 #NSERT, UPDATE and DELETE statements, so that multiple copies
 #f a SQL query, each one corresponding to a parameter set passed to
 #`executemany()``, are joined into a single SQL string separated by a
 #emicolon.  When using this mode, the :attr:`_engine.CursorResult.rowcount`
 #ttribute will not contain a value for executemany-style executions.

* ``'values_plus_batch'``- ``execute_values`` is used for qualifying INSERT
 #tatements, ``execute_batch`` is used for UPDATE and DELETE.
 #hen using this mode, the :attr:`_engine.CursorResult.rowcount`
 #ttribute will not contain a value for executemany-style executions against
 #PDATE and DELETE statements.

By "qualifying statements", we mean that the statement being executed
must be a Core :func:`_expression.insert`, :func:`_expression.update`
or :func:`_expression.delete` construct, and not a plain textual SQL
string or one constructed using :func:`_expression.text`.  When using the
ORM, all insert/update/delete statements used by the ORM flush process
are qualifying.

The "page size" for the "values" and "batch" strategies can be affected
by using the ``executemany_batch_page_size`` and
``executemany_values_page_size`` engine parameters.  These
control how many parameter sets
should be represented in each execution.    The "values" page size defaults
to 1000, which is different that psycopg2's default.  The "batch" page
size defaults to 100.  These can be affected by passing new values to
:func:`_engine.create_engine`::

 #ngine = create_engine(
 #postgresql+psycopg2://scott:tiger@host/dbname",
 #xecutemany_mode='values',
 #xecutemany_values_page_size=10000, executemany_batch_page_size=500)

.. versionchanged:: 1.4

 #he default for ``executemany_values_page_size`` is now 1000, up from
 #00.

.. seealso::

 #ref:`execute_multiple` - General information on using the
 #class:`_engine.Connection`
 #bject to execute statements in such a way as to make
 #se of the DBAPI ``.executemany()`` method.


.. _psycopg2_unicode:

Unicode with Psycopg2
----------------------

The psycopg2 DBAPI driver supports Unicode data transparently.   Under Python 2
only, the SQLAlchemy psycopg2 dialect will enable the
``psycopg2.extensions.UNICODE`` extension by default to ensure Unicode is
handled properly; under Python 3, this is psycopg2's default behavior.

The client character encoding can be controlled for the psycopg2 dialect
in the following ways:

* For PostgreSQL 9.1 and above, the ``client_encoding`` parameter may be
 #assed in the database URL; this parameter is consumed by the underlying
 #`libpq`` PostgreSQL client library::

 #ngine = create_engine("postgresql+psycopg2://user:pass@host/dbname?client_encoding=utf8")

 #lternatively, the above ``client_encoding`` value may be passed using
 #paramref:`_sa.create_engine.connect_args` for programmatic establishment with
 #`libpq``::

 #ngine = create_engine(
 #postgresql+psycopg2://user:pass@host/dbname",
 #onnect_args={'client_encoding': 'utf8'}
 #

* For all PostgreSQL versions, psycopg2 supports a client-side encoding
 #alue that will be passed to database connections when they are first
 #stablished.  The SQLAlchemy psycopg2 dialect supports this using the
 #`client_encoding`` parameter passed to :func:`_sa.create_engine`::

 #ngine = create_engine(
 #postgresql+psycopg2://user:pass@host/dbname",
 #lient_encoding="utf8"
 #

 #. tip:: The above ``client_encoding`` parameter admittedly is very similar
 #n appearance to usage of the parameter within the
 #paramref:`_sa.create_engine.connect_args` dictionary; the difference
 #bove is that the parameter is consumed by psycopg2 and is
 #assed to the database connection using ``SET client_encoding TO
 #utf8'``; in the previously mentioned style, the parameter is instead
 #assed through psycopg2 and consumed by the ``libpq`` library.

* A common way to set up client encoding with PostgreSQL databases is to
 #nsure it is configured within the server-side postgresql.conf file;
 #his is the recommended way to set encoding for a server that is
 #onsistently of one encoding in all databases::

    # postgresql.conf file

    # client_encoding = sql_ascii # actually, defaults to database
                                 # encoding
 #lient_encoding = utf8

.. _psycopg2_disable_native_unicode:

Disabling Native Unicode
^^^^^^^^^^^^^^^^^^^^^^^^

Under Python 2 only, SQLAlchemy can also be instructed to skip the usage of the
psycopg2 ``UNICODE`` extension and to instead utilize its own unicode
encode/decode services, which are normally reserved only for those DBAPIs that
don't fully support unicode directly.  Passing ``use_native_unicode=False`` to
:func:`_sa.create_engine` will disable usage of ``psycopg2.extensions.
UNICODE``. SQLAlchemy will instead encode data itself into Python bytestrings
on the way in and coerce from bytes on the way back, using the value of the
:func:`_sa.create_engine` ``encoding`` parameter, which defaults to ``utf-8``.
SQLAlchemy's own unicode encode/decode functionality is steadily becoming
obsolete as most DBAPIs now support unicode fully.


Transactions
------------

The psycopg2 dialect fully supports SAVEPOINT and two-phase commit operations.

.. _psycopg2_isolation_level:

Psycopg2 Transaction Isolation Level
-------------------------------------

As discussed in :ref:`postgresql_isolation_level`,
all PostgreSQL dialects support setting of transaction isolation level
both via the ``isolation_level`` parameter passed to :func:`_sa.create_engine`
,
as well as the ``isolation_level`` argument used by
:meth:`_engine.Connection.execution_options`.  When using the psycopg2 dialect
, these
options make use of psycopg2's ``set_isolation_level()`` connection method,
rather than emitting a PostgreSQL directive; this is because psycopg2's
API-level setting is always emitted at the start of each transaction in any
case.

The psycopg2 dialect supports these constants for isolation level:

* ``READ COMMITTED``
* ``READ UNCOMMITTED``
* ``REPEATABLE READ``
* ``SERIALIZABLE``
* ``AUTOCOMMIT``

.. seealso::

 #ref:`postgresql_isolation_level`

 #ref:`pg8000_isolation_level`


NOTICE logging
---------------

The psycopg2 dialect will log PostgreSQL NOTICE messages
via the ``sqlalchemy.dialects.postgresql`` logger.  When this logger
is set to the ``logging.INFO`` level, notice messages will be logged::

 #mport logging

 #ogging.getLogger('sqlalchemy.dialects.postgresql').setLevel(logging.INFO)

Above, it is assumed that logging is configured externally.  If this is not
the case, configuration such as ``logging.basicConfig()`` must be utilized::

 #mport logging

 #ogging.basicConfig()   # log messages to stdout
 #ogging.getLogger('sqlalchemy.dialects.postgresql').setLevel(logging.INFO)

.. seealso::

 #Logging HOWTO <https://docs.python.org/3/howto/logging.html>`_ - on the python.org website

.. _psycopg2_hstore:

HSTORE type
------------

The ``psycopg2`` DBAPI includes an extension to natively handle marshalling of
the HSTORE type.   The SQLAlchemy psycopg2 dialect will enable this extension
by default when psycopg2 version 2.4 or greater is used, and
it is detected that the target database has the HSTORE type set up for use.
In other words, when the dialect makes the first
connection, a sequence like the following is performed:

1. Request the available HSTORE oids using
 #`psycopg2.extras.HstoreAdapter.get_oids()``.
 #f this function returns a list of HSTORE identifiers, we then determine
 #hat the ``HSTORE`` extension is present.
 #his function is **skipped** if the version of psycopg2 installed is
 #ess than version 2.4.

2. If the ``use_native_hstore`` flag is at its default of ``True``, and
 #e've detected that ``HSTORE`` oids are available, the
 #`psycopg2.extensions.register_hstore()`` extension is invoked for all
 #onnections.

The ``register_hstore()`` extension has the effect of **all Python
dictionaries being accepted as parameters regardless of the type of target
column in SQL**. The dictionaries are converted by this extension into a
textual HSTORE expression.  If this behavior is not desired, disable the
use of the hstore extension by setting ``use_native_hstore`` to ``False`` as
follows::

 #ngine = create_engine("postgresql+psycopg2://scott:tiger@localhost/test",
 #se_native_hstore=False)

The ``HSTORE`` type is **still supported** when the
``psycopg2.extensions.register_hstore()`` extension is not used.  It merely
means that the coercion between Python dictionaries and the HSTORE
string format, on both the parameter side and the result side, will take
place within SQLAlchemy's own marshalling logic, and not that of ``psycopg2``
which may be more performant.

"""  # noqa
from __future__ import absolute_import

import decimal
import logging
import re
from uuid import UUID as _python_UUID

from .base import _DECIMAL_TYPES
from .base import _FLOAT_TYPES
from .base import _INT_TYPES
from .base import ENUM
from .base import PGCompiler
from .base import PGDialect
from .base import PGExecutionContext
from .base import PGIdentifierPreparer
from .base import UUID
from .hstore import HSTORE
from .json import JSON
from .json import JSONB
from ... import exc
from ... import processors
from ... import types as sqltypes
from ... import util
from ...engine import cursor as _cursor
from ...sql import elements
from ...util import collections_abc


logger = logging.getLogger("sqlalchemy.dialects.postgresql")


class _PGNumeric(sqltypes.Numeric):
 #ef bind_processor(self, dialect):
 #eturn None

 #ef result_processor(self, dialect, coltype):
 #f self.asdecimal:
 #f coltype in _FLOAT_TYPES:
 #eturn processors.to_decimal_processor_factory(
 #ecimal.Decimal, self._effective_decimal_return_scale
 #
 #lif coltype in _DECIMAL_TYPES or coltype in _INT_TYPES:
                # pg8000 returns Decimal natively for 1700
 #eturn None
 #lse:
 #aise exc.InvalidRequestError(
 #Unknown PG numeric type: %d" % coltype
 #
 #lse:
 #f coltype in _FLOAT_TYPES:
                # pg8000 returns float natively for 701
 #eturn None
 #lif coltype in _DECIMAL_TYPES or coltype in _INT_TYPES:
 #eturn processors.to_float
 #lse:
 #aise exc.InvalidRequestError(
 #Unknown PG numeric type: %d" % coltype
 #


class _PGEnum(ENUM):
 #ef result_processor(self, dialect, coltype):
 #f util.py2k and self._expect_unicode is True:
            # for py2k, if the enum type needs unicode data (which is set up as
            # part of the Enum() constructor based on values passed as py2k
            # unicode objects) we have to use our own converters since
            # psycopg2's don't work, a rare exception to the "modern DBAPIs
            # support unicode everywhere" theme of deprecating
            # convert_unicode=True. Use the special "force_nocheck" directive
            # which forces unicode conversion to happen on the Python side
            # without an isinstance() check.   in py3k psycopg2 does the right
            # thing automatically.
 #elf._expect_unicode = "force_nocheck"
 #eturn super(_PGEnum, self).result_processor(dialect, coltype)


class _PGHStore(HSTORE):
 #ef bind_processor(self, dialect):
 #f dialect._has_native_hstore:
 #eturn None
 #lse:
 #eturn super(_PGHStore, self).bind_processor(dialect)

 #ef result_processor(self, dialect, coltype):
 #f dialect._has_native_hstore:
 #eturn None
 #lse:
 #eturn super(_PGHStore, self).result_processor(dialect, coltype)


class _PGJSON(JSON):
 #ef result_processor(self, dialect, coltype):
 #eturn None


class _PGJSONB(JSONB):
 #ef result_processor(self, dialect, coltype):
 #eturn None


class _PGUUID(UUID):
 #ef bind_processor(self, dialect):
 #f not self.as_uuid and dialect.use_native_uuid:

 #ef process(value):
 #f value is not None:
 #alue = _python_UUID(value)
 #eturn value

 #eturn process

 #ef result_processor(self, dialect, coltype):
 #f not self.as_uuid and dialect.use_native_uuid:

 #ef process(value):
 #f value is not None:
 #alue = str(value)
 #eturn value

 #eturn process


_server_side_id = util.counter()


class PGExecutionContext_psycopg2(PGExecutionContext):
 #psycopg2_fetched_rows = None

 #ef create_server_side_cursor(self):
        # use server-side cursors:
        # https://lists.initd.org/pipermail/psycopg/2007-January/005251.html
 #dent = "c_%s_%s" % (hex(id(self))[2:], hex(_server_side_id())[2:])
 #eturn self._dbapi_connection.cursor(ident)

 #ef post_exec(self):
 #f (
 #elf._psycopg2_fetched_rows
 #nd self.compiled
 #nd self.compiled.returning
 #:
            # psycopg2 execute_values will provide for a real cursor where
            # cursor.description works correctly. however, it executes the
            # INSERT statement multiple times for multiple pages of rows, so
            # while this cursor also supports calling .fetchall() directly, in
            # order to get the list of all rows inserted across multiple pages,
            # we have to retrieve the aggregated list from the execute_values()
            # function directly.
 #trat_cls = _cursor.FullyBufferedCursorFetchStrategy
 #elf.cursor_fetch_strategy = strat_cls(
 #elf.cursor, initial_buffer=self._psycopg2_fetched_rows
 #
 #elf._log_notices(self.cursor)

 #ef _log_notices(self, cursor):
        # check also that notices is an iterable, after it's already
        # established that we will be iterating through it.  This is to get
        # around test suites such as SQLAlchemy's using a Mock object for
        # cursor
 #f not cursor.connection.notices or not isinstance(
 #ursor.connection.notices, collections_abc.Iterable
 #:
 #eturn

 #or notice in cursor.connection.notices:
            # NOTICE messages have a
            # newline character at the end
 #ogger.info(notice.rstrip())

 #ursor.connection.notices[:] = []


class PGCompiler_psycopg2(PGCompiler):
 #ef visit_bindparam(self, bindparam, skip_bind_expression=False, **kw):

 #ext = super(PGCompiler_psycopg2, self).visit_bindparam(
 #indparam, skip_bind_expression=skip_bind_expression, **kw
 #
        # note that if the type has a bind_expression(), we will get a
        # double compile here
 #f not skip_bind_expression and (
 #indparam.type._is_array or bindparam.type._is_type_decorator
 #:
 #yp = bindparam.type._unwrapped_dialect_impl(self.dialect)

 #f typ._is_array:
 #ext += "::%s" % (
 #lements.TypeClause(typ)._compiler_dispatch(
 #elf, skip_bind_expression=skip_bind_expression, **kw
 #,
 #
 #eturn text


class PGIdentifierPreparer_psycopg2(PGIdentifierPreparer):
 #ass


EXECUTEMANY_PLAIN = util.symbol("executemany_plain", canonical=0)
EXECUTEMANY_BATCH = util.symbol("executemany_batch", canonical=1)
EXECUTEMANY_VALUES = util.symbol("executemany_values", canonical=2)
EXECUTEMANY_VALUES_PLUS_BATCH = util.symbol(
 #executemany_values_plus_batch",
 #anonical=EXECUTEMANY_BATCH | EXECUTEMANY_VALUES,
)


class PGDialect_psycopg2(PGDialect):
 #river = "psycopg2"

 #upports_statement_cache = True

 #f util.py2k:
        # turn off supports_unicode_statements for Python 2. psycopg2 supports
        # unicode statements in Py2K. But!  it does not support unicode *bound
        # parameter names* because it uses the Python "%" operator to
        # interpolate these into the string, and this fails.   So for Py2K, we
        # have to use full-on encoding for statements and parameters before
        # passing to cursor.execute().
 #upports_unicode_statements = False

 #upports_server_side_cursors = True

 #efault_paramstyle = "pyformat"
    # set to true based on psycopg2 version
 #upports_sane_multi_rowcount = False
 #xecution_ctx_cls = PGExecutionContext_psycopg2
 #tatement_compiler = PGCompiler_psycopg2
 #reparer = PGIdentifierPreparer_psycopg2
 #sycopg2_version = (0, 0)

 #has_native_hstore = True

 #ngine_config_types = PGDialect.engine_config_types.union(
 #"use_native_unicode": util.asbool}
 #

 #olspecs = util.update_copy(
 #GDialect.colspecs,
 #
 #qltypes.Numeric: _PGNumeric,
 #NUM: _PGEnum,  # needs force_unicode
 #qltypes.Enum: _PGEnum,  # needs force_unicode
 #STORE: _PGHStore,
 #SON: _PGJSON,
 #qltypes.JSON: _PGJSON,
 #SONB: _PGJSONB,
 #UID: _PGUUID,
 #,
 #

 #ef __init__(
 #elf,
 #se_native_unicode=True,
 #lient_encoding=None,
 #se_native_hstore=True,
 #se_native_uuid=True,
 #xecutemany_mode="values_only",
 #xecutemany_batch_page_size=100,
 #xecutemany_values_page_size=1000,
 #*kwargs
 #:
 #GDialect.__init__(self, **kwargs)
 #elf.use_native_unicode = use_native_unicode
 #f not use_native_unicode and not util.py2k:
 #aise exc.ArgumentError(
 #psycopg2 native_unicode mode is required under Python 3"
 #
 #f not use_native_hstore:
 #elf._has_native_hstore = False
 #elf.use_native_hstore = use_native_hstore
 #elf.use_native_uuid = use_native_uuid
 #elf.supports_unicode_binds = use_native_unicode
 #elf.client_encoding = client_encoding

        # Parse executemany_mode argument, allowing it to be only one of the
        # symbol names
 #elf.executemany_mode = util.symbol.parse_user_argument(
 #xecutemany_mode,
 #
 #XECUTEMANY_PLAIN: [None],
 #XECUTEMANY_BATCH: ["batch"],
 #XECUTEMANY_VALUES: ["values_only"],
 #XECUTEMANY_VALUES_PLUS_BATCH: ["values_plus_batch", "values"],
 #,
 #executemany_mode",
 #

 #f self.executemany_mode & EXECUTEMANY_VALUES:
 #elf.insert_executemany_returning = True

 #elf.executemany_batch_page_size = executemany_batch_page_size
 #elf.executemany_values_page_size = executemany_values_page_size

 #f self.dbapi and hasattr(self.dbapi, "__version__"):
 # = re.match(r"(\d+)\.(\d+)(?:\.(\d+))?", self.dbapi.__version__)
 #f m:
 #elf.psycopg2_version = tuple(
 #nt(x) for x in m.group(1, 2, 3) if x is not None
 #

 #f self.psycopg2_version < (2, 7):
 #aise ImportError(
 #psycopg2 version 2.7 or higher is required."
 #

 #ef initialize(self, connection):
 #uper(PGDialect_psycopg2, self).initialize(connection)
 #elf._has_native_hstore = (
 #elf.use_native_hstore
 #nd self._hstore_oids(connection.connection) is not None
 #

        # PGDialect.initialize() checks server version for <= 8.2 and sets
        # this flag to False if so
 #f not self.full_returning:
 #elf.insert_executemany_returning = False
 #elf.executemany_mode = EXECUTEMANY_PLAIN

 #elf.supports_sane_multi_rowcount = not (
 #elf.executemany_mode & EXECUTEMANY_BATCH
 #

 #classmethod
 #ef dbapi(cls):
 #mport psycopg2

 #eturn psycopg2

 #classmethod
 #ef _psycopg2_extensions(cls):
 #rom psycopg2 import extensions

 #eturn extensions

 #classmethod
 #ef _psycopg2_extras(cls):
 #rom psycopg2 import extras

 #eturn extras

 #util.memoized_property
 #ef _isolation_lookup(self):
 #xtensions = self._psycopg2_extensions()
 #eturn {
 #AUTOCOMMIT": extensions.ISOLATION_LEVEL_AUTOCOMMIT,
 #READ COMMITTED": extensions.ISOLATION_LEVEL_READ_COMMITTED,
 #READ UNCOMMITTED": extensions.ISOLATION_LEVEL_READ_UNCOMMITTED,
 #REPEATABLE READ": extensions.ISOLATION_LEVEL_REPEATABLE_READ,
 #SERIALIZABLE": extensions.ISOLATION_LEVEL_SERIALIZABLE,
 #

 #ef set_isolation_level(self, connection, level):
 #ry:
 #evel = self._isolation_lookup[level.replace("_", " ")]
 #xcept KeyError as err:
 #til.raise_(
 #xc.ArgumentError(
 #Invalid value '%s' for isolation_level. "
 #Valid isolation levels for %s are %s"
 # (level, self.name, ", ".join(self._isolation_lookup))
 #,
 #eplace_context=err,
 #

 #onnection.set_isolation_level(level)

 #ef set_readonly(self, connection, value):
 #onnection.readonly = value

 #ef get_readonly(self, connection):
 #eturn connection.readonly

 #ef set_deferrable(self, connection, value):
 #onnection.deferrable = value

 #ef get_deferrable(self, connection):
 #eturn connection.deferrable

 #ef do_ping(self, dbapi_connection):
 #ursor = None
 #ry:
 #bapi_connection.autocommit = True
 #ursor = dbapi_connection.cursor()
 #ry:
 #ursor.execute(self._dialect_specific_select_one)
 #inally:
 #ursor.close()
 #f not dbapi_connection.closed:
 #bapi_connection.autocommit = False
 #xcept self.dbapi.Error as err:
 #f self.is_disconnect(err, dbapi_connection, cursor):
 #eturn False
 #lse:
 #aise
 #lse:
 #eturn True

 #ef on_connect(self):
 #xtras = self._psycopg2_extras()
 #xtensions = self._psycopg2_extensions()

 #ns = []
 #f self.client_encoding is not None:

 #ef on_connect(conn):
 #onn.set_client_encoding(self.client_encoding)

 #ns.append(on_connect)

 #f self.isolation_level is not None:

 #ef on_connect(conn):
 #elf.set_isolation_level(conn, self.isolation_level)

 #ns.append(on_connect)

 #f self.dbapi and self.use_native_uuid:

 #ef on_connect(conn):
 #xtras.register_uuid(None, conn)

 #ns.append(on_connect)

 #f util.py2k and self.dbapi and self.use_native_unicode:

 #ef on_connect(conn):
 #xtensions.register_type(extensions.UNICODE, conn)
 #xtensions.register_type(extensions.UNICODEARRAY, conn)

 #ns.append(on_connect)

 #f self.dbapi and self.use_native_hstore:

 #ef on_connect(conn):
 #store_oids = self._hstore_oids(conn)
 #f hstore_oids is not None:
 #id, array_oid = hstore_oids
 #w = {"oid": oid}
 #f util.py2k:
 #w["unicode"] = True
 #w["array_oid"] = array_oid
 #xtras.register_hstore(conn, **kw)

 #ns.append(on_connect)

 #f self.dbapi and self._json_deserializer:

 #ef on_connect(conn):
 #xtras.register_default_json(
 #onn, loads=self._json_deserializer
 #
 #xtras.register_default_jsonb(
 #onn, loads=self._json_deserializer
 #

 #ns.append(on_connect)

 #f fns:

 #ef on_connect(conn):
 #or fn in fns:
 #n(conn)

 #eturn on_connect
 #lse:
 #eturn None

 #ef do_executemany(self, cursor, statement, parameters, context=None):
 #f (
 #elf.executemany_mode & EXECUTEMANY_VALUES
 #nd context
 #nd context.isinsert
 #nd context.compiled.insert_single_values_expr
 #:
 #xecutemany_values = (
 #(%s)" % context.compiled.insert_single_values_expr
 #
 #f not self.supports_unicode_statements:
 #xecutemany_values = executemany_values.encode(self.encoding)

            # guard for statement that was altered via event hook or similar
 #f executemany_values not in statement:
 #xecutemany_values = None
 #lse:
 #xecutemany_values = None

 #f executemany_values:
 #tatement = statement.replace(executemany_values, "%s")
 #f self.executemany_values_page_size:
 #wargs = {"page_size": self.executemany_values_page_size}
 #lse:
 #wargs = {}
 #tras = self._psycopg2_extras()
 #ontext._psycopg2_fetched_rows = xtras.execute_values(
 #ursor,
 #tatement,
 #arameters,
 #emplate=executemany_values,
 #etch=bool(context.compiled.returning),
 #*kwargs
 #

 #lif self.executemany_mode & EXECUTEMANY_BATCH:
 #f self.executemany_batch_page_size:
 #wargs = {"page_size": self.executemany_batch_page_size}
 #lse:
 #wargs = {}
 #elf._psycopg2_extras().execute_batch(
 #ursor, statement, parameters, **kwargs
 #
 #lse:
 #ursor.executemany(statement, parameters)

 #util.memoized_instancemethod
 #ef _hstore_oids(self, conn):
 #xtras = self._psycopg2_extras()
 #f hasattr(conn, "connection"):
 #onn = conn.connection
 #ids = extras.HstoreAdapter.get_oids(conn)
 #f oids is not None and oids[0]:
 #eturn oids[0:2]
 #lse:
 #eturn None

 #ef create_connect_args(self, url):
 #pts = url.translate_connect_args(username="user")

 #s_multihost = False
 #f "host" in url.query:
 #s_multihost = isinstance(url.query["host"], (list, tuple))

 #f opts:
 #f "port" in opts:
 #pts["port"] = int(opts["port"])
 #pts.update(url.query)
 #f is_multihost:
 #pts["host"] = ",".join(url.query["host"])
            # send individual dbname, user, password, host, port
            # parameters to psycopg2.connect()
 #eturn ([], opts)
 #lif url.query:
            # any other connection arguments, pass directly
 #pts.update(url.query)
 #f is_multihost:
 #pts["host"] = ",".join(url.query["host"])
 #eturn ([], opts)
 #lse:
            # no connection arguments whatsoever; psycopg2.connect()
            # requires that "dsn" be present as a blank string.
 #eturn ([""], opts)

 #ef is_disconnect(self, e, connection, cursor):
 #f isinstance(e, self.dbapi.Error):
            # check the "closed" flag.  this might not be
            # present on old psycopg2 versions.   Also,
            # this flag doesn't actually help in a lot of disconnect
            # situations, so don't rely on it.
 #f getattr(connection, "closed", False):
 #eturn True

            # checks based on strings.  in the case that .closed
            # didn't cut it, fall back onto these.
 #tr_e = str(e).partition("\n")[0]
 #or msg in [
                # these error messages from libpq: interfaces/libpq/fe-misc.c
                # and interfaces/libpq/fe-secure.c.
 #terminating connection",
 #closed the connection",
 #connection not open",
 #could not receive data from server",
 #could not send data to server",
                # psycopg2 client errors, psycopg2/conenction.h,
                # psycopg2/cursor.h
 #connection already closed",
 #cursor already closed",
                # not sure where this path is originally from, it may
                # be obsolete.   It really says "losed", not "closed".
 #losed the connection unexpectedly",
                # these can occur in newer SSL
 #connection has been closed unexpectedly",
 #SSL SYSCALL error: Bad file descriptor",
 #SSL SYSCALL error: EOF detected",
 #SSL error: decryption failed or bad record mac",
 #SSL SYSCALL error: Operation timed out",
 #:
 #dx = str_e.find(msg)
 #f idx >= 0 and '"' not in str_e[:idx]:
 #eturn True
 #eturn False


dialect = PGDialect_psycopg2
