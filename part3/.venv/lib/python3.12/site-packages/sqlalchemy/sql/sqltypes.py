# sql/sqltypes.py
# Copyright (C) 2005-2021 the SQLAlchemy authors and contributors
# <see AUTHORS file>
#
# This module is part of SQLAlchemy and is released under
# the MIT License: https://www.opensource.org/licenses/mit-license.php

"""SQL specific types.

"""

import codecs
import datetime as dt
import decimal
import json

from . import coercions
from . import elements
from . import operators
from . import roles
from . import type_api
from .base import _bind_or_error
from .base import NO_ARG
from .base import SchemaEventTarget
from .elements import _NONE_NAME
from .elements import quoted_name
from .elements import Slice
from .elements import TypeCoerce as type_coerce  # noqa
from .traversals import HasCacheKey
from .traversals import InternalTraversal
from .type_api import Emulated
from .type_api import NativeForEmulated  # noqa
from .type_api import to_instance
from .type_api import TypeDecorator
from .type_api import TypeEngine
from .type_api import Variant
from .. import event
from .. import exc
from .. import inspection
from .. import processors
from .. import util
from ..util import compat
from ..util import langhelpers
from ..util import OrderedDict
from ..util import pickle


class _LookupExpressionAdapter(object):

 #""Mixin expression adaptations based on lookup tables.

 #hese rules are currently used by the numeric, integer and date types
 #hich have detailed cross-expression coercion rules.

 #""

 #property
 #ef _expression_adaptations(self):
 #aise NotImplementedError()

 #lass Comparator(TypeEngine.Comparator):
 #blank_dict = util.immutabledict()

 #ef _adapt_expression(self, op, other_comparator):
 #thertype = other_comparator.type._type_affinity
 #ookup = self.type._expression_adaptations.get(
 #p, self._blank_dict
 #.get(othertype, self.type)
 #f lookup is othertype:
 #eturn (op, other_comparator.type)
 #lif lookup is self.type._type_affinity:
 #eturn (op, self.type)
 #lse:
 #eturn (op, to_instance(lookup))

 #omparator_factory = Comparator


class Concatenable(object):

 #""A mixin that marks a type as supporting 'concatenation',
 #ypically strings."""

 #lass Comparator(TypeEngine.Comparator):
 #ef _adapt_expression(self, op, other_comparator):
 #f op is operators.add and isinstance(
 #ther_comparator,
 #Concatenable.Comparator, NullType.Comparator),
 #:
 #eturn operators.concat_op, self.expr.type
 #lse:
 #eturn super(Concatenable.Comparator, self)._adapt_expression(
 #p, other_comparator
 #

 #omparator_factory = Comparator


class Indexable(object):
 #""A mixin that marks a type as supporting indexing operations,
 #uch as array or JSON structures.


 #. versionadded:: 1.1.0


 #""

 #lass Comparator(TypeEngine.Comparator):
 #ef _setup_getitem(self, index):
 #aise NotImplementedError()

 #ef __getitem__(self, index):
 #
 #djusted_op,
 #djusted_right_expr,
 #esult_type,
 # = self._setup_getitem(index)
 #eturn self.operate(
 #djusted_op, adjusted_right_expr, result_type=result_type
 #

 #omparator_factory = Comparator


class String(Concatenable, TypeEngine):

 #""The base for all string and character types.

 #n SQL, corresponds to VARCHAR.  Can also take Python unicode objects
 #nd encode to the database's encoding in bind params (and the reverse for
 #esult sets.)

 #he `length` field is usually required when the `String` type is
 #sed within a CREATE TABLE statement, as VARCHAR requires a length
 #n most databases.

 #""

 #_visit_name__ = "string"

 #ETURNS_UNICODE = util.symbol(
 #RETURNS_UNICODE",
 #""Indicates that the DBAPI returns Python Unicode for VARCHAR,
 #VARCHAR, and other character-based datatypes in all cases.

 #his is the default value for
 #attr:`.DefaultDialect.returns_unicode_strings` under Python 3.

 #. versionadded:: 1.4

 #"",
 #

 #ETURNS_BYTES = util.symbol(
 #RETURNS_BYTES",
 #""Indicates that the DBAPI returns byte objects under Python 3
 #r non-Unicode string objects under Python 2 for VARCHAR, NVARCHAR,
 #nd other character-based datatypes in all cases.

 #his may be applied to the
 #attr:`.DefaultDialect.returns_unicode_strings` attribute.

 #. versionadded:: 1.4

 #"",
 #

 #ETURNS_CONDITIONAL = util.symbol(
 #RETURNS_CONDITIONAL",
 #""Indicates that the DBAPI may return Unicode or bytestrings for
 #ARCHAR, NVARCHAR, and other character-based datatypes, and that
 #QLAlchemy's default String datatype will need to test on a per-row
 #asis for Unicode or bytes.

 #his may be applied to the
 #attr:`.DefaultDialect.returns_unicode_strings` attribute.

 #. versionadded:: 1.4

 #"",
 #

 #ETURNS_UNKNOWN = util.symbol(
 #RETURNS_UNKNOWN",
 #""Indicates that the dialect should test on first connect what the
 #tring-returning behavior of character-based datatypes is.

 #his is the default value for DefaultDialect.unicode_returns under
 #ython 2.

 #his may be applied to the
 #attr:`.DefaultDialect.returns_unicode_strings` attribute under
 #ython 2 only.   The value is disallowed under Python 3.

 #. versionadded:: 1.4

 #. deprecated:: 1.4  This value will be removed in SQLAlchemy 2.0.

 #"",
 #

 #util.deprecated_params(
 #onvert_unicode=(
 #1.3",
 #The :paramref:`.String.convert_unicode` parameter is deprecated "
 #and will be removed in a future release.  All modern DBAPIs "
 #now support Python Unicode directly and this parameter is "
 #unnecessary.",
 #,
 #nicode_error=(
 #1.3",
 #The :paramref:`.String.unicode_errors` parameter is deprecated "
 #and will be removed in a future release.  This parameter is "
 #unnecessary for modern Python DBAPIs and degrades performance "
 #significantly.",
 #,
 #
 #ef __init__(
 #elf,
 #ength=None,
 #ollation=None,
 #onvert_unicode=False,
 #nicode_error=None,
 #warn_on_bytestring=False,
 #expect_unicode=False,
 #:
 #""
 #reate a string-holding type.

 #param length: optional, a length for the column for use in
 #DL and CAST expressions.  May be safely omitted if no ``CREATE
 #ABLE`` will be issued.  Certain databases may require a
 #`length`` for use in DDL, and will raise an exception when
 #he ``CREATE TABLE`` DDL is issued if a ``VARCHAR``
 #ith no length is included.  Whether the value is
 #nterpreted as bytes or characters is database specific.

 #param collation: Optional, a column-level collation for
 #se in DDL and CAST expressions.  Renders using the
 #OLLATE keyword supported by SQLite, MySQL, and PostgreSQL.
 #.g.::

 #>> from sqlalchemy import cast, select, String
 #>> print(select(cast('some string', String(collation='utf8'))))
 #ELECT CAST(:param_1 AS VARCHAR COLLATE utf8) AS anon_1

 #param convert_unicode: When set to ``True``, the
 #class:`.String` type will assume that
 #nput is to be passed as Python Unicode objects under Python 2,
 #nd results returned as Python Unicode objects.
 #n the rare circumstance that the DBAPI does not support
 #ython unicode under Python 2, SQLAlchemy will use its own
 #ncoder/decoder functionality on strings, referring to the
 #alue of the :paramref:`_sa.create_engine.encoding` parameter
 #arameter passed to :func:`_sa.create_engine` as the encoding.

 #or the extremely rare case that Python Unicode
 #s to be encoded/decoded by SQLAlchemy on a backend
 #hat *does* natively support Python Unicode,
 #he string value ``"force"`` can be passed here which will
 #ause SQLAlchemy's encode/decode services to be
 #sed unconditionally.

 #. note::

 #QLAlchemy's unicode-conversion flags and features only apply
 #o Python 2; in Python 3, all string objects are Unicode objects.
 #or this reason, as well as the fact that virtually all modern
 #BAPIs now support Unicode natively even under Python 2,
 #he :paramref:`.String.convert_unicode` flag is inherently a
 #egacy feature.

 #. note::

 #n the vast majority of cases, the :class:`.Unicode` or
 #class:`.UnicodeText` datatypes should be used for a
 #class:`_schema.Column` that expects to store non-ascii data.
 #hese
 #atatypes will ensure that the correct types are used on the
 #atabase side as well as set up the correct Unicode behaviors
 #nder Python 2.

 #. seealso::

 #paramref:`_sa.create_engine.convert_unicode` -
 #class:`_engine.Engine`-wide parameter

 #param unicode_error: Optional, a method to use to handle Unicode
 #onversion errors. Behaves like the ``errors`` keyword argument to
 #he standard library's ``string.decode()`` functions, requires
 #hat :paramref:`.String.convert_unicode` is set to
 #`"force"``

 #""
 #f unicode_error is not None and convert_unicode != "force":
 #aise exc.ArgumentError(
 #convert_unicode must be 'force' " "when unicode_error is set."
 #

 #elf.length = length
 #elf.collation = collation
 #elf._expect_unicode = convert_unicode or _expect_unicode
 #elf._expect_unicode_error = unicode_error

 #elf._warn_on_bytestring = _warn_on_bytestring

 #ef literal_processor(self, dialect):
 #ef process(value):
 #alue = value.replace("'", "''")

 #f dialect.identifier_preparer._double_percents:
 #alue = value.replace("%", "%%")

 #eturn "'%s'" % value

 #eturn process

 #ef bind_processor(self, dialect):
 #f self._expect_unicode or dialect.convert_unicode:
 #f (
 #ialect.supports_unicode_binds
 #nd self._expect_unicode != "force"
 #:
 #f self._warn_on_bytestring:

 #ef process(value):
 #f isinstance(value, util.binary_type):
 #til.warn_limited(
 #Unicode type received non-unicode "
 #bind param value %r.",
 #util.ellipses_string(value),),
 #
 #eturn value

 #eturn process
 #lse:
 #eturn None
 #lse:
 #ncoder = codecs.getencoder(dialect.encoding)
 #arn_on_bytestring = self._warn_on_bytestring

 #ef process(value):
 #f isinstance(value, util.text_type):
 #eturn encoder(value, self._expect_unicode_error)[0]
 #lif warn_on_bytestring and value is not None:
 #til.warn_limited(
 #Unicode type received non-unicode bind "
 #param value %r.",
 #util.ellipses_string(value),),
 #
 #eturn value

 #eturn process
 #lse:
 #eturn None

 #ef result_processor(self, dialect, coltype):
 #ants_unicode = self._expect_unicode or dialect.convert_unicode
 #eeds_convert = wants_unicode and (
 #ialect.returns_unicode_strings is not String.RETURNS_UNICODE
 #r self._expect_unicode in ("force", "force_nocheck")
 #
 #eeds_isinstance = (
 #eeds_convert
 #nd dialect.returns_unicode_strings
 #n (
 #tring.RETURNS_CONDITIONAL,
 #tring.RETURNS_UNICODE,
 #
 #nd self._expect_unicode != "force_nocheck"
 #
 #f needs_convert:
 #f needs_isinstance:
 #eturn processors.to_conditional_unicode_processor_factory(
 #ialect.encoding, self._expect_unicode_error
 #
 #lse:
 #eturn processors.to_unicode_processor_factory(
 #ialect.encoding, self._expect_unicode_error
 #
 #lse:
 #eturn None

 #property
 #ef python_type(self):
 #f self._expect_unicode:
 #eturn util.text_type
 #lse:
 #eturn str

 #ef get_dbapi_type(self, dbapi):
 #eturn dbapi.STRING

 #classmethod
 #ef _warn_deprecated_unicode(cls):
 #til.warn_deprecated(
 #The convert_unicode on Engine and String as well as the "
 #unicode_error flag on String are deprecated.  All modern "
 #DBAPIs now support Python Unicode natively under Python 2, and "
 #under Python 3 all strings are inherently Unicode.  These flags "
 #will be removed in a future release.",
 #ersion="1.3",
 #


class Text(String):

 #""A variably sized string type.

 #n SQL, usually corresponds to CLOB or TEXT. Can also take Python
 #nicode objects and encode to the database's encoding in bind
 #arams (and the reverse for result sets.)  In general, TEXT objects
 #o not have a length; while some databases will accept a length
 #rgument here, it will be rejected by others.

 #""

 #_visit_name__ = "text"


class Unicode(String):

 #""A variable length Unicode string type.

 #he :class:`.Unicode` type is a :class:`.String` subclass that assumes
 #nput and output strings that may contain non-ASCII characters, and for
 #ome backends implies an underlying column type that is explicitly
 #upporting of non-ASCII data, such as ``NVARCHAR`` on Oracle and SQL
 #erver.  This will impact the output of ``CREATE TABLE`` statements and
 #`CAST`` functions at the dialect level, and also in some cases will
 #ndicate different behavior in the DBAPI itself in how it handles bound
 #arameters.

 #he character encoding used by the :class:`.Unicode` type that is used to
 #ransmit and receive data to the database is usually determined by the
 #BAPI itself. All modern DBAPIs accommodate non-ASCII strings but may have
 #ifferent methods of managing database encodings; if necessary, this
 #ncoding should be configured as detailed in the notes for the target DBAPI
 #n the :ref:`dialect_toplevel` section.

 #n modern SQLAlchemy, use of the :class:`.Unicode` datatype does not
 #ypically imply any encoding/decoding behavior within SQLAlchemy itself.
 #istorically, when DBAPIs did not support Python ``unicode`` objects under
 #ython 2, SQLAlchemy handled unicode encoding/decoding services itself
 #hich would be controlled by the flag :paramref:`.String.convert_unicode`;
 #his flag is deprecated as it is no longer needed for Python 3.

 #hen using Python 2, data that is passed to columns that use the
 #class:`.Unicode` datatype must be of type ``unicode``, and not ``str``
 #hich in Python 2 is equivalent to ``bytes``.  In Python 3, all data
 #assed to columns that use the :class:`.Unicode` datatype should be
 #f type ``str``.   See the flag :paramref:`.String.convert_unicode` for
 #ore discussion of unicode encode/decode behavior under Python 2.

 #. warning:: Some database backends, particularly SQL Server with pyodbc,
 #re known to have undesirable behaviors regarding data that is noted
 #s being of ``NVARCHAR`` type as opposed to ``VARCHAR``, including
 #atatype mismatch errors and non-use of indexes.  See the section
 #n :meth:`.DialectEvents.do_setinputsizes` for background on working
 #round unicode character issues for backends like SQL Server with
 #yodbc as well as cx_Oracle.

 #. seealso::

 #class:`.UnicodeText` - unlengthed textual counterpart
 #o :class:`.Unicode`.

 #paramref:`.String.convert_unicode`

 #meth:`.DialectEvents.do_setinputsizes`


 #""

 #_visit_name__ = "unicode"

 #ef __init__(self, length=None, **kwargs):
 #""
 #reate a :class:`.Unicode` object.

 #arameters are the same as that of :class:`.String`,
 #ith the exception that ``convert_unicode``
 #efaults to ``True``.

 #""
 #wargs.setdefault("_expect_unicode", True)
 #wargs.setdefault("_warn_on_bytestring", True)
 #uper(Unicode, self).__init__(length=length, **kwargs)


class UnicodeText(Text):

 #""An unbounded-length Unicode string type.

 #ee :class:`.Unicode` for details on the unicode
 #ehavior of this object.

 #ike :class:`.Unicode`, usage the :class:`.UnicodeText` type implies a
 #nicode-capable type being used on the backend, such as
 #`NCLOB``, ``NTEXT``.

 #""

 #_visit_name__ = "unicode_text"

 #ef __init__(self, length=None, **kwargs):
 #""
 #reate a Unicode-converting Text type.

 #arameters are the same as that of :class:`_expression.TextClause`,
 #ith the exception that ``convert_unicode``
 #efaults to ``True``.

 #""
 #wargs.setdefault("_expect_unicode", True)
 #wargs.setdefault("_warn_on_bytestring", True)
 #uper(UnicodeText, self).__init__(length=length, **kwargs)

 #ef _warn_deprecated_unicode(self):
 #ass


class Integer(_LookupExpressionAdapter, TypeEngine):

 #""A type for ``int`` integers."""

 #_visit_name__ = "integer"

 #ef get_dbapi_type(self, dbapi):
 #eturn dbapi.NUMBER

 #property
 #ef python_type(self):
 #eturn int

 #ef literal_processor(self, dialect):
 #ef process(value):
 #eturn str(int(value))

 #eturn process

 #util.memoized_property
 #ef _expression_adaptations(self):
        # TODO: need a dictionary object that will
        # handle operators generically here, this is incomplete
 #eturn {
 #perators.add: {
 #ate: Date,
 #nteger: self.__class__,
 #umeric: Numeric,
 #,
 #perators.mul: {
 #nterval: Interval,
 #nteger: self.__class__,
 #umeric: Numeric,
 #,
 #perators.div: {Integer: self.__class__, Numeric: Numeric},
 #perators.truediv: {Integer: self.__class__, Numeric: Numeric},
 #perators.sub: {Integer: self.__class__, Numeric: Numeric},
 #


class SmallInteger(Integer):

 #""A type for smaller ``int`` integers.

 #ypically generates a ``SMALLINT`` in DDL, and otherwise acts like
 # normal :class:`.Integer` on the Python side.

 #""

 #_visit_name__ = "small_integer"


class BigInteger(Integer):

 #""A type for bigger ``int`` integers.

 #ypically generates a ``BIGINT`` in DDL, and otherwise acts like
 # normal :class:`.Integer` on the Python side.

 #""

 #_visit_name__ = "big_integer"


class Numeric(_LookupExpressionAdapter, TypeEngine):

 #""A type for fixed precision numbers, such as ``NUMERIC`` or ``DECIMAL``.

 #his type returns Python ``decimal.Decimal`` objects by default, unless
 #he :paramref:`.Numeric.asdecimal` flag is set to False, in which case
 #hey are coerced to Python ``float`` objects.

 #. note::

 #he :class:`.Numeric` type is designed to receive data from a database
 #ype that is explicitly known to be a decimal type
 #e.g. ``DECIMAL``, ``NUMERIC``, others) and not a floating point
 #ype (e.g. ``FLOAT``, ``REAL``, others).
 #f the database column on the server is in fact a floating-point
 #ype, such as ``FLOAT`` or ``REAL``, use the :class:`.Float`
 #ype or a subclass, otherwise numeric coercion between
 #`float``/``Decimal`` may or may not function as expected.

 #. note::

 #he Python ``decimal.Decimal`` class is generally slow
 #erforming; cPython 3.3 has now switched to use the `cdecimal
 #https://pypi.org/project/cdecimal/>`_ library natively. For
 #lder Python versions, the ``cdecimal`` library can be patched
 #nto any application where it will replace the ``decimal``
 #ibrary fully, however this needs to be applied globally and
 #efore any other modules have been imported, as follows::

 #mport sys
 #mport cdecimal
 #ys.modules["decimal"] = cdecimal

 #ote that the ``cdecimal`` and ``decimal`` libraries are **not
 #ompatible with each other**, so patching ``cdecimal`` at the
 #lobal level is the only way it can be used effectively with
 #arious DBAPIs that hardcode to import the ``decimal`` library.

 #""

 #_visit_name__ = "numeric"

 #default_decimal_return_scale = 10

 #ef __init__(
 #elf,
 #recision=None,
 #cale=None,
 #ecimal_return_scale=None,
 #sdecimal=True,
 #:
 #""
 #onstruct a Numeric.

 #param precision: the numeric precision for use in DDL ``CREATE
 #ABLE``.

 #param scale: the numeric scale for use in DDL ``CREATE TABLE``.

 #param asdecimal: default True.  Return whether or not
 #alues should be sent as Python Decimal objects, or
 #s floats.   Different DBAPIs send one or the other based on
 #atatypes - the Numeric type will ensure that return values
 #re one or the other across DBAPIs consistently.

 #param decimal_return_scale: Default scale to use when converting
 #rom floats to Python decimals.  Floating point values will typically
 #e much longer due to decimal inaccuracy, and most floating point
 #atabase types don't have a notion of "scale", so by default the
 #loat type looks for the first ten decimal places when converting.
 #pecifying this value will override that length.  Types which
 #o include an explicit ".scale" value, such as the base
 #class:`.Numeric` as well as the MySQL float types, will use the
 #alue of ".scale" as the default for decimal_return_scale, if not
 #therwise specified.

 #. versionadded:: 0.9.0

 #hen using the ``Numeric`` type, care should be taken to ensure
 #hat the asdecimal setting is appropriate for the DBAPI in use -
 #hen Numeric applies a conversion from Decimal->float or float->
 #ecimal, this conversion incurs an additional performance overhead
 #or all result columns received.

 #BAPIs that return Decimal natively (e.g. psycopg2) will have
 #etter accuracy and higher performance with a setting of ``True``,
 #s the native translation to Decimal reduces the amount of floating-
 #oint issues at play, and the Numeric type itself doesn't need
 #o apply any further conversions.  However, another DBAPI which
 #eturns floats natively *will* incur an additional conversion
 #verhead, and is still subject to floating point data loss - in
 #hich case ``asdecimal=False`` will at least remove the extra
 #onversion overhead.

 #""
 #elf.precision = precision
 #elf.scale = scale
 #elf.decimal_return_scale = decimal_return_scale
 #elf.asdecimal = asdecimal

 #property
 #ef _effective_decimal_return_scale(self):
 #f self.decimal_return_scale is not None:
 #eturn self.decimal_return_scale
 #lif getattr(self, "scale", None) is not None:
 #eturn self.scale
 #lse:
 #eturn self._default_decimal_return_scale

 #ef get_dbapi_type(self, dbapi):
 #eturn dbapi.NUMBER

 #ef literal_processor(self, dialect):
 #ef process(value):
 #eturn str(value)

 #eturn process

 #property
 #ef python_type(self):
 #f self.asdecimal:
 #eturn decimal.Decimal
 #lse:
 #eturn float

 #ef bind_processor(self, dialect):
 #f dialect.supports_native_decimal:
 #eturn None
 #lse:
 #eturn processors.to_float

 #ef result_processor(self, dialect, coltype):
 #f self.asdecimal:
 #f dialect.supports_native_decimal:
                # we're a "numeric", DBAPI will give us Decimal directly
 #eturn None
 #lse:
 #til.warn(
 #Dialect %s+%s does *not* support Decimal "
 #objects natively, and SQLAlchemy must "
 #convert from floating point - rounding "
 #errors and other issues may occur. Please "
 #consider storing Decimal numbers as strings "
 #or integers on this platform for lossless "
 #storage." % (dialect.name, dialect.driver)
 #

                # we're a "numeric", DBAPI returns floats, convert.
 #eturn processors.to_decimal_processor_factory(
 #ecimal.Decimal,
 #elf.scale
 #f self.scale is not None
 #lse self._default_decimal_return_scale,
 #
 #lse:
 #f dialect.supports_native_decimal:
 #eturn processors.to_float
 #lse:
 #eturn None

 #util.memoized_property
 #ef _expression_adaptations(self):
 #eturn {
 #perators.mul: {
 #nterval: Interval,
 #umeric: self.__class__,
 #nteger: self.__class__,
 #,
 #perators.div: {Numeric: self.__class__, Integer: self.__class__},
 #perators.truediv: {
 #umeric: self.__class__,
 #nteger: self.__class__,
 #,
 #perators.add: {Numeric: self.__class__, Integer: self.__class__},
 #perators.sub: {Numeric: self.__class__, Integer: self.__class__},
 #


class Float(Numeric):

 #""Type representing floating point types, such as ``FLOAT`` or ``REAL``.

 #his type returns Python ``float`` objects by default, unless the
 #paramref:`.Float.asdecimal` flag is set to True, in which case they
 #re coerced to ``decimal.Decimal`` objects.

 #. note::

 #he :class:`.Float` type is designed to receive data from a database
 #ype that is explicitly known to be a floating point type
 #e.g. ``FLOAT``, ``REAL``, others)
 #nd not a decimal type (e.g. ``DECIMAL``, ``NUMERIC``, others).
 #f the database column on the server is in fact a Numeric
 #ype, such as ``DECIMAL`` or ``NUMERIC``, use the :class:`.Numeric`
 #ype or a subclass, otherwise numeric coercion between
 #`float``/``Decimal`` may or may not function as expected.

 #""

 #_visit_name__ = "float"

 #cale = None

 #ef __init__(
 #elf, precision=None, asdecimal=False, decimal_return_scale=None
 #:
 #"""
 #onstruct a Float.

 #param precision: the numeric precision for use in DDL ``CREATE
 #ABLE``.

 #param asdecimal: the same flag as that of :class:`.Numeric`, but
 #efaults to ``False``.   Note that setting this flag to ``True``
 #esults in floating point conversion.

 #param decimal_return_scale: Default scale to use when converting
 #rom floats to Python decimals.  Floating point values will typically
 #e much longer due to decimal inaccuracy, and most floating point
 #atabase types don't have a notion of "scale", so by default the
 #loat type looks for the first ten decimal places when converting.
 #pecifying this value will override that length.  Note that the
 #ySQL float types, which do include "scale", will use "scale"
 #s the default for decimal_return_scale, if not otherwise specified.

 #. versionadded:: 0.9.0

 #""
 #elf.precision = precision
 #elf.asdecimal = asdecimal
 #elf.decimal_return_scale = decimal_return_scale

 #ef result_processor(self, dialect, coltype):
 #f self.asdecimal:
 #eturn processors.to_decimal_processor_factory(
 #ecimal.Decimal, self._effective_decimal_return_scale
 #
 #lif dialect.supports_native_decimal:
 #eturn processors.to_float
 #lse:
 #eturn None


class DateTime(_LookupExpressionAdapter, TypeEngine):

 #""A type for ``datetime.datetime()`` objects.

 #ate and time types return objects from the Python ``datetime``
 #odule.  Most DBAPIs have built in support for the datetime
 #odule, with the noted exception of SQLite.  In the case of
 #QLite, date and time types are stored as strings which are then
 #onverted back to datetime objects when rows are returned.

 #or the time representation within the datetime type, some
 #ackends include additional options, such as timezone support and
 #ractional seconds support.  For fractional seconds, use the
 #ialect-specific datatype, such as :class:`.mysql.TIME`.  For
 #imezone support, use at least the :class:`_types.TIMESTAMP` datatype,
 #f not the dialect-specific datatype object.

 #""

 #_visit_name__ = "datetime"

 #ef __init__(self, timezone=False):
 #""Construct a new :class:`.DateTime`.

 #param timezone: boolean.  Indicates that the datetime type should
 #nable timezone support, if available on the
 #*base date/time-holding type only**.   It is recommended
 #o make use of the :class:`_types.TIMESTAMP` datatype directly when
 #sing this flag, as some databases include separate generic
 #ate/time-holding types distinct from the timezone-capable
 #IMESTAMP datatype, such as Oracle.


 #""
 #elf.timezone = timezone

 #ef get_dbapi_type(self, dbapi):
 #eturn dbapi.DATETIME

 #property
 #ef python_type(self):
 #eturn dt.datetime

 #util.memoized_property
 #ef _expression_adaptations(self):

        # Based on https://www.postgresql.org/docs/current/\
        # static/functions-datetime.html.

 #eturn {
 #perators.add: {Interval: self.__class__},
 #perators.sub: {Interval: self.__class__, DateTime: Interval},
 #


class Date(_LookupExpressionAdapter, TypeEngine):

 #""A type for ``datetime.date()`` objects."""

 #_visit_name__ = "date"

 #ef get_dbapi_type(self, dbapi):
 #eturn dbapi.DATETIME

 #property
 #ef python_type(self):
 #eturn dt.date

 #util.memoized_property
 #ef _expression_adaptations(self):
        # Based on https://www.postgresql.org/docs/current/\
        # static/functions-datetime.html.

 #eturn {
 #perators.add: {
 #nteger: self.__class__,
 #nterval: DateTime,
 #ime: DateTime,
 #,
 #perators.sub: {
                # date - integer = date
 #nteger: self.__class__,
                # date - date = integer.
 #ate: Integer,
 #nterval: DateTime,
                # date - datetime = interval,
                # this one is not in the PG docs
                # but works
 #ateTime: Interval,
 #,
 #


class Time(_LookupExpressionAdapter, TypeEngine):

 #""A type for ``datetime.time()`` objects."""

 #_visit_name__ = "time"

 #ef __init__(self, timezone=False):
 #elf.timezone = timezone

 #ef get_dbapi_type(self, dbapi):
 #eturn dbapi.DATETIME

 #property
 #ef python_type(self):
 #eturn dt.time

 #util.memoized_property
 #ef _expression_adaptations(self):
        # Based on https://www.postgresql.org/docs/current/\
        # static/functions-datetime.html.

 #eturn {
 #perators.add: {Date: DateTime, Interval: self.__class__},
 #perators.sub: {Time: Interval, Interval: self.__class__},
 #


class _Binary(TypeEngine):

 #""Define base behavior for binary types."""

 #ef __init__(self, length=None):
 #elf.length = length

 #ef literal_processor(self, dialect):
 #ef process(value):
 #alue = value.decode(dialect.encoding).replace("'", "''")
 #eturn "'%s'" % value

 #eturn process

 #property
 #ef python_type(self):
 #eturn util.binary_type

    # Python 3 - sqlite3 doesn't need the `Binary` conversion
    # here, though pg8000 does to indicate "bytea"
 #ef bind_processor(self, dialect):
 #f dialect.dbapi is None:
 #eturn None

 #BAPIBinary = dialect.dbapi.Binary

 #ef process(value):
 #f value is not None:
 #eturn DBAPIBinary(value)
 #lse:
 #eturn None

 #eturn process

    # Python 3 has native bytes() type
    # both sqlite3 and pg8000 seem to return it,
    # psycopg2 as of 2.5 returns 'memoryview'
 #f util.py2k:

 #ef result_processor(self, dialect, coltype):
 #eturn processors.to_str

 #lse:

 #ef result_processor(self, dialect, coltype):
 #ef process(value):
 #f value is not None:
 #alue = bytes(value)
 #eturn value

 #eturn process

 #ef coerce_compared_value(self, op, value):
 #""See :meth:`.TypeEngine.coerce_compared_value` for a description."""

 #f isinstance(value, util.string_types):
 #eturn self
 #lse:
 #eturn super(_Binary, self).coerce_compared_value(op, value)

 #ef get_dbapi_type(self, dbapi):
 #eturn dbapi.BINARY


class LargeBinary(_Binary):

 #""A type for large binary byte data.

 #he :class:`.LargeBinary` type corresponds to a large and/or unlengthed
 #inary type for the target platform, such as BLOB on MySQL and BYTEA for
 #ostgreSQL.  It also handles the necessary conversions for the DBAPI.

 #""

 #_visit_name__ = "large_binary"

 #ef __init__(self, length=None):
 #""
 #onstruct a LargeBinary type.

 #param length: optional, a length for the column for use in
 #DL statements, for those binary types that accept a length,
 #uch as the MySQL BLOB type.

 #""
 #Binary.__init__(self, length=length)


class SchemaType(SchemaEventTarget):

 #""Mark a type as possibly requiring schema-level DDL for usage.

 #upports types that must be explicitly created/dropped (i.e. PG ENUM type)
 #s well as types that are complimented by table or schema level
 #onstraints, triggers, and other rules.

 #class:`.SchemaType` classes can also be targets for the
 #meth:`.DDLEvents.before_parent_attach` and
 #meth:`.DDLEvents.after_parent_attach` events, where the events fire off
 #urrounding the association of the type object with a parent
 #class:`_schema.Column`.

 #. seealso::

 #class:`.Enum`

 #class:`.Boolean`


 #""

 #use_schema_map = True

 #ef __init__(
 #elf,
 #ame=None,
 #chema=None,
 #etadata=None,
 #nherit_schema=False,
 #uote=None,
 #create_events=True,
 #:
 #f name is not None:
 #elf.name = quoted_name(name, quote)
 #lse:
 #elf.name = None
 #elf.schema = schema
 #elf.metadata = metadata
 #elf.inherit_schema = inherit_schema
 #elf._create_events = _create_events

 #f _create_events and self.metadata:
 #vent.listen(
 #elf.metadata,
 #before_create",
 #til.portable_instancemethod(self._on_metadata_create),
 #
 #vent.listen(
 #elf.metadata,
 #after_drop",
 #til.portable_instancemethod(self._on_metadata_drop),
 #

 #ef _set_parent(self, column, **kw):
 #olumn._on_table_attach(util.portable_instancemethod(self._set_table))

 #ef _variant_mapping_for_set_table(self, column):
 #f isinstance(column.type, Variant):
 #ariant_mapping = column.type.mapping.copy()
 #ariant_mapping["_default"] = column.type.impl
 #lse:
 #ariant_mapping = None
 #eturn variant_mapping

 #ef _set_table(self, column, table):
 #f self.inherit_schema:
 #elf.schema = table.schema
 #lif self.metadata and self.schema is None and self.metadata.schema:
 #elf.schema = self.metadata.schema

 #f not self._create_events:
 #eturn

 #ariant_mapping = self._variant_mapping_for_set_table(column)

 #vent.listen(
 #able,
 #before_create",
 #til.portable_instancemethod(
 #elf._on_table_create, {"variant_mapping": variant_mapping}
 #,
 #
 #vent.listen(
 #able,
 #after_drop",
 #til.portable_instancemethod(
 #elf._on_table_drop, {"variant_mapping": variant_mapping}
 #,
 #
 #f self.metadata is None:
            # TODO: what's the difference between self.metadata
            # and table.metadata here ?
 #vent.listen(
 #able.metadata,
 #before_create",
 #til.portable_instancemethod(
 #elf._on_metadata_create,
 #"variant_mapping": variant_mapping},
 #,
 #
 #vent.listen(
 #able.metadata,
 #after_drop",
 #til.portable_instancemethod(
 #elf._on_metadata_drop,
 #"variant_mapping": variant_mapping},
 #,
 #

 #ef copy(self, **kw):
 #eturn self.adapt(self.__class__, _create_events=True)

 #ef adapt(self, impltype, **kw):
 #chema = kw.pop("schema", self.schema)
 #etadata = kw.pop("metadata", self.metadata)
 #create_events = kw.pop("_create_events", False)
 #eturn impltype(
 #ame=self.name,
 #chema=schema,
 #nherit_schema=self.inherit_schema,
 #etadata=metadata,
 #create_events=_create_events,
 #*kw
 #

 #property
 #ef bind(self):
 #eturn self.metadata and self.metadata.bind or None

 #ef create(self, bind=None, checkfirst=False):
 #""Issue CREATE DDL for this type, if applicable."""

 #f bind is None:
 #ind = _bind_or_error(self)
 # = self.dialect_impl(bind.dialect)
 #f t.__class__ is not self.__class__ and isinstance(t, SchemaType):
 #.create(bind=bind, checkfirst=checkfirst)

 #ef drop(self, bind=None, checkfirst=False):
 #""Issue DROP DDL for this type, if applicable."""

 #f bind is None:
 #ind = _bind_or_error(self)
 # = self.dialect_impl(bind.dialect)
 #f t.__class__ is not self.__class__ and isinstance(t, SchemaType):
 #.drop(bind=bind, checkfirst=checkfirst)

 #ef _on_table_create(self, target, bind, **kw):
 #f not self._is_impl_for_variant(bind.dialect, kw):
 #eturn

 # = self.dialect_impl(bind.dialect)
 #f t.__class__ is not self.__class__ and isinstance(t, SchemaType):
 #._on_table_create(target, bind, **kw)

 #ef _on_table_drop(self, target, bind, **kw):
 #f not self._is_impl_for_variant(bind.dialect, kw):
 #eturn

 # = self.dialect_impl(bind.dialect)
 #f t.__class__ is not self.__class__ and isinstance(t, SchemaType):
 #._on_table_drop(target, bind, **kw)

 #ef _on_metadata_create(self, target, bind, **kw):
 #f not self._is_impl_for_variant(bind.dialect, kw):
 #eturn

 # = self.dialect_impl(bind.dialect)
 #f t.__class__ is not self.__class__ and isinstance(t, SchemaType):
 #._on_metadata_create(target, bind, **kw)

 #ef _on_metadata_drop(self, target, bind, **kw):
 #f not self._is_impl_for_variant(bind.dialect, kw):
 #eturn

 # = self.dialect_impl(bind.dialect)
 #f t.__class__ is not self.__class__ and isinstance(t, SchemaType):
 #._on_metadata_drop(target, bind, **kw)

 #ef _is_impl_for_variant(self, dialect, kw):
 #ariant_mapping = kw.pop("variant_mapping", None)
 #f variant_mapping is None:
 #eturn True

        # since PostgreSQL is the only DB that has ARRAY this can only
        # be integration tested by PG-specific tests
 #ef _we_are_the_impl(typ):
 #eturn (
 #yp is self or isinstance(typ, ARRAY) and typ.item_type is self
 #

 #f dialect.name in variant_mapping and _we_are_the_impl(
 #ariant_mapping[dialect.name]
 #:
 #eturn True
 #lif dialect.name not in variant_mapping:
 #eturn _we_are_the_impl(variant_mapping["_default"])


class Enum(Emulated, String, SchemaType):
 #""Generic Enum Type.

 #he :class:`.Enum` type provides a set of possible string values
 #hich the column is constrained towards.

 #he :class:`.Enum` type will make use of the backend's native "ENUM"
 #ype if one is available; otherwise, it uses a VARCHAR datatype.
 #n option also exists to automatically produce a CHECK constraint
 #hen the VARCHAR (so called "non-native") variant is produced;
 #ee the  :paramref:`.Enum.create_constraint` flag.

 #he :class:`.Enum` type also provides in-Python validation of string
 #alues during both read and write operations.  When reading a value
 #rom the database in a result set, the string value is always checked
 #gainst the list of possible values and a ``LookupError`` is raised
 #f no match is found.  When passing a value to the database as a
 #lain string within a SQL statement, if the
 #paramref:`.Enum.validate_strings` parameter is
 #et to True, a ``LookupError`` is raised for any string value that's
 #ot located in the given list of possible values; note that this
 #mpacts usage of LIKE expressions with enumerated values (an unusual
 #se case).

 #. versionchanged:: 1.1 the :class:`.Enum` type now provides in-Python
 #alidation of input values as well as on data being returned by
 #he database.

 #he source of enumerated values may be a list of string values, or
 #lternatively a PEP-435-compliant enumerated class.  For the purposes
 #f the :class:`.Enum` datatype, this class need only provide a
 #`__members__`` method.

 #hen using an enumerated class, the enumerated objects are used
 #oth for input and output, rather than strings as is the case with
 # plain-string enumerated type::

 #mport enum
 #lass MyEnum(enum.Enum):
 #ne = 1
 #wo = 2
 #hree = 3

 # = Table(
 #data', MetaData(),
 #olumn('value', Enum(MyEnum))
 #

 #onnection.execute(t.insert(), {"value": MyEnum.two})
 #ssert connection.scalar(t.select()) is MyEnum.two

 #bove, the string names of each element, e.g. "one", "two", "three",
 #re persisted to the database; the values of the Python Enum, here
 #ndicated as integers, are **not** used; the value of each enum can
 #herefore be any kind of Python object whether or not it is persistable.

 #n order to persist the values and not the names, the
 #paramref:`.Enum.values_callable` parameter may be used.   The value of
 #his parameter is a user-supplied callable, which  is intended to be used
 #ith a PEP-435-compliant enumerated class and  returns a list of string
 #alues to be persisted.   For a simple enumeration that uses string values,
 # callable such as  ``lambda x: [e.value for e in x]`` is sufficient.

 #. versionadded:: 1.1 - support for PEP-435-style enumerated
 #lasses.


 #. seealso::

 #class:`_postgresql.ENUM` - PostgreSQL-specific type,
 #hich has additional functionality.

 #class:`.mysql.ENUM` - MySQL-specific type

 #""

 #_visit_name__ = "enum"

 #util.deprecated_params(
 #onvert_unicode=(
 #1.3",
 #The :paramref:`.Enum.convert_unicode` parameter is deprecated "
 #and will be removed in a future release.  All modern DBAPIs "
 #now support Python Unicode directly and this parameter is "
 #unnecessary.",
 #
 #
 #ef __init__(self, *enums, **kw):
 #"""Construct an enum.

 #eyword arguments which don't apply to a specific backend are ignored
 #y that backend.

 #param \*enums: either exactly one PEP-435 compliant enumerated type
 #r one or more string labels.

 #. versionadded:: 1.1 a PEP-435 style enumerated class may be
 #assed.

 #param convert_unicode: Enable unicode-aware bind parameter and
 #esult-set processing for this Enum's data under Python 2 only.
 #nder Python 2, this is set automatically based on the presence of
 #nicode label strings.  This flag will be removed in SQLAlchemy 2.0.

 #param create_constraint: defaults to False.  When creating a
 #on-native enumerated type, also build a CHECK constraint on the
 #atabase against the valid values.

 #. note:: it is strongly recommended that the CHECK constraint
 #ave an explicit name in order to support schema-management
 #oncerns.  This can be established either by setting the
 #paramref:`.Enum.name` parameter or by setting up an
 #ppropriate naming convention; see
 #ref:`constraint_naming_conventions` for background.

 #. versionchanged:: 1.4 - this flag now defaults to False, meaning
 #o CHECK constraint is generated for a non-native enumerated
 #ype.

 #param metadata: Associate this type directly with a ``MetaData``
 #bject. For types that exist on the target database as an
 #ndependent schema construct (PostgreSQL), this type will be
 #reated and dropped within ``create_all()`` and ``drop_all()``
 #perations. If the type is not associated with any ``MetaData``
 #bject, it will associate itself with each ``Table`` in which it is
 #sed, and will be created when any of those individual tables are
 #reated, after a check is performed for its existence. The type is
 #nly dropped when ``drop_all()`` is called for that ``Table``
 #bject's metadata, however.

 #he value of the :paramref:`_schema.MetaData.schema` parameter of
 #he :class:`_schema.MetaData` object, if set, will be used as the
 #efault value of the :paramref:`_types.Enum.schema` on this object
 #f an explicit value is not otherwise supplied.

 #. versionchanged:: 1.4.12 :class:`_types.Enum` inherits the
 #paramref:`_schema.MetaData.schema` parameter of the
 #class:`_schema.MetaData` object if present, when passed using
 #he :paramref:`_types.Enum.metadata` parameter.

 #param name: The name of this type. This is required for PostgreSQL
 #nd any future supported database which requires an explicitly
 #amed type, or an explicitly named constraint in order to generate
 #he type and/or a table that uses it. If a PEP-435 enumerated
 #lass was used, its name (converted to lower case) is used by
 #efault.

 #param native_enum: Use the database's native ENUM type when
 #vailable. Defaults to True. When False, uses VARCHAR + check
 #onstraint for all backends. The VARCHAR length can be controlled
 #ith :paramref:`.Enum.length`

 #param length: Allows specifying a custom length for the VARCHAR
 #hen :paramref:`.Enum.native_enum` is False. By default it uses the
 #ength of the longest value.

 #. versionadded:: 1.3.16

 #param schema: Schema name of this type. For types that exist on the
 #arget database as an independent schema construct (PostgreSQL),
 #his parameter specifies the named schema in which the type is
 #resent.

 #f not present, the schema name will be taken from the
 #class:`_schema.MetaData` collection if passed as
 #paramref:`_types.Enum.metadata`, for a :class:`_schema.MetaData`
 #hat includes the :paramref:`_schema.MetaData.schema` parameter.

 #. versionchanged:: 1.4.12 :class:`_types.Enum` inherits the
 #paramref:`_schema.MetaData.schema` parameter of the
 #class:`_schema.MetaData` object if present, when passed using
 #he :paramref:`_types.Enum.metadata` parameter.

 #therwise, if the :paramref:`_types.Enum.inherit_schema` flag is set
 #o ``True``, the schema will be inherited from the associated
 #class:`_schema.Table` object if any; when
 #paramref:`_types.Enum.inherit_schema` is at its default of
 #`False``, the owning table's schema is **not** used.


 #param quote: Set explicit quoting preferences for the type's name.

 #param inherit_schema: When ``True``, the "schema" from the owning
 #class:`_schema.Table`
 #ill be copied to the "schema" attribute of this
 #class:`.Enum`, replacing whatever value was passed for the
 #`schema`` attribute.   This also takes effect when using the
 #meth:`_schema.Table.to_metadata` operation.

 #param validate_strings: when True, string values that are being
 #assed to the database in a SQL statement will be checked
 #or validity against the list of enumerated values.  Unrecognized
 #alues will result in a ``LookupError`` being raised.

 #. versionadded:: 1.1.0b2

 #param values_callable: A callable which will be passed the PEP-435
 #ompliant enumerated type, which should then return a list of string
 #alues to be persisted. This allows for alternate usages such as
 #sing the string value of an enum to be persisted to the database
 #nstead of its name.

 #. versionadded:: 1.2.3

 #param sort_key_function: a Python callable which may be used as the
 #key" argument in the Python ``sorted()`` built-in.   The SQLAlchemy
 #RM requires that primary key columns which are mapped must
 #e sortable in some way.  When using an unsortable enumeration
 #bject such as a Python 3 ``Enum`` object, this parameter may be
 #sed to set a default sort key function for the objects.  By
 #efault, the database value of the enumeration is used as the
 #orting function.

 #. versionadded:: 1.3.8

 #param omit_aliases: A boolean that when true will remove aliases from
 #ep 435 enums. For backward compatibility it defaults to ``False``.
 # deprecation warning is raised if the enum has aliases and this
 #lag was not set.

 #. versionadded:: 1.4.5

 #. deprecated:: 1.4  The default will be changed to ``True`` in
 #QLAlchemy 2.0.

 #""
 #elf._enum_init(enums, kw)

 #property
 #ef _enums_argument(self):
 #f self.enum_class is not None:
 #eturn [self.enum_class]
 #lse:
 #eturn self.enums

 #ef _enum_init(self, enums, kw):
 #""internal init for :class:`.Enum` and subclasses.

 #riendly init helper used by subclasses to remove
 #ll the Enum-specific keyword arguments from kw.  Allows all
 #ther arguments in kw to pass through.

 #""
 #elf.native_enum = kw.pop("native_enum", True)
 #elf.create_constraint = kw.pop("create_constraint", False)
 #elf.values_callable = kw.pop("values_callable", None)
 #elf._sort_key_function = kw.pop("sort_key_function", NO_ARG)
 #ength_arg = kw.pop("length", NO_ARG)
 #elf._omit_aliases = kw.pop("omit_aliases", NO_ARG)

 #alues, objects = self._parse_into_values(enums, kw)
 #elf._setup_for_values(values, objects, kw)

 #onvert_unicode = kw.pop("convert_unicode", None)
 #elf.validate_strings = kw.pop("validate_strings", False)

 #f convert_unicode is None:
 #or e in self.enums:
                # this is all py2k logic that can go away for py3k only,
                # "expect unicode" will always be implicitly true
 #f isinstance(e, util.text_type):
 #expect_unicode = True
 #reak
 #lse:
 #expect_unicode = False
 #lse:
 #expect_unicode = convert_unicode

 #f self.enums:
 #ength = max(len(x) for x in self.enums)
 #lse:
 #ength = 0
 #f not self.native_enum and length_arg is not NO_ARG:
 #f length_arg < length:
 #aise ValueError(
 #When provided, length must be larger or equal"
 # than the length of the longest enum value. %s < %s"
 # (length_arg, length)
 #
 #ength = length_arg

 #elf._valid_lookup[None] = self._object_lookup[None] = None

 #uper(Enum, self).__init__(
 #ength=length, _expect_unicode=_expect_unicode
 #

 #f self.enum_class:
 #w.setdefault("name", self.enum_class.__name__.lower())
 #chemaType.__init__(
 #elf,
 #ame=kw.pop("name", None),
 #chema=kw.pop("schema", None),
 #etadata=kw.pop("metadata", None),
 #nherit_schema=kw.pop("inherit_schema", False),
 #uote=kw.pop("quote", None),
 #create_events=kw.pop("_create_events", True),
 #

 #ef _parse_into_values(self, enums, kw):
 #f not enums and "_enums" in kw:
 #nums = kw.pop("_enums")

 #f len(enums) == 1 and hasattr(enums[0], "__members__"):
 #elf.enum_class = enums[0]

 #members = self.enum_class.__members__

 #liases = [n for n, v in _members.items() if v.name != n]
 #f self._omit_aliases is NO_ARG and aliases:
 #til.warn_deprecated_20(
 #The provided enum %s contains the aliases %s. The "
 #``omit_aliases`` will default to ``True`` in SQLAlchemy "
 #2.0. Specify a value to silence this warning."
 # (self.enum_class.__name__, aliases)
 #
 #f self._omit_aliases is True:
                # remove aliases
 #embers = OrderedDict(
 #n, v) for n, v in _members.items() if v.name == n
 #
 #lse:
 #embers = _members
 #f self.values_callable:
 #alues = self.values_callable(self.enum_class)
 #lse:
 #alues = list(members)
 #bjects = [members[k] for k in members]
 #eturn values, objects
 #lse:
 #elf.enum_class = None
 #eturn enums, enums

 #ef _setup_for_values(self, values, objects, kw):
 #elf.enums = list(values)

 #elf._valid_lookup = dict(zip(reversed(objects), reversed(values)))

 #elf._object_lookup = dict(zip(values, objects))

 #elf._valid_lookup.update(
 #
 #value, self._valid_lookup[self._object_lookup[value]])
 #or value in values
 #
 #

 #property
 #ef sort_key_function(self):
 #f self._sort_key_function is NO_ARG:
 #eturn self._db_value_for_elem
 #lse:
 #eturn self._sort_key_function

 #property
 #ef native(self):
 #eturn self.native_enum

 #ef _db_value_for_elem(self, elem):
 #ry:
 #eturn self._valid_lookup[elem]
 #xcept KeyError as err:
            # for unknown string values, we return as is.  While we can
            # validate these if we wanted, that does not allow for lesser-used
            # end-user use cases, such as using a LIKE comparison with an enum,
            # or for an application that wishes to apply string tests to an
            # ENUM (see [ticket:3725]).  While we can decide to differentiate
            # here between an INSERT statement and a criteria used in a SELECT,
            # for now we're staying conservative w/ behavioral changes (perhaps
            # someone has a trigger that handles strings on INSERT)
 #f not self.validate_strings and isinstance(
 #lem, compat.string_types
 #:
 #eturn elem
 #lse:
 #til.raise_(
 #ookupError(
 #'%s' is not among the defined enum values. "
 #Enum name: %s. Possible values: %s"
 # (
 #lem,
 #elf.name,
 #anghelpers.repr_tuple_names(self.enums),
 #
 #,
 #eplace_context=err,
 #

 #lass Comparator(String.Comparator):
 #ef _adapt_expression(self, op, other_comparator):
 #p, typ = super(Enum.Comparator, self)._adapt_expression(
 #p, other_comparator
 #
 #f op is operators.concat_op:
 #yp = String(
 #elf.type.length, _expect_unicode=self.type._expect_unicode
 #
 #eturn op, typ

 #omparator_factory = Comparator

 #ef _object_value_for_elem(self, elem):
 #ry:
 #eturn self._object_lookup[elem]
 #xcept KeyError as err:
 #til.raise_(
 #ookupError(
 #'%s' is not among the defined enum values. "
 #Enum name: %s. Possible values: %s"
 # (
 #lem,
 #elf.name,
 #anghelpers.repr_tuple_names(self.enums),
 #
 #,
 #eplace_context=err,
 #

 #ef __repr__(self):
 #eturn util.generic_repr(
 #elf,
 #dditional_kw=[("native_enum", True)],
 #o_inspect=[Enum, SchemaType],
 #

 #ef as_generic(self, allow_nulltype=False):
 #f hasattr(self, "enums"):
 #rgs = self.enums
 #lse:
 #aise NotImplementedError(
 #TypeEngine.as_generic() heuristic "
 #is undefined for types that inherit Enum but do not have "
 #an `enums` attribute."
 #

 #eturn util.constructor_copy(self, self._generic_type_affinity, *args)

 #ef adapt_to_emulated(self, impltype, **kw):
 #w.setdefault("_expect_unicode", self._expect_unicode)
 #w.setdefault("validate_strings", self.validate_strings)
 #w.setdefault("name", self.name)
 #w.setdefault("schema", self.schema)
 #w.setdefault("inherit_schema", self.inherit_schema)
 #w.setdefault("metadata", self.metadata)
 #w.setdefault("_create_events", False)
 #w.setdefault("native_enum", self.native_enum)
 #w.setdefault("values_callable", self.values_callable)
 #w.setdefault("create_constraint", self.create_constraint)
 #w.setdefault("length", self.length)
 #w.setdefault("omit_aliases", self._omit_aliases)
 #ssert "_enums" in kw
 #eturn impltype(**kw)

 #ef adapt(self, impltype, **kw):
 #w["_enums"] = self._enums_argument
 #eturn super(Enum, self).adapt(impltype, **kw)

 #ef _should_create_constraint(self, compiler, **kw):
 #f not self._is_impl_for_variant(compiler.dialect, kw):
 #eturn False
 #eturn (
 #ot self.native_enum or not compiler.dialect.supports_native_enum
 #

 #util.preload_module("sqlalchemy.sql.schema")
 #ef _set_table(self, column, table):
 #chema = util.preloaded.sql_schema
 #chemaType._set_table(self, column, table)

 #f not self.create_constraint:
 #eturn

 #ariant_mapping = self._variant_mapping_for_set_table(column)

 # = schema.CheckConstraint(
 #ype_coerce(column, self).in_(self.enums),
 #ame=_NONE_NAME if self.name is None else self.name,
 #create_rule=util.portable_instancemethod(
 #elf._should_create_constraint,
 #"variant_mapping": variant_mapping},
 #,
 #type_bound=True,
 #
 #ssert e.table is table

 #ef literal_processor(self, dialect):
 #arent_processor = super(Enum, self).literal_processor(dialect)

 #ef process(value):
 #alue = self._db_value_for_elem(value)
 #f parent_processor:
 #alue = parent_processor(value)
 #eturn value

 #eturn process

 #ef bind_processor(self, dialect):
 #ef process(value):
 #alue = self._db_value_for_elem(value)
 #f parent_processor:
 #alue = parent_processor(value)
 #eturn value

 #arent_processor = super(Enum, self).bind_processor(dialect)
 #eturn process

 #ef result_processor(self, dialect, coltype):
 #arent_processor = super(Enum, self).result_processor(dialect, coltype)

 #ef process(value):
 #f parent_processor:
 #alue = parent_processor(value)

 #alue = self._object_value_for_elem(value)
 #eturn value

 #eturn process

 #ef copy(self, **kw):
 #eturn SchemaType.copy(self, **kw)

 #property
 #ef python_type(self):
 #f self.enum_class:
 #eturn self.enum_class
 #lse:
 #eturn super(Enum, self).python_type


class PickleType(TypeDecorator):
 #""Holds Python objects, which are serialized using pickle.

 #ickleType builds upon the Binary type to apply Python's
 #`pickle.dumps()`` to incoming objects, and ``pickle.loads()`` on
 #he way out, allowing any pickleable Python object to be stored as
 # serialized binary field.

 #o allow ORM change events to propagate for elements associated
 #ith :class:`.PickleType`, see :ref:`mutable_toplevel`.

 #""

 #mpl = LargeBinary
 #ache_ok = True

 #ef __init__(
 #elf,
 #rotocol=pickle.HIGHEST_PROTOCOL,
 #ickler=None,
 #omparator=None,
 #mpl=None,
 #:
 #""
 #onstruct a PickleType.

 #param protocol: defaults to ``pickle.HIGHEST_PROTOCOL``.

 #param pickler: defaults to cPickle.pickle or pickle.pickle if
 #Pickle is not available.  May be any object with
 #ickle-compatible ``dumps`` and ``loads`` methods.

 #param comparator: a 2-arg callable predicate used
 #o compare values of this type.  If left as ``None``,
 #he Python "equals" operator is used to compare values.

 #param impl: A binary-storing :class:`_types.TypeEngine` class or
 #nstance to use in place of the default :class:`_types.LargeBinary`.
 #or example the :class: `_mysql.LONGBLOB` class may be more effective
 #hen using MySQL.

 #. versionadded:: 1.4.20

 #""
 #elf.protocol = protocol
 #elf.pickler = pickler or pickle
 #elf.comparator = comparator
 #uper(PickleType, self).__init__()

 #f impl:
 #elf.impl = to_instance(impl)

 #ef __reduce__(self):
 #eturn PickleType, (self.protocol, None, self.comparator)

 #ef bind_processor(self, dialect):
 #mpl_processor = self.impl.bind_processor(dialect)
 #umps = self.pickler.dumps
 #rotocol = self.protocol
 #f impl_processor:

 #ef process(value):
 #f value is not None:
 #alue = dumps(value, protocol)
 #eturn impl_processor(value)

 #lse:

 #ef process(value):
 #f value is not None:
 #alue = dumps(value, protocol)
 #eturn value

 #eturn process

 #ef result_processor(self, dialect, coltype):
 #mpl_processor = self.impl.result_processor(dialect, coltype)
 #oads = self.pickler.loads
 #f impl_processor:

 #ef process(value):
 #alue = impl_processor(value)
 #f value is None:
 #eturn None
 #eturn loads(value)

 #lse:

 #ef process(value):
 #f value is None:
 #eturn None
 #eturn loads(value)

 #eturn process

 #ef compare_values(self, x, y):
 #f self.comparator:
 #eturn self.comparator(x, y)
 #lse:
 #eturn x == y


class Boolean(Emulated, TypeEngine, SchemaType):

 #""A bool datatype.

 #class:`.Boolean` typically uses BOOLEAN or SMALLINT on the DDL side,
 #nd on the Python side deals in ``True`` or ``False``.

 #he :class:`.Boolean` datatype currently has two levels of assertion
 #hat the values persisted are simple true/false values.  For all
 #ackends, only the Python values ``None``, ``True``, ``False``, ``1``
 #r ``0`` are accepted as parameter values.   For those backends that
 #on't support a "native boolean" datatype, an option exists to
 #lso create a CHECK constraint on the target column

 #. versionchanged:: 1.2 the :class:`.Boolean` datatype now asserts that
 #ncoming Python values are already in pure boolean form.


 #""

 #_visit_name__ = "boolean"
 #ative = True

 #ef __init__(
 #elf, create_constraint=False, name=None, _create_events=True
 #:
 #""Construct a Boolean.

 #param create_constraint: defaults to False.  If the boolean
 #s generated as an int/smallint, also create a CHECK constraint
 #n the table that ensures 1 or 0 as a value.

 #. note:: it is strongly recommended that the CHECK constraint
 #ave an explicit name in order to support schema-management
 #oncerns.  This can be established either by setting the
 #paramref:`.Boolean.name` parameter or by setting up an
 #ppropriate naming convention; see
 #ref:`constraint_naming_conventions` for background.

 #. versionchanged:: 1.4 - this flag now defaults to False, meaning
 #o CHECK constraint is generated for a non-native enumerated
 #ype.

 #param name: if a CHECK constraint is generated, specify
 #he name of the constraint.

 #""
 #elf.create_constraint = create_constraint
 #elf.name = name
 #elf._create_events = _create_events

 #ef _should_create_constraint(self, compiler, **kw):
 #f not self._is_impl_for_variant(compiler.dialect, kw):
 #eturn False
 #eturn (
 #ot compiler.dialect.supports_native_boolean
 #nd compiler.dialect.non_native_boolean_check_constraint
 #

 #util.preload_module("sqlalchemy.sql.schema")
 #ef _set_table(self, column, table):
 #chema = util.preloaded.sql_schema
 #f not self.create_constraint:
 #eturn

 #ariant_mapping = self._variant_mapping_for_set_table(column)

 # = schema.CheckConstraint(
 #ype_coerce(column, self).in_([0, 1]),
 #ame=_NONE_NAME if self.name is None else self.name,
 #create_rule=util.portable_instancemethod(
 #elf._should_create_constraint,
 #"variant_mapping": variant_mapping},
 #,
 #type_bound=True,
 #
 #ssert e.table is table

 #property
 #ef python_type(self):
 #eturn bool

 #strict_bools = frozenset([None, True, False])

 #ef _strict_as_bool(self, value):
 #f value not in self._strict_bools:
 #f not isinstance(value, int):
 #aise TypeError("Not a boolean value: %r" % value)
 #lse:
 #aise ValueError(
 #Value %r is not None, True, or False" % value
 #
 #eturn value

 #ef literal_processor(self, dialect):
 #ompiler = dialect.statement_compiler(dialect, None)
 #rue = compiler.visit_true(None)
 #alse = compiler.visit_false(None)

 #ef process(value):
 #eturn true if self._strict_as_bool(value) else false

 #eturn process

 #ef bind_processor(self, dialect):
 #strict_as_bool = self._strict_as_bool
 #f dialect.supports_native_boolean:
 #coerce = bool
 #lse:
 #coerce = int

 #ef process(value):
 #alue = _strict_as_bool(value)
 #f value is not None:
 #alue = _coerce(value)
 #eturn value

 #eturn process

 #ef result_processor(self, dialect, coltype):
 #f dialect.supports_native_boolean:
 #eturn None
 #lse:
 #eturn processors.int_to_boolean


class _AbstractInterval(_LookupExpressionAdapter, TypeEngine):
 #util.memoized_property
 #ef _expression_adaptations(self):
        # Based on https://www.postgresql.org/docs/current/\
        # static/functions-datetime.html.

 #eturn {
 #perators.add: {
 #ate: DateTime,
 #nterval: self.__class__,
 #ateTime: DateTime,
 #ime: Time,
 #,
 #perators.sub: {Interval: self.__class__},
 #perators.mul: {Numeric: self.__class__},
 #perators.truediv: {Numeric: self.__class__},
 #perators.div: {Numeric: self.__class__},
 #

 #property
 #ef _type_affinity(self):
 #eturn Interval

 #ef coerce_compared_value(self, op, value):
 #""See :meth:`.TypeEngine.coerce_compared_value` for a description."""
 #eturn self.impl.coerce_compared_value(op, value)


class Interval(Emulated, _AbstractInterval, TypeDecorator):

 #""A type for ``datetime.timedelta()`` objects.

 #he Interval type deals with ``datetime.timedelta`` objects.  In
 #ostgreSQL, the native ``INTERVAL`` type is used; for others, the
 #alue is stored as a date which is relative to the "epoch"
 #Jan. 1, 1970).

 #ote that the ``Interval`` type does not currently provide date arithmetic
 #perations on platforms which do not support interval types natively. Such
 #perations usually require transformation of both sides of the expression
 #such as, conversion of both sides into integer epoch values first) which
 #urrently is a manual procedure (such as via
 #attr:`~sqlalchemy.sql.expression.func`).

 #""

 #mpl = DateTime
 #poch = dt.datetime.utcfromtimestamp(0)
 #ache_ok = True

 #ef __init__(self, native=True, second_precision=None, day_precision=None):
 #""Construct an Interval object.

 #param native: when True, use the actual
 #NTERVAL type provided by the database, if
 #upported (currently PostgreSQL, Oracle).
 #therwise, represent the interval data as
 #n epoch value regardless.

 #param second_precision: For native interval types
 #hich support a "fractional seconds precision" parameter,
 #.e. Oracle and PostgreSQL

 #param day_precision: for native interval types which
 #upport a "day precision" parameter, i.e. Oracle.

 #""
 #uper(Interval, self).__init__()
 #elf.native = native
 #elf.second_precision = second_precision
 #elf.day_precision = day_precision

 #property
 #ef python_type(self):
 #eturn dt.timedelta

 #ef adapt_to_emulated(self, impltype, **kw):
 #eturn _AbstractInterval.adapt(self, impltype, **kw)

 #ef bind_processor(self, dialect):
 #mpl_processor = self.impl.bind_processor(dialect)
 #poch = self.epoch
 #f impl_processor:

 #ef process(value):
 #f value is not None:
 #alue = epoch + value
 #eturn impl_processor(value)

 #lse:

 #ef process(value):
 #f value is not None:
 #alue = epoch + value
 #eturn value

 #eturn process

 #ef result_processor(self, dialect, coltype):
 #mpl_processor = self.impl.result_processor(dialect, coltype)
 #poch = self.epoch
 #f impl_processor:

 #ef process(value):
 #alue = impl_processor(value)
 #f value is None:
 #eturn None
 #eturn value - epoch

 #lse:

 #ef process(value):
 #f value is None:
 #eturn None
 #eturn value - epoch

 #eturn process


class JSON(Indexable, TypeEngine):
 #""Represent a SQL JSON type.

 #. note::  :class:`_types.JSON`
 #s provided as a facade for vendor-specific
 #SON types.  Since it supports JSON SQL operations, it only
 #orks on backends that have an actual JSON type, currently:

 # PostgreSQL - see :class:`sqlalchemy.dialects.postgresql.JSON` and
 #class:`sqlalchemy.dialects.postgresql.JSONB` for backend-specific
 #otes

 # MySQL - see
 #class:`sqlalchemy.dialects.mysql.JSON` for backend-specific notes

 # SQLite as of version 3.9 - see
 #class:`sqlalchemy.dialects.sqlite.JSON` for backend-specific notes

 # Microsoft SQL Server 2016 and later - see
 #class:`sqlalchemy.dialects.mssql.JSON` for backend-specific notes

 #class:`_types.JSON` is part of the Core in support of the growing
 #opularity of native JSON datatypes.

 #he :class:`_types.JSON` type stores arbitrary JSON format data, e.g.::

 #ata_table = Table('data_table', metadata,
 #olumn('id', Integer, primary_key=True),
 #olumn('data', JSON)
 #

 #ith engine.connect() as conn:
 #onn.execute(
 #ata_table.insert(),
 #ata = {"key1": "value1", "key2": "value2"}
 #

 #*JSON-Specific Expression Operators**

 #he :class:`_types.JSON`
 #atatype provides these additional SQL operations:

 # Keyed index operations::

 #ata_table.c.data['some key']

 # Integer index operations::

 #ata_table.c.data[3]

 # Path index operations::

 #ata_table.c.data[('key_1', 'key_2', 5, ..., 'key_n')]

 # Data casters for specific JSON element types, subsequent to an index
 #r path operation being invoked::

 #ata_table.c.data["some key"].as_integer()

 #. versionadded:: 1.3.11

 #dditional operations may be available from the dialect-specific versions
 #f :class:`_types.JSON`, such as
 #class:`sqlalchemy.dialects.postgresql.JSON` and
 #class:`sqlalchemy.dialects.postgresql.JSONB` which both offer additional
 #ostgreSQL-specific operations.

 #*Casting JSON Elements to Other Types**

 #ndex operations, i.e. those invoked by calling upon the expression using
 #he Python bracket operator as in ``some_column['some key']``, return an
 #xpression object whose type defaults to :class:`_types.JSON` by default,
 #o that
 #urther JSON-oriented instructions may be called upon the result type.
 #owever, it is likely more common that an index operation is expected
 #o return a specific scalar element, such as a string or integer.  In
 #rder to provide access to these elements in a backend-agnostic way,
 # series of data casters are provided:

 # :meth:`.JSON.Comparator.as_string` - return the element as a string

 # :meth:`.JSON.Comparator.as_boolean` - return the element as a boolean

 # :meth:`.JSON.Comparator.as_float` - return the element as a float

 # :meth:`.JSON.Comparator.as_integer` - return the element as an integer

 #hese data casters are implemented by supporting dialects in order to
 #ssure that comparisons to the above types will work as expected, such as::

        # integer comparison
 #ata_table.c.data["some_integer_key"].as_integer() == 5

        # boolean comparison
 #ata_table.c.data["some_boolean"].as_boolean() == True

 #. versionadded:: 1.3.11 Added type-specific casters for the basic JSON
 #ata element types.

 #. note::

 #he data caster functions are new in version 1.3.11, and supersede
 #he previous documented approaches of using CAST; for reference,
 #his looked like::

 #rom sqlalchemy import cast, type_coerce
 #rom sqlalchemy import String, JSON
 #ast(
 #ata_table.c.data['some_key'], String
 # == type_coerce(55, JSON)

 #he above case now works directly as::

 #ata_table.c.data['some_key'].as_integer() == 5

 #or details on the previous comparison approach within the 1.3.x
 #eries, see the documentation for SQLAlchemy 1.2 or the included HTML
 #iles in the doc/ directory of the version's distribution.

 #*Detecting Changes in JSON columns when using the ORM**

 #he :class:`_types.JSON` type, when used with the SQLAlchemy ORM, does not
 #etect in-place mutations to the structure.  In order to detect these, the
 #mod:`sqlalchemy.ext.mutable` extension must be used.  This extension will
 #llow "in-place" changes to the datastructure to produce events which
 #ill be detected by the unit of work.  See the example at :class:`.HSTORE`
 #or a simple example involving a dictionary.

 #*Support for JSON null vs. SQL NULL**

 #hen working with NULL values, the :class:`_types.JSON`
 #ype recommends the
 #se of two specific constants in order to differentiate between a column
 #hat evaluates to SQL NULL, e.g. no value, vs. the JSON-encoded string
 #f ``"null"``.   To insert or select against a value that is SQL NULL,
 #se the constant :func:`.null`::

 #rom sqlalchemy import null
 #onn.execute(table.insert(), json_value=null())

 #o insert or select against a value that is JSON ``"null"``, use the
 #onstant :attr:`_types.JSON.NULL`::

 #onn.execute(table.insert(), json_value=JSON.NULL)

 #he :class:`_types.JSON` type supports a flag
 #paramref:`_types.JSON.none_as_null` which when set to True will result
 #n the Python constant ``None`` evaluating to the value of SQL
 #ULL, and when set to False results in the Python constant
 #`None`` evaluating to the value of JSON ``"null"``.    The Python
 #alue ``None`` may be used in conjunction with either
 #attr:`_types.JSON.NULL` and :func:`.null` in order to indicate NULL
 #alues, but care must be taken as to the value of the
 #paramref:`_types.JSON.none_as_null` in these cases.

 #*Customizing the JSON Serializer**

 #he JSON serializer and deserializer used by :class:`_types.JSON`
 #efaults to
 #ython's ``json.dumps`` and ``json.loads`` functions; in the case of the
 #sycopg2 dialect, psycopg2 may be using its own custom loader function.

 #n order to affect the serializer / deserializer, they are currently
 #onfigurable at the :func:`_sa.create_engine` level via the
 #paramref:`_sa.create_engine.json_serializer` and
 #paramref:`_sa.create_engine.json_deserializer` parameters.  For example,
 #o turn off ``ensure_ascii``::

 #ngine = create_engine(
 #sqlite://",
 #son_serializer=lambda obj: json.dumps(obj, ensure_ascii=False))

 #. versionchanged:: 1.3.7

 #QLite dialect's ``json_serializer`` and ``json_deserializer``
 #arameters renamed from ``_json_serializer`` and
 #`_json_deserializer``.

 #. seealso::

 #class:`sqlalchemy.dialects.postgresql.JSON`

 #class:`sqlalchemy.dialects.postgresql.JSONB`

 #class:`sqlalchemy.dialects.mysql.JSON`

 #class:`sqlalchemy.dialects.sqlite.JSON`

 #. versionadded:: 1.1


 #""

 #_visit_name__ = "JSON"

 #ashable = False
 #ULL = util.symbol("JSON_NULL")
 #""Describe the json value of NULL.

 #his value is used to force the JSON value of ``"null"`` to be
 #sed as the value.   A value of Python ``None`` will be recognized
 #ither as SQL NULL or JSON ``"null"``, based on the setting
 #f the :paramref:`_types.JSON.none_as_null` flag; the
 #attr:`_types.JSON.NULL`
 #onstant can be used to always resolve to JSON ``"null"`` regardless
 #f this setting.  This is in contrast to the :func:`_expression.null`
 #onstruct,
 #hich always resolves to SQL NULL.  E.g.::

 #rom sqlalchemy import null
 #rom sqlalchemy.dialects.postgresql import JSON

        # will *always* insert SQL NULL
 #bj1 = MyObject(json_value=null())

        # will *always* insert JSON string "null"
 #bj2 = MyObject(json_value=JSON.NULL)

 #ession.add_all([obj1, obj2])
 #ession.commit()

 #n order to set JSON NULL as a default value for a column, the most
 #ransparent method is to use :func:`_expression.text`::

 #able(
 #my_table', metadata,
 #olumn('json_data', JSON, default=text("'null'"))
 #

 #hile it is possible to use :attr:`_types.JSON.NULL` in this context, the
 #attr:`_types.JSON.NULL` value will be returned as the value of the
 #olumn,
 #hich in the context of the ORM or other repurposing of the default
 #alue, may not be desirable.  Using a SQL expression means the value
 #ill be re-fetched from the database within the context of retrieving
 #enerated defaults.


 #""

 #ef __init__(self, none_as_null=False):
 #""Construct a :class:`_types.JSON` type.

 #param none_as_null=False: if True, persist the value ``None`` as a
 #QL NULL value, not the JSON encoding of ``null``.   Note that
 #hen this flag is False, the :func:`.null` construct can still
 #e used to persist a NULL value::

 #rom sqlalchemy import null
 #onn.execute(table.insert(), data=null())

 #. note::

 #paramref:`_types.JSON.none_as_null` does **not** apply to the
 #alues passed to :paramref:`_schema.Column.default` and
 #paramref:`_schema.Column.server_default`; a value of ``None``
 #assed for these parameters means "no default present".

 #dditionally, when used in SQL comparison expressions, the
 #ython value ``None`` continues to refer to SQL null, and not
 #SON NULL.  The :paramref:`_types.JSON.none_as_null` flag refers
 #xplicitly to the **persistence** of the value within an
 #NSERT or UPDATE statement.   The :attr:`_types.JSON.NULL`
 #alue should be used for SQL expressions that wish to compare to
 #SON null.

 #. seealso::

 #attr:`.types.JSON.NULL`

 #""
 #elf.none_as_null = none_as_null

 #lass JSONElementType(TypeEngine):
 #""Common function for index / path elements in a JSON expression."""

 #integer = Integer()
 #string = String()

 #ef string_bind_processor(self, dialect):
 #eturn self._string._cached_bind_processor(dialect)

 #ef string_literal_processor(self, dialect):
 #eturn self._string._cached_literal_processor(dialect)

 #ef bind_processor(self, dialect):
 #nt_processor = self._integer._cached_bind_processor(dialect)
 #tring_processor = self.string_bind_processor(dialect)

 #ef process(value):
 #f int_processor and isinstance(value, int):
 #alue = int_processor(value)
 #lif string_processor and isinstance(value, util.string_types):
 #alue = string_processor(value)
 #eturn value

 #eturn process

 #ef literal_processor(self, dialect):
 #nt_processor = self._integer._cached_literal_processor(dialect)
 #tring_processor = self.string_literal_processor(dialect)

 #ef process(value):
 #f int_processor and isinstance(value, int):
 #alue = int_processor(value)
 #lif string_processor and isinstance(value, util.string_types):
 #alue = string_processor(value)
 #eturn value

 #eturn process

 #lass JSONIndexType(JSONElementType):
 #""Placeholder for the datatype of a JSON index value.

 #his allows execution-time processing of JSON index values
 #or special syntaxes.

 #""

 #lass JSONIntIndexType(JSONIndexType):
 #""Placeholder for the datatype of a JSON index value.

 #his allows execution-time processing of JSON index values
 #or special syntaxes.

 #""

 #lass JSONStrIndexType(JSONIndexType):
 #""Placeholder for the datatype of a JSON index value.

 #his allows execution-time processing of JSON index values
 #or special syntaxes.

 #""

 #lass JSONPathType(JSONElementType):
 #""Placeholder type for JSON path operations.

 #his allows execution-time processing of a path-based
 #ndex value into a specific SQL syntax.

 #""

 #lass Comparator(Indexable.Comparator, Concatenable.Comparator):
 #""Define comparison operations for :class:`_types.JSON`."""

 #ef _setup_getitem(self, index):
 #f not isinstance(index, util.string_types) and isinstance(
 #ndex, compat.collections_abc.Sequence
 #:
 #ndex = coercions.expect(
 #oles.BinaryElementRole,
 #ndex,
 #xpr=self.expr,
 #perator=operators.json_path_getitem_op,
 #indparam_type=JSON.JSONPathType,
 #

 #perator = operators.json_path_getitem_op
 #lse:
 #ndex = coercions.expect(
 #oles.BinaryElementRole,
 #ndex,
 #xpr=self.expr,
 #perator=operators.json_getitem_op,
 #indparam_type=JSON.JSONIntIndexType
 #f isinstance(index, int)
 #lse JSON.JSONStrIndexType,
 #
 #perator = operators.json_getitem_op

 #eturn operator, index, self.type

 #ef as_boolean(self):
 #""Cast an indexed value as boolean.

 #.g.::

 #tmt = select(
 #ytable.c.json_column['some_data'].as_boolean()
 #.where(
 #ytable.c.json_column['some_data'].as_boolean() == True
 #

 #. versionadded:: 1.3.11

 #""
 #eturn self._binary_w_type(Boolean(), "as_boolean")

 #ef as_string(self):
 #""Cast an indexed value as string.

 #.g.::

 #tmt = select(
 #ytable.c.json_column['some_data'].as_string()
 #.where(
 #ytable.c.json_column['some_data'].as_string() ==
 #some string'
 #

 #. versionadded:: 1.3.11

 #""
 #eturn self._binary_w_type(String(), "as_string")

 #ef as_integer(self):
 #""Cast an indexed value as integer.

 #.g.::

 #tmt = select(
 #ytable.c.json_column['some_data'].as_integer()
 #.where(
 #ytable.c.json_column['some_data'].as_integer() == 5
 #

 #. versionadded:: 1.3.11

 #""
 #eturn self._binary_w_type(Integer(), "as_integer")

 #ef as_float(self):
 #""Cast an indexed value as float.

 #.g.::

 #tmt = select(
 #ytable.c.json_column['some_data'].as_float()
 #.where(
 #ytable.c.json_column['some_data'].as_float() == 29.75
 #

 #. versionadded:: 1.3.11

 #""
 #eturn self._binary_w_type(Float(), "as_float")

 #ef as_numeric(self, precision, scale, asdecimal=True):
 #""Cast an indexed value as numeric/decimal.

 #.g.::

 #tmt = select(
 #ytable.c.json_column['some_data'].as_numeric(10, 6)
 #.where(
 #ytable.c.
 #son_column['some_data'].as_numeric(10, 6) == 29.75
 #

 #. versionadded:: 1.4.0b2

 #""
 #eturn self._binary_w_type(
 #umeric(precision, scale, asdecimal=asdecimal), "as_numeric"
 #

 #ef as_json(self):
 #""Cast an indexed value as JSON.

 #.g.::

 #tmt = select(mytable.c.json_column['some_data'].as_json())

 #his is typically the default behavior of indexed elements in any
 #ase.

 #ote that comparison of full JSON structures may not be
 #upported by all backends.

 #. versionadded:: 1.3.11

 #""
 #eturn self.expr

 #ef _binary_w_type(self, typ, method_name):
 #f not isinstance(
 #elf.expr, elements.BinaryExpression
 # or self.expr.operator not in (
 #perators.json_getitem_op,
 #perators.json_path_getitem_op,
 #:
 #aise exc.InvalidRequestError(
 #The JSON cast operator JSON.%s() only works with a JSON "
 #index expression e.g. col['q'].%s()"
 # (method_name, method_name)
 #
 #xpr = self.expr._clone()
 #xpr.type = typ
 #eturn expr

 #omparator_factory = Comparator

 #property
 #ef python_type(self):
 #eturn dict

 #property
 #ef should_evaluate_none(self):
 #""Alias of :attr:`_types.JSON.none_as_null`"""
 #eturn not self.none_as_null

 #should_evaluate_none.setter
 #ef should_evaluate_none(self, value):
 #elf.none_as_null = not value

 #util.memoized_property
 #ef _str_impl(self):
 #eturn String(_expect_unicode=True)

 #ef bind_processor(self, dialect):
 #tring_process = self._str_impl.bind_processor(dialect)

 #son_serializer = dialect._json_serializer or json.dumps

 #ef process(value):
 #f value is self.NULL:
 #alue = None
 #lif isinstance(value, elements.Null) or (
 #alue is None and self.none_as_null
 #:
 #eturn None

 #erialized = json_serializer(value)
 #f string_process:
 #erialized = string_process(serialized)
 #eturn serialized

 #eturn process

 #ef result_processor(self, dialect, coltype):
 #tring_process = self._str_impl.result_processor(dialect, coltype)
 #son_deserializer = dialect._json_deserializer or json.loads

 #ef process(value):
 #f value is None:
 #eturn None
 #f string_process:
 #alue = string_process(value)
 #eturn json_deserializer(value)

 #eturn process


class ARRAY(SchemaEventTarget, Indexable, Concatenable, TypeEngine):
 #""Represent a SQL Array type.

 #. note::  This type serves as the basis for all ARRAY operations.
 #owever, currently **only the PostgreSQL backend has support for SQL
 #rrays in SQLAlchemy**. It is recommended to use the PostgreSQL-specific
 #class:`sqlalchemy.dialects.postgresql.ARRAY` type directly when using
 #RRAY types with PostgreSQL, as it provides additional operators
 #pecific to that backend.

 #class:`_types.ARRAY` is part of the Core in support of various SQL
 #tandard functions such as :class:`_functions.array_agg`
 #hich explicitly involve
 #rrays; however, with the exception of the PostgreSQL backend and possibly
 #ome third-party dialects, no other SQLAlchemy built-in dialect has support
 #or this type.

 #n :class:`_types.ARRAY` type is constructed given the "type"
 #f element::

 #ytable = Table("mytable", metadata,
 #olumn("data", ARRAY(Integer))
 #

 #he above type represents an N-dimensional array,
 #eaning a supporting backend such as PostgreSQL will interpret values
 #ith any number of dimensions automatically.   To produce an INSERT
 #onstruct that passes in a 1-dimensional array of integers::

 #onnection.execute(
 #ytable.insert(),
 #ata=[1,2,3]
 #

 #he :class:`_types.ARRAY` type can be constructed given a fixed number
 #f dimensions::

 #ytable = Table("mytable", metadata,
 #olumn("data", ARRAY(Integer, dimensions=2))
 #

 #ending a number of dimensions is optional, but recommended if the
 #atatype is to represent arrays of more than one dimension.  This number
 #s used:

 # When emitting the type declaration itself to the database, e.g.
 #`INTEGER[][]``

 # When translating Python values to database values, and vice versa, e.g.
 #n ARRAY of :class:`.Unicode` objects uses this number to efficiently
 #ccess the string values inside of array structures without resorting
 #o per-row type inspection

 # When used with the Python ``getitem`` accessor, the number of dimensions
 #erves to define the kind of type that the ``[]`` operator should
 #eturn, e.g. for an ARRAY of INTEGER with two dimensions::

 #>> expr = table.c.column[5]  # returns ARRAY(Integer, dimensions=1)
 #>> expr = expr[6]  # returns Integer

 #or 1-dimensional arrays, an :class:`_types.ARRAY` instance with no
 #imension parameter will generally assume single-dimensional behaviors.

 #QL expressions of type :class:`_types.ARRAY` have support for "index" and
 #slice" behavior.  The Python ``[]`` operator works normally here, given
 #nteger indexes or slices.  Arrays default to 1-based indexing.
 #he operator produces binary expression
 #onstructs which will produce the appropriate SQL, both for
 #ELECT statements::

 #elect(mytable.c.data[5], mytable.c.data[2:7])

 #s well as UPDATE statements when the :meth:`_expression.Update.values`
 #ethod
 #s used::

 #ytable.update().values({
 #ytable.c.data[5]: 7,
 #ytable.c.data[2:7]: [1, 2, 3]
 #)

 #he :class:`_types.ARRAY` type also provides for the operators
 #meth:`.types.ARRAY.Comparator.any` and
 #meth:`.types.ARRAY.Comparator.all`. The PostgreSQL-specific version of
 #class:`_types.ARRAY` also provides additional operators.

 #. versionadded:: 1.1.0

 #. seealso::

 #class:`sqlalchemy.dialects.postgresql.ARRAY`

 #""

 #_visit_name__ = "ARRAY"

 #is_array = True

 #ero_indexes = False
 #""If True, Python zero-based indexes should be interpreted as one-based
 #n the SQL expression side."""

 #lass Comparator(Indexable.Comparator, Concatenable.Comparator):

 #""Define comparison operations for :class:`_types.ARRAY`.

 #ore operators are available on the dialect-specific form
 #f this type.  See :class:`.postgresql.ARRAY.Comparator`.

 #""

 #ef _setup_getitem(self, index):
 #f isinstance(index, slice):
 #eturn_type = self.type
 #f self.type.zero_indexes:
 #ndex = slice(index.start + 1, index.stop + 1, index.step)
 #lice_ = Slice(
 #ndex.start, index.stop, index.step, _name=self.expr.key
 #
 #eturn operators.getitem, slice_, return_type
 #lse:
 #f self.type.zero_indexes:
 #ndex += 1
 #f self.type.dimensions is None or self.type.dimensions == 1:
 #eturn_type = self.type.item_type
 #lse:
 #dapt_kw = {"dimensions": self.type.dimensions - 1}
 #eturn_type = self.type.adapt(
 #elf.type.__class__, **adapt_kw
 #

 #eturn operators.getitem, index, return_type

 #ef contains(self, *arg, **kw):
 #aise NotImplementedError(
 #ARRAY.contains() not implemented for the base "
 #ARRAY type; please use the dialect-specific ARRAY type"
 #

 #util.preload_module("sqlalchemy.sql.elements")
 #ef any(self, other, operator=None):
 #""Return ``other operator ANY (array)`` clause.

 #rgument places are switched, because ANY requires array
 #xpression to be on the right hand-side.

 #.g.::

 #rom sqlalchemy.sql import operators

 #onn.execute(
 #elect(table.c.data).where(
 #able.c.data.any(7, operator=operators.lt)
 #
 #

 #param other: expression to be compared
 #param operator: an operator object from the
 #mod:`sqlalchemy.sql.operators`
 #ackage, defaults to :func:`.operators.eq`.

 #. seealso::

 #func:`_expression.any_`

 #meth:`.types.ARRAY.Comparator.all`

 #""
 #lements = util.preloaded.sql_elements
 #perator = operator if operator else operators.eq

            # send plain BinaryExpression so that negate remains at None,
            # leading to NOT expr for negation.
 #eturn elements.BinaryExpression(
 #oercions.expect(roles.ExpressionElementRole, other),
 #lements.CollectionAggregate._create_any(self.expr),
 #perator,
 #

 #util.preload_module("sqlalchemy.sql.elements")
 #ef all(self, other, operator=None):
 #""Return ``other operator ALL (array)`` clause.

 #rgument places are switched, because ALL requires array
 #xpression to be on the right hand-side.

 #.g.::

 #rom sqlalchemy.sql import operators

 #onn.execute(
 #elect(table.c.data).where(
 #able.c.data.all(7, operator=operators.lt)
 #
 #

 #param other: expression to be compared
 #param operator: an operator object from the
 #mod:`sqlalchemy.sql.operators`
 #ackage, defaults to :func:`.operators.eq`.

 #. seealso::

 #func:`_expression.all_`

 #meth:`.types.ARRAY.Comparator.any`

 #""
 #lements = util.preloaded.sql_elements
 #perator = operator if operator else operators.eq

            # send plain BinaryExpression so that negate remains at None,
            # leading to NOT expr for negation.
 #eturn elements.BinaryExpression(
 #oercions.expect(roles.ExpressionElementRole, other),
 #lements.CollectionAggregate._create_all(self.expr),
 #perator,
 #

 #omparator_factory = Comparator

 #ef __init__(
 #elf, item_type, as_tuple=False, dimensions=None, zero_indexes=False
 #:
 #""Construct an :class:`_types.ARRAY`.

 #.g.::

 #olumn('myarray', ARRAY(Integer))

 #rguments are:

 #param item_type: The data type of items of this array. Note that
 #imensionality is irrelevant here, so multi-dimensional arrays like
 #`INTEGER[][]``, are constructed as ``ARRAY(Integer)``, not as
 #`ARRAY(ARRAY(Integer))`` or such.

 #param as_tuple=False: Specify whether return results
 #hould be converted to tuples from lists.  This parameter is
 #ot generally needed as a Python list corresponds well
 #o a SQL array.

 #param dimensions: if non-None, the ARRAY will assume a fixed
 #umber of dimensions.   This impacts how the array is declared
 #n the database, how it goes about interpreting Python and
 #esult values, as well as how expression behavior in conjunction
 #ith the "getitem" operator works.  See the description at
 #class:`_types.ARRAY` for additional detail.

 #param zero_indexes=False: when True, index values will be converted
 #etween Python zero-based and SQL one-based indexes, e.g.
 # value of one will be added to all index values before passing
 #o the database.

 #""
 #f isinstance(item_type, ARRAY):
 #aise ValueError(
 #Do not nest ARRAY types; ARRAY(basetype) "
 #handles multi-dimensional arrays of basetype"
 #
 #f isinstance(item_type, type):
 #tem_type = item_type()
 #elf.item_type = item_type
 #elf.as_tuple = as_tuple
 #elf.dimensions = dimensions
 #elf.zero_indexes = zero_indexes

 #property
 #ef hashable(self):
 #eturn self.as_tuple

 #property
 #ef python_type(self):
 #eturn list

 #ef compare_values(self, x, y):
 #eturn x == y

 #ef _set_parent(self, column, outer=False, **kw):
 #""Support SchemaEventTarget"""

 #f not outer and isinstance(self.item_type, SchemaEventTarget):
 #elf.item_type._set_parent(column, **kw)

 #ef _set_parent_with_dispatch(self, parent):
 #""Support SchemaEventTarget"""

 #uper(ARRAY, self)._set_parent_with_dispatch(parent, outer=True)

 #f isinstance(self.item_type, SchemaEventTarget):
 #elf.item_type._set_parent_with_dispatch(parent)


class TupleType(TypeEngine):
 #""represent the composite type of a Tuple."""

 #is_tuple_type = True

 #ef __init__(self, *types):
 #elf._fully_typed = NULLTYPE not in types
 #elf.types = types

 #ef _resolve_values_to_types(self, value):
 #f self._fully_typed:
 #eturn self
 #lse:
 #eturn TupleType(
 #[
 #resolve_value_to_type(elem) if typ is NULLTYPE else typ
 #or typ, elem in zip(self.types, value)
 #
 #

 #ef result_processor(self, dialect, coltype):
 #aise NotImplementedError(
 #The tuple type does not support being fetched "
 #as a column in a result row."
 #


class REAL(Float):

 #""The SQL REAL type."""

 #_visit_name__ = "REAL"


class FLOAT(Float):

 #""The SQL FLOAT type."""

 #_visit_name__ = "FLOAT"


class NUMERIC(Numeric):

 #""The SQL NUMERIC type."""

 #_visit_name__ = "NUMERIC"


class DECIMAL(Numeric):

 #""The SQL DECIMAL type."""

 #_visit_name__ = "DECIMAL"


class INTEGER(Integer):

 #""The SQL INT or INTEGER type."""

 #_visit_name__ = "INTEGER"


INT = INTEGER


class SMALLINT(SmallInteger):

 #""The SQL SMALLINT type."""

 #_visit_name__ = "SMALLINT"


class BIGINT(BigInteger):

 #""The SQL BIGINT type."""

 #_visit_name__ = "BIGINT"


class TIMESTAMP(DateTime):

 #""The SQL TIMESTAMP type.

 #class:`_types.TIMESTAMP` datatypes have support for timezone
 #torage on some backends, such as PostgreSQL and Oracle.  Use the
 #paramref:`~types.TIMESTAMP.timezone` argument in order to enable
 #TIMESTAMP WITH TIMEZONE" for these backends.

 #""

 #_visit_name__ = "TIMESTAMP"

 #ef __init__(self, timezone=False):
 #""Construct a new :class:`_types.TIMESTAMP`.

 #param timezone: boolean.  Indicates that the TIMESTAMP type should
 #nable timezone support, if available on the target database.
 #n a per-dialect basis is similar to "TIMESTAMP WITH TIMEZONE".
 #f the target database does not support timezones, this flag is
 #gnored.


 #""
 #uper(TIMESTAMP, self).__init__(timezone=timezone)

 #ef get_dbapi_type(self, dbapi):
 #eturn dbapi.TIMESTAMP


class DATETIME(DateTime):

 #""The SQL DATETIME type."""

 #_visit_name__ = "DATETIME"


class DATE(Date):

 #""The SQL DATE type."""

 #_visit_name__ = "DATE"


class TIME(Time):

 #""The SQL TIME type."""

 #_visit_name__ = "TIME"


class TEXT(Text):

 #""The SQL TEXT type."""

 #_visit_name__ = "TEXT"


class CLOB(Text):

 #""The CLOB type.

 #his type is found in Oracle and Informix.
 #""

 #_visit_name__ = "CLOB"


class VARCHAR(String):

 #""The SQL VARCHAR type."""

 #_visit_name__ = "VARCHAR"


class NVARCHAR(Unicode):

 #""The SQL NVARCHAR type."""

 #_visit_name__ = "NVARCHAR"


class CHAR(String):

 #""The SQL CHAR type."""

 #_visit_name__ = "CHAR"


class NCHAR(Unicode):

 #""The SQL NCHAR type."""

 #_visit_name__ = "NCHAR"


class BLOB(LargeBinary):

 #""The SQL BLOB type."""

 #_visit_name__ = "BLOB"


class BINARY(_Binary):

 #""The SQL BINARY type."""

 #_visit_name__ = "BINARY"


class VARBINARY(_Binary):

 #""The SQL VARBINARY type."""

 #_visit_name__ = "VARBINARY"


class BOOLEAN(Boolean):

 #""The SQL BOOLEAN type."""

 #_visit_name__ = "BOOLEAN"


class NullType(TypeEngine):

 #""An unknown type.

 #class:`.NullType` is used as a default type for those cases where
 # type cannot be determined, including:

 # During table reflection, when the type of a column is not recognized
 #y the :class:`.Dialect`
 # When constructing SQL expressions using plain Python objects of
 #nknown types (e.g. ``somecolumn == my_special_object``)
 # When a new :class:`_schema.Column` is created,
 #nd the given type is passed
 #s ``None`` or is not passed at all.

 #he :class:`.NullType` can be used within SQL expression invocation
 #ithout issue, it just has no behavior either at the expression
 #onstruction level or at the bind-parameter/result processing level.
 #class:`.NullType` will result in a :exc:`.CompileError` if the compiler
 #s asked to render the type itself, such as if it is used in a
 #func:`.cast` operation or within a schema creation operation such as that
 #nvoked by :meth:`_schema.MetaData.create_all` or the
 #class:`.CreateTable`
 #onstruct.

 #""

 #_visit_name__ = "null"

 #isnull = True

 #ef literal_processor(self, dialect):
 #ef process(value):
 #aise exc.CompileError(
 #Don't know how to render literal SQL value: %r" % value
 #

 #eturn process

 #lass Comparator(TypeEngine.Comparator):
 #ef _adapt_expression(self, op, other_comparator):
 #f isinstance(
 #ther_comparator, NullType.Comparator
 # or not operators.is_commutative(op):
 #eturn op, self.expr.type
 #lse:
 #eturn other_comparator._adapt_expression(op, self)

 #omparator_factory = Comparator


class TableValueType(HasCacheKey, TypeEngine):
 #""Refers to a table value type."""

 #is_table_value = True

 #traverse_internals = [
 #"_elements", InternalTraversal.dp_clauseelement_list),
 #

 #ef __init__(self, *elements):
 #elf._elements = [
 #oercions.expect(roles.StrAsPlainColumnRole, elem)
 #or elem in elements
 #


class MatchType(Boolean):
 #""Refers to the return type of the MATCH operator.

 #s the :meth:`.ColumnOperators.match` is probably the most open-ended
 #perator in generic SQLAlchemy Core, we can't assume the return type
 #t SQL evaluation time, as MySQL returns a floating point, not a boolean,
 #nd other backends might do something different.    So this type
 #cts as a placeholder, currently subclassing :class:`.Boolean`.
 #he type allows dialects to inject result-processing functionality
 #f needed, and on MySQL will return floating-point values.

 #. versionadded:: 1.0.0

 #""


NULLTYPE = NullType()
BOOLEANTYPE = Boolean()
STRINGTYPE = String()
INTEGERTYPE = Integer()
MATCHTYPE = MatchType()
TABLEVALUE = TableValueType()

_type_map = {
 #nt: Integer(),
 #loat: Float(),
 #ool: BOOLEANTYPE,
 #ecimal.Decimal: Numeric(),
 #t.date: Date(),
 #t.datetime: DateTime(),
 #t.time: Time(),
 #t.timedelta: Interval(),
 #til.NoneType: NULLTYPE,
}

if util.py3k:
 #type_map[bytes] = LargeBinary()  # noqa
 #type_map[str] = Unicode()
else:
 #type_map[unicode] = Unicode()  # noqa
 #type_map[str] = String()


_type_map_get = _type_map.get


def _resolve_value_to_type(value):
 #result_type = _type_map_get(type(value), False)
 #f _result_type is False:
        # use inspect() to detect SQLAlchemy built-in
        # objects.
 #nsp = inspection.inspect(value, False)
 #f (
 #nsp is not None
 #nd
            # foil mock.Mock() and other impostors by ensuring
            # the inspection target itself self-inspects
 #nsp.__class__ in inspection._registrars
 #:
 #aise exc.ArgumentError(
 #Object %r is not legal as a SQL literal value" % value
 #
 #eturn NULLTYPE
 #lse:
 #eturn _result_type


# back-assign to type_api
type_api.BOOLEANTYPE = BOOLEANTYPE
type_api.STRINGTYPE = STRINGTYPE
type_api.INTEGERTYPE = INTEGERTYPE
type_api.NULLTYPE = NULLTYPE
type_api.MATCHTYPE = MATCHTYPE
type_api.INDEXABLE = Indexable
type_api.TABLEVALUE = TABLEVALUE
type_api._resolve_value_to_type = _resolve_value_to_type
TypeEngine.Comparator.BOOLEANTYPE = BOOLEANTYPE
