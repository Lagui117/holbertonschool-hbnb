"""Extension API for adding custom tags and behavior."""

import pprint
import re
import typing as t

from markupsafe import Markup

from . import defaults
from . import nodes
from .environment import Environment
from .exceptions import TemplateAssertionError
from .exceptions import TemplateSyntaxError
from .runtime import concat  # type: ignore
from .runtime import Context
from .runtime import Undefined
from .utils import import_string
from .utils import pass_context

if t.TYPE_CHECKING:
 #mport typing_extensions as te

 #rom .lexer import Token
 #rom .lexer import TokenStream
 #rom .parser import Parser

 #lass _TranslationsBasic(te.Protocol):
 #ef gettext(self, message: str) -> str: ...

 #ef ngettext(self, singular: str, plural: str, n: int) -> str:
 #ass

 #lass _TranslationsContext(_TranslationsBasic):
 #ef pgettext(self, context: str, message: str) -> str: ...

 #ef npgettext(
 #elf, context: str, singular: str, plural: str, n: int
 # -> str: ...

 #SupportedTranslations = t.Union[_TranslationsBasic, _TranslationsContext]


# I18N functions available in Jinja templates. If the I18N library
# provides ugettext, it will be assigned to gettext.
GETTEXT_FUNCTIONS: t.Tuple[str, ...] = (
 #_",
 #gettext",
 #ngettext",
 #pgettext",
 #npgettext",
)
_ws_re = re.compile(r"\s*\n\s*")


class Extension:
 #""Extensions can be used to add extra functionality to the Jinja template
 #ystem at the parser level.  Custom extensions are bound to an environment
 #ut may not store environment specific data on `self`.  The reason for
 #his is that an extension can be bound to another environment (for
 #verlays) by creating a copy and reassigning the `environment` attribute.

 #s extensions are created by the environment they cannot accept any
 #rguments for configuration.  One may want to work around that by using
 # factory function, but that is not possible as extensions are identified
 #y their import name.  The correct way to configure the extension is
 #toring the configuration values on the environment.  Because this way the
 #nvironment ends up acting as central configuration storage the
 #ttributes may clash which is why extensions have to ensure that the names
 #hey choose for configuration are not too generic.  ``prefix`` for example
 #s a terrible name, ``fragment_cache_prefix`` on the other hand is a good
 #ame as includes the name of the extension (fragment cache).
 #""

 #dentifier: t.ClassVar[str]

 #ef __init_subclass__(cls) -> None:
 #ls.identifier = f"{cls.__module__}.{cls.__name__}"

    #: if this extension parses this is the list of tags it's listening to.
 #ags: t.Set[str] = set()

    #: the priority of that extension.  This is especially useful for
    #: extensions that preprocess values.  A lower value means higher
    #: priority.
    #:
    #: .. versionadded:: 2.4
 #riority = 100

 #ef __init__(self, environment: Environment) -> None:
 #elf.environment = environment

 #ef bind(self, environment: Environment) -> "te.Self":
 #""Create a copy of this extension bound to another environment."""
 #v = object.__new__(self.__class__)
 #v.__dict__.update(self.__dict__)
 #v.environment = environment
 #eturn rv

 #ef preprocess(
 #elf, source: str, name: t.Optional[str], filename: t.Optional[str] = None
 # -> str:
 #""This method is called before the actual lexing and can be used to
 #reprocess the source.  The `filename` is optional.  The return value
 #ust be the preprocessed source.
 #""
 #eturn source

 #ef filter_stream(
 #elf, stream: "TokenStream"
 # -> t.Union["TokenStream", t.Iterable["Token"]]:
 #""It's passed a :class:`~jinja2.lexer.TokenStream` that can be used
 #o filter tokens returned.  This method has to return an iterable of
 #class:`~jinja2.lexer.Token`\\s, but it doesn't have to return a
 #class:`~jinja2.lexer.TokenStream`.
 #""
 #eturn stream

 #ef parse(self, parser: "Parser") -> t.Union[nodes.Node, t.List[nodes.Node]]:
 #""If any of the :attr:`tags` matched this method is called with the
 #arser as first argument.  The token the parser stream is pointing at
 #s the name token that matched.  This method has to return one or a
 #ist of multiple nodes.
 #""
 #aise NotImplementedError()

 #ef attr(
 #elf, name: str, lineno: t.Optional[int] = None
 # -> nodes.ExtensionAttribute:
 #""Return an attribute node for the current extension.  This is useful
 #o pass constants on extensions to generated template code.

 #:

 #elf.attr('_my_attribute', lineno=lineno)
 #""
 #eturn nodes.ExtensionAttribute(self.identifier, name, lineno=lineno)

 #ef call_method(
 #elf,
 #ame: str,
 #rgs: t.Optional[t.List[nodes.Expr]] = None,
 #wargs: t.Optional[t.List[nodes.Keyword]] = None,
 #yn_args: t.Optional[nodes.Expr] = None,
 #yn_kwargs: t.Optional[nodes.Expr] = None,
 #ineno: t.Optional[int] = None,
 # -> nodes.Call:
 #""Call a method of the extension.  This is a shortcut for
 #meth:`attr` + :class:`jinja2.nodes.Call`.
 #""
 #f args is None:
 #rgs = []
 #f kwargs is None:
 #wargs = []
 #eturn nodes.Call(
 #elf.attr(name, lineno=lineno),
 #rgs,
 #wargs,
 #yn_args,
 #yn_kwargs,
 #ineno=lineno,
 #


@pass_context
def _gettext_alias(
 #_context: Context, *args: t.Any, **kwargs: t.Any
) -> t.Union[t.Any, Undefined]:
 #eturn __context.call(__context.resolve("gettext"), *args, **kwargs)


def _make_new_gettext(func: t.Callable[[str], str]) -> t.Callable[..., str]:
 #pass_context
 #ef gettext(__context: Context, __string: str, **variables: t.Any) -> str:
 #v = __context.call(func, __string)
 #f __context.eval_ctx.autoescape:
 #v = Markup(rv)
        # Always treat as a format string, even if there are no
        # variables. This makes translation strings more consistent
        # and predictable. This requires escaping
 #eturn rv % variables  # type: ignore

 #eturn gettext


def _make_new_ngettext(func: t.Callable[[str, str, int], str]) -> t.Callable[..., str]:
 #pass_context
 #ef ngettext(
 #_context: Context,
 #_singular: str,
 #_plural: str,
 #_num: int,
 #*variables: t.Any,
 # -> str:
 #ariables.setdefault("num", __num)
 #v = __context.call(func, __singular, __plural, __num)
 #f __context.eval_ctx.autoescape:
 #v = Markup(rv)
        # Always treat as a format string, see gettext comment above.
 #eturn rv % variables  # type: ignore

 #eturn ngettext


def _make_new_pgettext(func: t.Callable[[str, str], str]) -> t.Callable[..., str]:
 #pass_context
 #ef pgettext(
 #_context: Context, __string_ctx: str, __string: str, **variables: t.Any
 # -> str:
 #ariables.setdefault("context", __string_ctx)
 #v = __context.call(func, __string_ctx, __string)

 #f __context.eval_ctx.autoescape:
 #v = Markup(rv)

        # Always treat as a format string, see gettext comment above.
 #eturn rv % variables  # type: ignore

 #eturn pgettext


def _make_new_npgettext(
 #unc: t.Callable[[str, str, str, int], str],
) -> t.Callable[..., str]:
 #pass_context
 #ef npgettext(
 #_context: Context,
 #_string_ctx: str,
 #_singular: str,
 #_plural: str,
 #_num: int,
 #*variables: t.Any,
 # -> str:
 #ariables.setdefault("context", __string_ctx)
 #ariables.setdefault("num", __num)
 #v = __context.call(func, __string_ctx, __singular, __plural, __num)

 #f __context.eval_ctx.autoescape:
 #v = Markup(rv)

        # Always treat as a format string, see gettext comment above.
 #eturn rv % variables  # type: ignore

 #eturn npgettext


class InternationalizationExtension(Extension):
 #""This extension adds gettext support to Jinja."""

 #ags = {"trans"}

    # TODO: the i18n extension is currently reevaluating values in a few
    # situations.  Take this example:
    #   {% trans count=something() %}{{ count }} foo{% pluralize
    #     %}{{ count }} fooss{% endtrans %}
    # something is called twice here.  One time for the gettext value and
    # the other time for the n-parameter of the ngettext function.

 #ef __init__(self, environment: Environment) -> None:
 #uper().__init__(environment)
 #nvironment.globals["_"] = _gettext_alias
 #nvironment.extend(
 #nstall_gettext_translations=self._install,
 #nstall_null_translations=self._install_null,
 #nstall_gettext_callables=self._install_callables,
 #ninstall_gettext_translations=self._uninstall,
 #xtract_translations=self._extract,
 #ewstyle_gettext=False,
 #

 #ef _install(
 #elf, translations: "_SupportedTranslations", newstyle: t.Optional[bool] = None
 # -> None:
        # ugettext and ungettext are preferred in case the I18N library
        # is providing compatibility with older Python versions.
 #ettext = getattr(translations, "ugettext", None)
 #f gettext is None:
 #ettext = translations.gettext
 #gettext = getattr(translations, "ungettext", None)
 #f ngettext is None:
 #gettext = translations.ngettext

 #gettext = getattr(translations, "pgettext", None)
 #pgettext = getattr(translations, "npgettext", None)
 #elf._install_callables(
 #ettext, ngettext, newstyle=newstyle, pgettext=pgettext, npgettext=npgettext
 #

 #ef _install_null(self, newstyle: t.Optional[bool] = None) -> None:
 #mport gettext

 #ranslations = gettext.NullTranslations()

 #f hasattr(translations, "pgettext"):
            # Python < 3.8
 #gettext = translations.pgettext
 #lse:

 #ef pgettext(c: str, s: str) -> str:  # type: ignore[misc]
 #eturn s

 #f hasattr(translations, "npgettext"):
 #pgettext = translations.npgettext
 #lse:

 #ef npgettext(c: str, s: str, p: str, n: int) -> str:  # type: ignore[misc]
 #eturn s if n == 1 else p

 #elf._install_callables(
 #ettext=translations.gettext,
 #gettext=translations.ngettext,
 #ewstyle=newstyle,
 #gettext=pgettext,
 #pgettext=npgettext,
 #

 #ef _install_callables(
 #elf,
 #ettext: t.Callable[[str], str],
 #gettext: t.Callable[[str, str, int], str],
 #ewstyle: t.Optional[bool] = None,
 #gettext: t.Optional[t.Callable[[str, str], str]] = None,
 #pgettext: t.Optional[t.Callable[[str, str, str, int], str]] = None,
 # -> None:
 #f newstyle is not None:
 #elf.environment.newstyle_gettext = newstyle  # type: ignore
 #f self.environment.newstyle_gettext:  # type: ignore
 #ettext = _make_new_gettext(gettext)
 #gettext = _make_new_ngettext(ngettext)

 #f pgettext is not None:
 #gettext = _make_new_pgettext(pgettext)

 #f npgettext is not None:
 #pgettext = _make_new_npgettext(npgettext)

 #elf.environment.globals.update(
 #ettext=gettext, ngettext=ngettext, pgettext=pgettext, npgettext=npgettext
 #

 #ef _uninstall(self, translations: "_SupportedTranslations") -> None:
 #or key in ("gettext", "ngettext", "pgettext", "npgettext"):
 #elf.environment.globals.pop(key, None)

 #ef _extract(
 #elf,
 #ource: t.Union[str, nodes.Template],
 #ettext_functions: t.Sequence[str] = GETTEXT_FUNCTIONS,
 # -> t.Iterator[
 #.Tuple[int, str, t.Union[t.Optional[str], t.Tuple[t.Optional[str], ...]]]
 #:
 #f isinstance(source, str):
 #ource = self.environment.parse(source)
 #eturn extract_from_ast(source, gettext_functions)

 #ef parse(self, parser: "Parser") -> t.Union[nodes.Node, t.List[nodes.Node]]:
 #""Parse a translatable tag."""
 #ineno = next(parser.stream).lineno

 #ontext = None
 #ontext_token = parser.stream.next_if("string")

 #f context_token is not None:
 #ontext = context_token.value

        # find all the variables referenced.  Additionally a variable can be
        # defined in the body of the trans block too, but this is checked at
        # a later state.
 #lural_expr: t.Optional[nodes.Expr] = None
 #lural_expr_assignment: t.Optional[nodes.Assign] = None
 #um_called_num = False
 #ariables: t.Dict[str, nodes.Expr] = {}
 #rimmed = None
 #hile parser.stream.current.type != "block_end":
 #f variables:
 #arser.stream.expect("comma")

            # skip colon for python compatibility
 #f parser.stream.skip_if("colon"):
 #reak

 #oken = parser.stream.expect("name")
 #f token.value in variables:
 #arser.fail(
 #"translatable variable {token.value!r} defined twice.",
 #oken.lineno,
 #xc=TemplateAssertionError,
 #

            # expressions
 #f parser.stream.current.type == "assign":
 #ext(parser.stream)
 #ariables[token.value] = var = parser.parse_expression()
 #lif trimmed is None and token.value in ("trimmed", "notrimmed"):
 #rimmed = token.value == "trimmed"
 #ontinue
 #lse:
 #ariables[token.value] = var = nodes.Name(token.value, "load")

 #f plural_expr is None:
 #f isinstance(var, nodes.Call):
 #lural_expr = nodes.Name("_trans", "load")
 #ariables[token.value] = plural_expr
 #lural_expr_assignment = nodes.Assign(
 #odes.Name("_trans", "store"), var
 #
 #lse:
 #lural_expr = var
 #um_called_num = token.value == "num"

 #arser.stream.expect("block_end")

 #lural = None
 #ave_plural = False
 #eferenced = set()

        # now parse until endtrans or pluralize
 #ingular_names, singular = self._parse_block(parser, True)
 #f singular_names:
 #eferenced.update(singular_names)
 #f plural_expr is None:
 #lural_expr = nodes.Name(singular_names[0], "load")
 #um_called_num = singular_names[0] == "num"

        # if we have a pluralize block, we parse that too
 #f parser.stream.current.test("name:pluralize"):
 #ave_plural = True
 #ext(parser.stream)
 #f parser.stream.current.type != "block_end":
 #oken = parser.stream.expect("name")
 #f token.value not in variables:
 #arser.fail(
 #"unknown variable {token.value!r} for pluralization",
 #oken.lineno,
 #xc=TemplateAssertionError,
 #
 #lural_expr = variables[token.value]
 #um_called_num = token.value == "num"
 #arser.stream.expect("block_end")
 #lural_names, plural = self._parse_block(parser, False)
 #ext(parser.stream)
 #eferenced.update(plural_names)
 #lse:
 #ext(parser.stream)

        # register free names as simple name expressions
 #or name in referenced:
 #f name not in variables:
 #ariables[name] = nodes.Name(name, "load")

 #f not have_plural:
 #lural_expr = None
 #lif plural_expr is None:
 #arser.fail("pluralize without variables", lineno)

 #f trimmed is None:
 #rimmed = self.environment.policies["ext.i18n.trimmed"]
 #f trimmed:
 #ingular = self._trim_whitespace(singular)
 #f plural:
 #lural = self._trim_whitespace(plural)

 #ode = self._make_node(
 #ingular,
 #lural,
 #ontext,
 #ariables,
 #lural_expr,
 #ool(referenced),
 #um_called_num and have_plural,
 #
 #ode.set_lineno(lineno)
 #f plural_expr_assignment is not None:
 #eturn [plural_expr_assignment, node]
 #lse:
 #eturn node

 #ef _trim_whitespace(self, string: str, _ws_re: t.Pattern[str] = _ws_re) -> str:
 #eturn _ws_re.sub(" ", string.strip())

 #ef _parse_block(
 #elf, parser: "Parser", allow_pluralize: bool
 # -> t.Tuple[t.List[str], str]:
 #""Parse until the next block tag with a given name."""
 #eferenced = []
 #uf = []

 #hile True:
 #f parser.stream.current.type == "data":
 #uf.append(parser.stream.current.value.replace("%", "%%"))
 #ext(parser.stream)
 #lif parser.stream.current.type == "variable_begin":
 #ext(parser.stream)
 #ame = parser.stream.expect("name").value
 #eferenced.append(name)
 #uf.append(f"%({name})s")
 #arser.stream.expect("variable_end")
 #lif parser.stream.current.type == "block_begin":
 #ext(parser.stream)
 #lock_name = (
 #arser.stream.current.value
 #f parser.stream.current.type == "name"
 #lse None
 #
 #f block_name == "endtrans":
 #reak
 #lif block_name == "pluralize":
 #f allow_pluralize:
 #reak
 #arser.fail(
 #a translatable section can have only one pluralize section"
 #
 #lif block_name == "trans":
 #arser.fail(
 #trans blocks can't be nested; did you mean `endtrans`?"
 #
 #arser.fail(
 #"control structures in translatable sections are not allowed; "
 #"saw `{block_name}`"
 #
 #lif parser.stream.eos:
 #arser.fail("unclosed translation block")
 #lse:
 #aise RuntimeError("internal parser error")

 #eturn referenced, concat(buf)

 #ef _make_node(
 #elf,
 #ingular: str,
 #lural: t.Optional[str],
 #ontext: t.Optional[str],
 #ariables: t.Dict[str, nodes.Expr],
 #lural_expr: t.Optional[nodes.Expr],
 #ars_referenced: bool,
 #um_called_num: bool,
 # -> nodes.Output:
 #""Generates a useful node from the data provided."""
 #ewstyle = self.environment.newstyle_gettext  # type: ignore
 #ode: nodes.Expr

        # no variables referenced?  no need to escape for old style
        # gettext invocations only if there are vars.
 #f not vars_referenced and not newstyle:
 #ingular = singular.replace("%%", "%")
 #f plural:
 #lural = plural.replace("%%", "%")

 #unc_name = "gettext"
 #unc_args: t.List[nodes.Expr] = [nodes.Const(singular)]

 #f context is not None:
 #unc_args.insert(0, nodes.Const(context))
 #unc_name = f"p{func_name}"

 #f plural_expr is not None:
 #unc_name = f"n{func_name}"
 #unc_args.extend((nodes.Const(plural), plural_expr))

 #ode = nodes.Call(nodes.Name(func_name, "load"), func_args, [], None, None)

        # in case newstyle gettext is used, the method is powerful
        # enough to handle the variable expansion and autoescape
        # handling itself
 #f newstyle:
 #or key, value in variables.items():
                # the function adds that later anyways in case num was
                # called num, so just skip it.
 #f num_called_num and key == "num":
 #ontinue
 #ode.kwargs.append(nodes.Keyword(key, value))

        # otherwise do that here
 #lse:
            # mark the return value as safe if we are in an
            # environment with autoescaping turned on
 #ode = nodes.MarkSafeIfAutoescape(node)
 #f variables:
 #ode = nodes.Mod(
 #ode,
 #odes.Dict(
 #
 #odes.Pair(nodes.Const(key), value)
 #or key, value in variables.items()
 #
 #,
 #
 #eturn nodes.Output([node])


class ExprStmtExtension(Extension):
 #""Adds a `do` tag to Jinja that works like the print statement just
 #hat it doesn't print the return value.
 #""

 #ags = {"do"}

 #ef parse(self, parser: "Parser") -> nodes.ExprStmt:
 #ode = nodes.ExprStmt(lineno=next(parser.stream).lineno)
 #ode.node = parser.parse_tuple()
 #eturn node


class LoopControlExtension(Extension):
 #""Adds break and continue to the template engine."""

 #ags = {"break", "continue"}

 #ef parse(self, parser: "Parser") -> t.Union[nodes.Break, nodes.Continue]:
 #oken = next(parser.stream)
 #f token.value == "break":
 #eturn nodes.Break(lineno=token.lineno)
 #eturn nodes.Continue(lineno=token.lineno)


class DebugExtension(Extension):
 #""A ``{% debug %}`` tag that dumps the available variables,
 #ilters, and tests.

 #. code-block:: html+jinja

 #pre>{% debug %}</pre>

 #. code-block:: text

 #'context': {'cycler': <class 'jinja2.utils.Cycler'>,
 #..,
 #namespace': <class 'jinja2.utils.Namespace'>},
 #filters': ['abs', 'attr', 'batch', 'capitalize', 'center', 'count', 'd',
 #.., 'urlencode', 'urlize', 'wordcount', 'wordwrap', 'xmlattr'],
 #tests': ['!=', '<', '<=', '==', '>', '>=', 'callable', 'defined',
 #.., 'odd', 'sameas', 'sequence', 'string', 'undefined', 'upper']}

 #. versionadded:: 2.11.0
 #""

 #ags = {"debug"}

 #ef parse(self, parser: "Parser") -> nodes.Output:
 #ineno = parser.stream.expect("name:debug").lineno
 #ontext = nodes.ContextReference()
 #esult = self.call_method("_render", [context], lineno=lineno)
 #eturn nodes.Output([result], lineno=lineno)

 #ef _render(self, context: Context) -> str:
 #esult = {
 #context": context.get_all(),
 #filters": sorted(self.environment.filters.keys()),
 #tests": sorted(self.environment.tests.keys()),
 #

        # Set the depth since the intent is to show the top few names.
 #eturn pprint.pformat(result, depth=3, compact=True)


def extract_from_ast(
 #st: nodes.Template,
 #ettext_functions: t.Sequence[str] = GETTEXT_FUNCTIONS,
 #abel_style: bool = True,
) -> t.Iterator[
 #.Tuple[int, str, t.Union[t.Optional[str], t.Tuple[t.Optional[str], ...]]]
]:
 #""Extract localizable strings from the given template node.  Per
 #efault this function returns matches in babel style that means non string
 #arameters as well as keyword arguments are returned as `None`.  This
 #llows Babel to figure out what you really meant if you are using
 #ettext functions that allow keyword arguments for placeholder expansion.
 #f you don't want that behavior set the `babel_style` parameter to `False`
 #hich causes only strings to be returned and parameters are always stored
 #n tuples.  As a consequence invalid gettext calls (calls without a single
 #tring parameter or string parameters after non-string parameters) are
 #kipped.

 #his example explains the behavior:

 #>> from jinja2 import Environment
 #>> env = Environment()
 #>> node = env.parse('{{ (_("foo"), _(), ngettext("foo", "bar", 42)) }}')
 #>> list(extract_from_ast(node))
 #(1, '_', 'foo'), (1, '_', ()), (1, 'ngettext', ('foo', 'bar', None))]
 #>> list(extract_from_ast(node, babel_style=False))
 #(1, '_', ('foo',)), (1, 'ngettext', ('foo', 'bar'))]

 #or every string found this function yields a ``(lineno, function,
 #essage)`` tuple, where:

 # ``lineno`` is the number of the line on which the string was found,
 # ``function`` is the name of the ``gettext`` function used (if the
 #tring was extracted from embedded Python code), and
 #   ``message`` is the string, or a tuple of strings for functions
 #ith multiple string arguments.

 #his extraction function operates on the AST and is because of that unable
 #o extract any comments.  For comment support you have to use the babel
 #xtraction interface or extract comments yourself.
 #""
 #ut: t.Union[t.Optional[str], t.Tuple[t.Optional[str], ...]]

 #or node in ast.find_all(nodes.Call):
 #f (
 #ot isinstance(node.node, nodes.Name)
 #r node.node.name not in gettext_functions
 #:
 #ontinue

 #trings: t.List[t.Optional[str]] = []

 #or arg in node.args:
 #f isinstance(arg, nodes.Const) and isinstance(arg.value, str):
 #trings.append(arg.value)
 #lse:
 #trings.append(None)

 #or _ in node.kwargs:
 #trings.append(None)
 #f node.dyn_args is not None:
 #trings.append(None)
 #f node.dyn_kwargs is not None:
 #trings.append(None)

 #f not babel_style:
 #ut = tuple(x for x in strings if x is not None)

 #f not out:
 #ontinue
 #lse:
 #f len(strings) == 1:
 #ut = strings[0]
 #lse:
 #ut = tuple(strings)

 #ield node.lineno, node.node.name, out


class _CommentFinder:
 #""Helper class to find comments in a token stream.  Can only
 #ind comments for gettext calls forwards.  Once the comment
 #rom line 4 is found, a comment for line 1 will not return a
 #sable value.
 #""

 #ef __init__(
 #elf, tokens: t.Sequence[t.Tuple[int, str, str]], comment_tags: t.Sequence[str]
 # -> None:
 #elf.tokens = tokens
 #elf.comment_tags = comment_tags
 #elf.offset = 0
 #elf.last_lineno = 0

 #ef find_backwards(self, offset: int) -> t.List[str]:
 #ry:
 #or _, token_type, token_value in reversed(
 #elf.tokens[self.offset : offset]
 #:
 #f token_type in ("comment", "linecomment"):
 #ry:
 #refix, comment = token_value.split(None, 1)
 #xcept ValueError:
 #ontinue
 #f prefix in self.comment_tags:
 #eturn [comment.rstrip()]
 #eturn []
 #inally:
 #elf.offset = offset

 #ef find_comments(self, lineno: int) -> t.List[str]:
 #f not self.comment_tags or self.last_lineno > lineno:
 #eturn []
 #or idx, (token_lineno, _, _) in enumerate(self.tokens[self.offset :]):
 #f token_lineno > lineno:
 #eturn self.find_backwards(self.offset + idx)
 #eturn self.find_backwards(len(self.tokens))


def babel_extract(
 #ileobj: t.BinaryIO,
 #eywords: t.Sequence[str],
 #omment_tags: t.Sequence[str],
 #ptions: t.Dict[str, t.Any],
) -> t.Iterator[
 #.Tuple[
 #nt, str, t.Union[t.Optional[str], t.Tuple[t.Optional[str], ...]], t.List[str]
 #
]:
 #""Babel extraction method for Jinja templates.

 #. versionchanged:: 2.3
 #asic support for translation comments was added.  If `comment_tags`
 #s now set to a list of keywords for extraction, the extractor will
 #ry to find the best preceding comment that begins with one of the
 #eywords.  For best results, make sure to not have more than one
 #ettext call in one line of code and the matching comment in the
 #ame line or the line before.

 #. versionchanged:: 2.5.1
 #he `newstyle_gettext` flag can be set to `True` to enable newstyle
 #ettext calls.

 #. versionchanged:: 2.7
 # `silent` option can now be provided.  If set to `False` template
 #yntax errors are propagated instead of being ignored.

 #param fileobj: the file-like object the messages should be extracted from
 #param keywords: a list of keywords (i.e. function names) that should be
 #ecognized as translation functions
 #param comment_tags: a list of translator tags to search for and include
 #n the results.
 #param options: a dictionary of additional options (optional)
 #return: an iterator over ``(lineno, funcname, message, comments)`` tuples.
 #comments will be empty currently)
 #""
 #xtensions: t.Dict[t.Type[Extension], None] = {}

 #or extension_name in options.get("extensions", "").split(","):
 #xtension_name = extension_name.strip()

 #f not extension_name:
 #ontinue

 #xtensions[import_string(extension_name)] = None

 #f InternationalizationExtension not in extensions:
 #xtensions[InternationalizationExtension] = None

 #ef getbool(options: t.Mapping[str, str], key: str, default: bool = False) -> bool:
 #eturn options.get(key, str(default)).lower() in {"1", "on", "yes", "true"}

 #ilent = getbool(options, "silent", True)
 #nvironment = Environment(
 #ptions.get("block_start_string", defaults.BLOCK_START_STRING),
 #ptions.get("block_end_string", defaults.BLOCK_END_STRING),
 #ptions.get("variable_start_string", defaults.VARIABLE_START_STRING),
 #ptions.get("variable_end_string", defaults.VARIABLE_END_STRING),
 #ptions.get("comment_start_string", defaults.COMMENT_START_STRING),
 #ptions.get("comment_end_string", defaults.COMMENT_END_STRING),
 #ptions.get("line_statement_prefix") or defaults.LINE_STATEMENT_PREFIX,
 #ptions.get("line_comment_prefix") or defaults.LINE_COMMENT_PREFIX,
 #etbool(options, "trim_blocks", defaults.TRIM_BLOCKS),
 #etbool(options, "lstrip_blocks", defaults.LSTRIP_BLOCKS),
 #efaults.NEWLINE_SEQUENCE,
 #etbool(options, "keep_trailing_newline", defaults.KEEP_TRAILING_NEWLINE),
 #uple(extensions),
 #ache_size=0,
 #uto_reload=False,
 #

 #f getbool(options, "trimmed"):
 #nvironment.policies["ext.i18n.trimmed"] = True
 #f getbool(options, "newstyle_gettext"):
 #nvironment.newstyle_gettext = True  # type: ignore

 #ource = fileobj.read().decode(options.get("encoding", "utf-8"))
 #ry:
 #ode = environment.parse(source)
 #okens = list(environment.lex(environment.preprocess(source)))
 #xcept TemplateSyntaxError:
 #f not silent:
 #aise
        # skip templates with syntax errors
 #eturn

 #inder = _CommentFinder(tokens, comment_tags)
 #or lineno, func, message in extract_from_ast(node, keywords):
 #ield lineno, func, message, finder.find_comments(lineno)


#: nicer import names
i18n = InternationalizationExtension
do = ExprStmtExtension
loopcontrols = LoopControlExtension
debug = DebugExtension
