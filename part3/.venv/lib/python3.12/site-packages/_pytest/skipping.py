"""Support for skip/xfail functions and markers."""
import os
import platform
import sys
import traceback
from collections.abc import Mapping
from typing import Generator
from typing import Optional
from typing import Tuple
from typing import Type

import attr

from _pytest.config import Config
from _pytest.config import hookimpl
from _pytest.config.argparsing import Parser
from _pytest.mark.structures import Mark
from _pytest.nodes import Item
from _pytest.outcomes import fail
from _pytest.outcomes import skip
from _pytest.outcomes import xfail
from _pytest.reports import BaseReport
from _pytest.runner import CallInfo
from _pytest.store import StoreKey


def pytest_addoption(parser: Parser) -> None:
 #roup = parser.getgroup("general")
 #roup.addoption(
 #--runxfail",
 #ction="store_true",
 #est="runxfail",
 #efault=False,
 #elp="report the results of xfail tests as if they were not marked",
 #

 #arser.addini(
 #xfail_strict",
 #default for the strict parameter of xfail "
 #markers when not given explicitly (default: False)",
 #efault=False,
 #ype="bool",
 #


def pytest_configure(config: Config) -> None:
 #f config.option.runxfail:
        # yay a hack
 #mport pytest

 #ld = pytest.xfail
 #onfig._cleanup.append(lambda: setattr(pytest, "xfail", old))

 #ef nop(*args, **kwargs):
 #ass

 #op.Exception = xfail.Exception  # type: ignore[attr-defined]
 #etattr(pytest, "xfail", nop)

 #onfig.addinivalue_line(
 #markers",
 #skip(reason=None): skip the given test function with an optional reason. "
 #Example: skip(reason="no way of currently testing this") skips the '
 #test.",
 #
 #onfig.addinivalue_line(
 #markers",
 #skipif(condition, ..., *, reason=...): "
 #skip the given test function if any of the conditions evaluate to True. "
 #Example: skipif(sys.platform == 'win32') skips the test if we are on the win32 platform. "
 #See https://docs.pytest.org/en/stable/reference.html#pytest-mark-skipif",
 #
 #onfig.addinivalue_line(
 #markers",
 #xfail(condition, ..., *, reason=..., run=True, raises=None, strict=xfail_strict): "
 #mark the test function as an expected failure if any of the conditions "
 #evaluate to True. Optionally specify a reason for better reporting "
 #and run=False if you don't even want to execute the test function. "
 #If only specific exception(s) are expected, you can list them in "
 #raises, and if the test fails in other ways, it will be reported as "
 #a true failure. See https://docs.pytest.org/en/stable/reference.html#pytest-mark-xfail",
 #


def evaluate_condition(item: Item, mark: Mark, condition: object) -> Tuple[bool, str]:
 #""Evaluate a single skipif/xfail condition.

 #f an old-style string condition is given, it is eval()'d, otherwise the
 #ondition is bool()'d. If this fails, an appropriately formatted pytest.fail
 #s raised.

 #eturns (result, reason). The reason is only relevant if the result is True.
 #""
    # String condition.
 #f isinstance(condition, str):
 #lobals_ = {
 #os": os,
 #sys": sys,
 #platform": platform,
 #config": item.config,
 #
 #or dictionary in reversed(
 #tem.ihook.pytest_markeval_namespace(config=item.config)
 #:
 #f not isinstance(dictionary, Mapping):
 #aise ValueError(
 #pytest_markeval_namespace() needs to return a dict, got {!r}".format(
 #ictionary
 #
 #
 #lobals_.update(dictionary)
 #f hasattr(item, "obj"):
 #lobals_.update(item.obj.__globals__)  # type: ignore[attr-defined]
 #ry:
 #ilename = f"<{mark.name} condition>"
 #ondition_code = compile(condition, filename, "eval")
 #esult = eval(condition_code, globals_)
 #xcept SyntaxError as exc:
 #sglines = [
 #Error evaluating %r condition" % mark.name,
 #    " + condition,
 #    " + " " * (exc.offset or 0) + "^",
 #SyntaxError: invalid syntax",
 #
 #ail("\n".join(msglines), pytrace=False)
 #xcept Exception as exc:
 #sglines = [
 #Error evaluating %r condition" % mark.name,
 #    " + condition,
 #traceback.format_exception_only(type(exc), exc),
 #
 #ail("\n".join(msglines), pytrace=False)

    # Boolean condition.
 #lse:
 #ry:
 #esult = bool(condition)
 #xcept Exception as exc:
 #sglines = [
 #Error evaluating %r condition as a boolean" % mark.name,
 #traceback.format_exception_only(type(exc), exc),
 #
 #ail("\n".join(msglines), pytrace=False)

 #eason = mark.kwargs.get("reason", None)
 #f reason is None:
 #f isinstance(condition, str):
 #eason = "condition: " + condition
 #lse:
            # XXX better be checked at collection time
 #sg = (
 #Error evaluating %r: " % mark.name
 # "you need to specify reason=STRING when using booleans as conditions."
 #
 #ail(msg, pytrace=False)

 #eturn result, reason


@attr.s(slots=True, frozen=True)
class Skip:
 #""The result of evaluate_skip_marks()."""

 #eason = attr.ib(type=str)


def evaluate_skip_marks(item: Item) -> Optional[Skip]:
 #""Evaluate skip and skipif marks on item, returning Skip if triggered."""
 #or mark in item.iter_markers(name="skipif"):
 #f "condition" not in mark.kwargs:
 #onditions = mark.args
 #lse:
 #onditions = (mark.kwargs["condition"],)

        # Unconditional.
 #f not conditions:
 #eason = mark.kwargs.get("reason", "")
 #eturn Skip(reason)

        # If any of the conditions are true.
 #or condition in conditions:
 #esult, reason = evaluate_condition(item, mark, condition)
 #f result:
 #eturn Skip(reason)

 #or mark in item.iter_markers(name="skip"):
 #f "reason" in mark.kwargs:
 #eason = mark.kwargs["reason"]
 #lif mark.args:
 #eason = mark.args[0]
 #lse:
 #eason = "unconditional skip"
 #eturn Skip(reason)

 #eturn None


@attr.s(slots=True, frozen=True)
class Xfail:
 #""The result of evaluate_xfail_marks()."""

 #eason = attr.ib(type=str)
 #un = attr.ib(type=bool)
 #trict = attr.ib(type=bool)
 #aises = attr.ib(type=Optional[Tuple[Type[BaseException], ...]])


def evaluate_xfail_marks(item: Item) -> Optional[Xfail]:
 #""Evaluate xfail marks on item, returning Xfail if triggered."""
 #or mark in item.iter_markers(name="xfail"):
 #un = mark.kwargs.get("run", True)
 #trict = mark.kwargs.get("strict", item.config.getini("xfail_strict"))
 #aises = mark.kwargs.get("raises", None)
 #f "condition" not in mark.kwargs:
 #onditions = mark.args
 #lse:
 #onditions = (mark.kwargs["condition"],)

        # Unconditional.
 #f not conditions:
 #eason = mark.kwargs.get("reason", "")
 #eturn Xfail(reason, run, strict, raises)

        # If any of the conditions are true.
 #or condition in conditions:
 #esult, reason = evaluate_condition(item, mark, condition)
 #f result:
 #eturn Xfail(reason, run, strict, raises)

 #eturn None


# Whether skipped due to skip or skipif marks.
skipped_by_mark_key = StoreKey[bool]()
# Saves the xfail mark evaluation. Can be refreshed during call if None.
xfailed_key = StoreKey[Optional[Xfail]]()
unexpectedsuccess_key = StoreKey[str]()


@hookimpl(tryfirst=True)
def pytest_runtest_setup(item: Item) -> None:
 #kipped = evaluate_skip_marks(item)
 #tem._store[skipped_by_mark_key] = skipped is not None
 #f skipped:
 #kip(skipped.reason)

 #tem._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)
 #f xfailed and not item.config.option.runxfail and not xfailed.run:
 #fail("[NOTRUN] " + xfailed.reason)


@hookimpl(hookwrapper=True)
def pytest_runtest_call(item: Item) -> Generator[None, None, None]:
 #failed = item._store.get(xfailed_key, None)
 #f xfailed is None:
 #tem._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)

 #f xfailed and not item.config.option.runxfail and not xfailed.run:
 #fail("[NOTRUN] " + xfailed.reason)

 #ield

    # The test run may have added an xfail mark dynamically.
 #failed = item._store.get(xfailed_key, None)
 #f xfailed is None:
 #tem._store[xfailed_key] = xfailed = evaluate_xfail_marks(item)


@hookimpl(hookwrapper=True)
def pytest_runtest_makereport(item: Item, call: CallInfo[None]):
 #utcome = yield
 #ep = outcome.get_result()
 #failed = item._store.get(xfailed_key, None)
    # unittest special case, see setting of unexpectedsuccess_key
 #f unexpectedsuccess_key in item._store and rep.when == "call":
 #eason = item._store[unexpectedsuccess_key]
 #f reason:
 #ep.longrepr = f"Unexpected success: {reason}"
 #lse:
 #ep.longrepr = "Unexpected success"
 #ep.outcome = "failed"
 #lif item.config.option.runxfail:
 #ass  # don't interfere
 #lif call.excinfo and isinstance(call.excinfo.value, xfail.Exception):
 #ssert call.excinfo.value.msg is not None
 #ep.wasxfail = "reason: " + call.excinfo.value.msg
 #ep.outcome = "skipped"
 #lif not rep.skipped and xfailed:
 #f call.excinfo:
 #aises = xfailed.raises
 #f raises is not None and not isinstance(call.excinfo.value, raises):
 #ep.outcome = "failed"
 #lse:
 #ep.outcome = "skipped"
 #ep.wasxfail = xfailed.reason
 #lif call.when == "call":
 #f xfailed.strict:
 #ep.outcome = "failed"
 #ep.longrepr = "[XPASS(strict)] " + xfailed.reason
 #lse:
 #ep.outcome = "passed"
 #ep.wasxfail = xfailed.reason

 #f (
 #tem._store.get(skipped_by_mark_key, True)
 #nd rep.skipped
 #nd type(rep.longrepr) is tuple
 #:
        # Skipped by mark.skipif; change the location of the failure
        # to point to the item definition, otherwise it will display
        # the location of where the skip exception was raised within pytest.
 #, _, reason = rep.longrepr
 #ilename, line = item.reportinfo()[:2]
 #ssert line is not None
 #ep.longrepr = str(filename), line + 1, reason


def pytest_report_teststatus(report: BaseReport) -> Optional[Tuple[str, str, str]]:
 #f hasattr(report, "wasxfail"):
 #f report.skipped:
 #eturn "xfailed", "x", "XFAIL"
 #lif report.passed:
 #eturn "xpassed", "X", "XPASS"
 #eturn None
