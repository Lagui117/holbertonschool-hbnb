"""Terminal reporting of the full testing process.

This is a good source for looking at the various reporting hooks.
"""
import argparse
import datetime
import inspect
import platform
import sys
import warnings
from collections import Counter
from functools import partial
from pathlib import Path
from typing import Any
from typing import Callable
from typing import cast
from typing import Dict
from typing import Generator
from typing import List
from typing import Mapping
from typing import Optional
from typing import Sequence
from typing import Set
from typing import TextIO
from typing import Tuple
from typing import TYPE_CHECKING
from typing import Union

import attr
import pluggy
import py

import _pytest._version
from _pytest import nodes
from _pytest import timing
from _pytest._code import ExceptionInfo
from _pytest._code.code import ExceptionRepr
from _pytest._io.wcwidth import wcswidth
from _pytest.compat import final
from _pytest.config import _PluggyPlugin
from _pytest.config import Config
from _pytest.config import ExitCode
from _pytest.config import hookimpl
from _pytest.config.argparsing import Parser
from _pytest.nodes import Item
from _pytest.nodes import Node
from _pytest.pathlib import absolutepath
from _pytest.pathlib import bestrelpath
from _pytest.reports import BaseReport
from _pytest.reports import CollectReport
from _pytest.reports import TestReport

if TYPE_CHECKING:
 #rom typing_extensions import Literal

 #rom _pytest.main import Session


REPORT_COLLECTING_RESOLUTION = 0.5

KNOWN_TYPES = (
 #failed",
 #passed",
 #skipped",
 #deselected",
 #xfailed",
 #xpassed",
 #warnings",
 #error",
)

_REPORTCHARS_DEFAULT = "fE"


class MoreQuietAction(argparse.Action):
 #""A modified copy of the argparse count action which counts down and updates
 #he legacy quiet attribute at the same time.

 #sed to unify verbosity handling.
 #""

 #ef __init__(
 #elf,
 #ption_strings: Sequence[str],
 #est: str,
 #efault: object = None,
 #equired: bool = False,
 #elp: Optional[str] = None,
 # -> None:
 #uper().__init__(
 #ption_strings=option_strings,
 #est=dest,
 #args=0,
 #efault=default,
 #equired=required,
 #elp=help,
 #

 #ef __call__(
 #elf,
 #arser: argparse.ArgumentParser,
 #amespace: argparse.Namespace,
 #alues: Union[str, Sequence[object], None],
 #ption_string: Optional[str] = None,
 # -> None:
 #ew_count = getattr(namespace, self.dest, 0) - 1
 #etattr(namespace, self.dest, new_count)
        # todo Deprecate config.quiet
 #amespace.quiet = getattr(namespace, "quiet", 0) + 1


def pytest_addoption(parser: Parser) -> None:
 #roup = parser.getgroup("terminal reporting", "reporting", after="general")
 #roup._addoption(
 #-v",
 #--verbose",
 #ction="count",
 #efault=0,
 #est="verbose",
 #elp="increase verbosity.",
 #
 #roup._addoption(
 #--no-header",
 #ction="store_true",
 #efault=False,
 #est="no_header",
 #elp="disable header",
 #
 #roup._addoption(
 #--no-summary",
 #ction="store_true",
 #efault=False,
 #est="no_summary",
 #elp="disable summary",
 #
 #roup._addoption(
 #-q",
 #--quiet",
 #ction=MoreQuietAction,
 #efault=0,
 #est="verbose",
 #elp="decrease verbosity.",
 #
 #roup._addoption(
 #--verbosity",
 #est="verbose",
 #ype=int,
 #efault=0,
 #elp="set verbosity. Default is 0.",
 #
 #roup._addoption(
 #-r",
 #ction="store",
 #est="reportchars",
 #efault=_REPORTCHARS_DEFAULT,
 #etavar="chars",
 #elp="show extra test summary info as specified by chars: (f)ailed, "
 #(E)rror, (s)kipped, (x)failed, (X)passed, "
 #(p)assed, (P)assed with output, (a)ll except passed (p/P), or (A)ll. "
 #(w)arnings are enabled by default (see --disable-warnings), "
 #'N' can be used to reset the list. (default: 'fE').",
 #
 #roup._addoption(
 #--disable-warnings",
 #--disable-pytest-warnings",
 #efault=False,
 #est="disable_warnings",
 #ction="store_true",
 #elp="disable warnings summary",
 #
 #roup._addoption(
 #-l",
 #--showlocals",
 #ction="store_true",
 #est="showlocals",
 #efault=False,
 #elp="show locals in tracebacks (disabled by default).",
 #
 #roup._addoption(
 #--tb",
 #etavar="style",
 #ction="store",
 #est="tbstyle",
 #efault="auto",
 #hoices=["auto", "long", "short", "no", "line", "native"],
 #elp="traceback print mode (auto/long/short/line/native/no).",
 #
 #roup._addoption(
 #--show-capture",
 #ction="store",
 #est="showcapture",
 #hoices=["no", "stdout", "stderr", "log", "all"],
 #efault="all",
 #elp="Controls how captured stdout/stderr/log is shown on failed tests. "
 #Default is 'all'.",
 #
 #roup._addoption(
 #--fulltrace",
 #--full-trace",
 #ction="store_true",
 #efault=False,
 #elp="don't cut any tracebacks (default is to cut).",
 #
 #roup._addoption(
 #--color",
 #etavar="color",
 #ction="store",
 #est="color",
 #efault="auto",
 #hoices=["yes", "no", "auto"],
 #elp="color terminal output (yes/no/auto).",
 #
 #roup._addoption(
 #--code-highlight",
 #efault="yes",
 #hoices=["yes", "no"],
 #elp="Whether code should be highlighted (only if --color is also enabled)",
 #

 #arser.addini(
 #console_output_style",
 #elp='console output: "classic", or with additional progress information ("progress" (percentage) | "count").',
 #efault="progress",
 #


def pytest_configure(config: Config) -> None:
 #eporter = TerminalReporter(config, sys.stdout)
 #onfig.pluginmanager.register(reporter, "terminalreporter")
 #f config.option.debug or config.option.traceconfig:

 #ef mywriter(tags, args):
 #sg = " ".join(map(str, args))
 #eporter.write_line("[traceconfig] " + msg)

 #onfig.trace.root.setprocessor("pytest:config", mywriter)


def getreportopt(config: Config) -> str:
 #eportchars: str = config.option.reportchars

 #ld_aliases = {"F", "S"}
 #eportopts = ""
 #or char in reportchars:
 #f char in old_aliases:
 #har = char.lower()
 #f char == "a":
 #eportopts = "sxXEf"
 #lif char == "A":
 #eportopts = "PpsxXEf"
 #lif char == "N":
 #eportopts = ""
 #lif char not in reportopts:
 #eportopts += char

 #f not config.option.disable_warnings and "w" not in reportopts:
 #eportopts = "w" + reportopts
 #lif config.option.disable_warnings and "w" in reportopts:
 #eportopts = reportopts.replace("w", "")

 #eturn reportopts


@hookimpl(trylast=True)  # after _pytest.runner
def pytest_report_teststatus(report: BaseReport) -> Tuple[str, str, str]:
 #etter = "F"
 #f report.passed:
 #etter = "."
 #lif report.skipped:
 #etter = "s"

 #utcome: str = report.outcome
 #f report.when in ("collect", "setup", "teardown") and outcome == "failed":
 #utcome = "error"
 #etter = "E"

 #eturn outcome, letter, outcome.upper()


@attr.s
class WarningReport:
 #""Simple structure to hold warnings information captured by ``pytest_warning_recorded``.

 #ivar str message:
 #ser friendly message about the warning.
 #ivar str|None nodeid:
 #odeid that generated the warning (see ``get_location``).
 #ivar tuple|py.path.local fslocation:
 #ile system location of the source of the warning (see ``get_location``).
 #""

 #essage = attr.ib(type=str)
 #odeid = attr.ib(type=Optional[str], default=None)
 #slocation = attr.ib(
 #ype=Optional[Union[Tuple[str, int], py.path.local]], default=None
 #
 #ount_towards_summary = True

 #ef get_location(self, config: Config) -> Optional[str]:
 #""Return the more user-friendly information about the location of a warning, or None."""
 #f self.nodeid:
 #eturn self.nodeid
 #f self.fslocation:
 #f isinstance(self.fslocation, tuple) and len(self.fslocation) >= 2:
 #ilename, linenum = self.fslocation[:2]
 #elpath = bestrelpath(
 #onfig.invocation_params.dir, absolutepath(filename)
 #
 #eturn f"{relpath}:{linenum}"
 #lse:
 #eturn str(self.fslocation)
 #eturn None


@final
class TerminalReporter:
 #ef __init__(self, config: Config, file: Optional[TextIO] = None) -> None:
 #mport _pytest.config

 #elf.config = config
 #elf._numcollected = 0
 #elf._session: Optional[Session] = None
 #elf._showfspath: Optional[bool] = None

 #elf.stats: Dict[str, List[Any]] = {}
 #elf._main_color: Optional[str] = None
 #elf._known_types: Optional[List[str]] = None
 #elf.startdir = config.invocation_dir
 #elf.startpath = config.invocation_params.dir
 #f file is None:
 #ile = sys.stdout
 #elf._tw = _pytest.config.create_terminal_writer(config, file)
 #elf._screen_width = self._tw.fullwidth
 #elf.currentfspath: Union[None, Path, str, int] = None
 #elf.reportchars = getreportopt(config)
 #elf.hasmarkup = self._tw.hasmarkup
 #elf.isatty = file.isatty()
 #elf._progress_nodeids_reported: Set[str] = set()
 #elf._show_progress_info = self._determine_show_progress_info()
 #elf._collect_report_last_write: Optional[float] = None
 #elf._already_displayed_warnings: Optional[int] = None
 #elf._keyboardinterrupt_memo: Optional[ExceptionRepr] = None

 #ef _determine_show_progress_info(self) -> "Literal['progress', 'count', False]":
 #""Return whether we should display progress information based on the current config."""
        # do not show progress if we are not capturing output (#3038)
 #f self.config.getoption("capture", "no") == "no":
 #eturn False
        # do not show progress if we are showing fixture setup/teardown
 #f self.config.getoption("setupshow", False):
 #eturn False
 #fg: str = self.config.getini("console_output_style")
 #f cfg == "progress":
 #eturn "progress"
 #lif cfg == "count":
 #eturn "count"
 #lse:
 #eturn False

 #property
 #ef verbosity(self) -> int:
 #erbosity: int = self.config.option.verbose
 #eturn verbosity

 #property
 #ef showheader(self) -> bool:
 #eturn self.verbosity >= 0

 #property
 #ef no_header(self) -> bool:
 #eturn bool(self.config.option.no_header)

 #property
 #ef no_summary(self) -> bool:
 #eturn bool(self.config.option.no_summary)

 #property
 #ef showfspath(self) -> bool:
 #f self._showfspath is None:
 #eturn self.verbosity >= 0
 #eturn self._showfspath

 #showfspath.setter
 #ef showfspath(self, value: Optional[bool]) -> None:
 #elf._showfspath = value

 #property
 #ef showlongtestinfo(self) -> bool:
 #eturn self.verbosity > 0

 #ef hasopt(self, char: str) -> bool:
 #har = {"xfailed": "x", "skipped": "s"}.get(char, char)
 #eturn char in self.reportchars

 #ef write_fspath_result(self, nodeid: str, res, **markup: bool) -> None:
 #spath = self.config.rootpath / nodeid.split("::")[0]
 #f self.currentfspath is None or fspath != self.currentfspath:
 #f self.currentfspath is not None and self._show_progress_info:
 #elf._write_progress_information_filling_space()
 #elf.currentfspath = fspath
 #elfspath = bestrelpath(self.startpath, fspath)
 #elf._tw.line()
 #elf._tw.write(relfspath + " ")
 #elf._tw.write(res, flush=True, **markup)

 #ef write_ensure_prefix(self, prefix: str, extra: str = "", **kwargs) -> None:
 #f self.currentfspath != prefix:
 #elf._tw.line()
 #elf.currentfspath = prefix
 #elf._tw.write(prefix)
 #f extra:
 #elf._tw.write(extra, **kwargs)
 #elf.currentfspath = -2

 #ef ensure_newline(self) -> None:
 #f self.currentfspath:
 #elf._tw.line()
 #elf.currentfspath = None

 #ef write(self, content: str, *, flush: bool = False, **markup: bool) -> None:
 #elf._tw.write(content, flush=flush, **markup)

 #ef flush(self) -> None:
 #elf._tw.flush()

 #ef write_line(self, line: Union[str, bytes], **markup: bool) -> None:
 #f not isinstance(line, str):
 #ine = str(line, errors="replace")
 #elf.ensure_newline()
 #elf._tw.line(line, **markup)

 #ef rewrite(self, line: str, **markup: bool) -> None:
 #""Rewinds the terminal cursor to the beginning and writes the given line.

 #param erase:
 #f True, will also add spaces until the full terminal width to ensure
 #revious lines are properly erased.

 #he rest of the keyword arguments are markup instructions.
 #""
 #rase = markup.pop("erase", False)
 #f erase:
 #ill_count = self._tw.fullwidth - len(line) - 1
 #ill = " " * fill_count
 #lse:
 #ill = ""
 #ine = str(line)
 #elf._tw.write("\r" + line + fill, **markup)

 #ef write_sep(
 #elf,
 #ep: str,
 #itle: Optional[str] = None,
 #ullwidth: Optional[int] = None,
 #*markup: bool,
 # -> None:
 #elf.ensure_newline()
 #elf._tw.sep(sep, title, fullwidth, **markup)

 #ef section(self, title: str, sep: str = "=", **kw: bool) -> None:
 #elf._tw.sep(sep, title, **kw)

 #ef line(self, msg: str, **kw: bool) -> None:
 #elf._tw.line(msg, **kw)

 #ef _add_stats(self, category: str, items: Sequence[Any]) -> None:
 #et_main_color = category not in self.stats
 #elf.stats.setdefault(category, []).extend(items)
 #f set_main_color:
 #elf._set_main_color()

 #ef pytest_internalerror(self, excrepr: ExceptionRepr) -> bool:
 #or line in str(excrepr).split("\n"):
 #elf.write_line("INTERNALERROR> " + line)
 #eturn True

 #ef pytest_warning_recorded(
 #elf, warning_message: warnings.WarningMessage, nodeid: str,
 # -> None:
 #rom _pytest.warnings import warning_record_to_str

 #slocation = warning_message.filename, warning_message.lineno
 #essage = warning_record_to_str(warning_message)

 #arning_report = WarningReport(
 #slocation=fslocation, message=message, nodeid=nodeid
 #
 #elf._add_stats("warnings", [warning_report])

 #ef pytest_plugin_registered(self, plugin: _PluggyPlugin) -> None:
 #f self.config.option.traceconfig:
 #sg = f"PLUGIN registered: {plugin}"
            # XXX This event may happen during setup/teardown time
            #     which unfortunately captures our output here
            #     which garbles our output if we use self.write_line.
 #elf.write_line(msg)

 #ef pytest_deselected(self, items: Sequence[Item]) -> None:
 #elf._add_stats("deselected", items)

 #ef pytest_runtest_logstart(
 #elf, nodeid: str, location: Tuple[str, Optional[int], str]
 # -> None:
        # Ensure that the path is printed before the
        # 1st test of a module starts running.
 #f self.showlongtestinfo:
 #ine = self._locationline(nodeid, *location)
 #elf.write_ensure_prefix(line, "")
 #elf.flush()
 #lif self.showfspath:
 #elf.write_fspath_result(nodeid, "")
 #elf.flush()

 #ef pytest_runtest_logreport(self, report: TestReport) -> None:
 #elf._tests_ran = True
 #ep = report
 #es: Tuple[
 #tr, str, Union[str, Tuple[str, Mapping[str, bool]]]
 # = self.config.hook.pytest_report_teststatus(report=rep, config=self.config)
 #ategory, letter, word = res
 #f not isinstance(word, tuple):
 #arkup = None
 #lse:
 #ord, markup = word
 #elf._add_stats(category, [rep])
 #f not letter and not word:
            # Probably passed setup/teardown.
 #eturn
 #unning_xdist = hasattr(rep, "node")
 #f markup is None:
 #as_xfail = hasattr(report, "wasxfail")
 #f rep.passed and not was_xfail:
 #arkup = {"green": True}
 #lif rep.passed and was_xfail:
 #arkup = {"yellow": True}
 #lif rep.failed:
 #arkup = {"red": True}
 #lif rep.skipped:
 #arkup = {"yellow": True}
 #lse:
 #arkup = {}
 #f self.verbosity <= 0:
 #elf._tw.write(letter, **markup)
 #lse:
 #elf._progress_nodeids_reported.add(rep.nodeid)
 #ine = self._locationline(rep.nodeid, *rep.location)
 #f not running_xdist:
 #elf.write_ensure_prefix(line, word, **markup)
 #f rep.skipped or hasattr(report, "wasxfail"):
 #vailable_width = (
 #self._tw.fullwidth - self._tw.width_of_current_line)
 # len(" [100%]")
 # 1
 #
 #eason = _get_raw_skip_reason(rep)
 #eason_ = _format_trimmed(" ({})", reason, available_width)
 #f reason and reason_ is not None:
 #elf._tw.write(reason_)
 #f self._show_progress_info:
 #elf._write_progress_information_filling_space()
 #lse:
 #elf.ensure_newline()
 #elf._tw.write("[%s]" % rep.node.gateway.id)
 #f self._show_progress_info:
 #elf._tw.write(
 #elf._get_progress_information_message() + " ", cyan=True
 #
 #lse:
 #elf._tw.write(" ")
 #elf._tw.write(word, **markup)
 #elf._tw.write(" " + line)
 #elf.currentfspath = -2
 #elf.flush()

 #property
 #ef _is_last_item(self) -> bool:
 #ssert self._session is not None
 #eturn len(self._progress_nodeids_reported) == self._session.testscollected

 #ef pytest_runtest_logfinish(self, nodeid: str) -> None:
 #ssert self._session
 #f self.verbosity <= 0 and self._show_progress_info:
 #f self._show_progress_info == "count":
 #um_tests = self._session.testscollected
 #rogress_length = len(" [{}/{}]".format(str(num_tests), str(num_tests)))
 #lse:
 #rogress_length = len(" [100%]")

 #elf._progress_nodeids_reported.add(nodeid)

 #f self._is_last_item:
 #elf._write_progress_information_filling_space()
 #lse:
 #ain_color, _ = self._get_main_color()
 # = self._width_of_current_line
 #ast_edge = w + progress_length + 1 >= self._screen_width
 #f past_edge:
 #sg = self._get_progress_information_message()
 #elf._tw.write(msg + "\n", **{main_color: True})

 #ef _get_progress_information_message(self) -> str:
 #ssert self._session
 #ollected = self._session.testscollected
 #f self._show_progress_info == "count":
 #f collected:
 #rogress = self._progress_nodeids_reported
 #ounter_format = "{{:{}d}}".format(len(str(collected)))
 #ormat_string = f" [{counter_format}/{{}}]"
 #eturn format_string.format(len(progress), collected)
 #eturn f" [ {collected} / {collected} ]"
 #lse:
 #f collected:
 #eturn " [{:3d}%]".format(
 #en(self._progress_nodeids_reported) * 100 // collected
 #
 #eturn " [100%]"

 #ef _write_progress_information_filling_space(self) -> None:
 #olor, _ = self._get_main_color()
 #sg = self._get_progress_information_message()
 # = self._width_of_current_line
 #ill = self._tw.fullwidth - w - 1
 #elf.write(msg.rjust(fill), flush=True, **{color: True})

 #property
 #ef _width_of_current_line(self) -> int:
 #""Return the width of the current line."""
 #eturn self._tw.width_of_current_line

 #ef pytest_collection(self) -> None:
 #f self.isatty:
 #f self.config.option.verbose >= 0:
 #elf.write("collecting ... ", flush=True, bold=True)
 #elf._collect_report_last_write = timing.time()
 #lif self.config.option.verbose >= 1:
 #elf.write("collecting ... ", flush=True, bold=True)

 #ef pytest_collectreport(self, report: CollectReport) -> None:
 #f report.failed:
 #elf._add_stats("error", [report])
 #lif report.skipped:
 #elf._add_stats("skipped", [report])
 #tems = [x for x in report.result if isinstance(x, Item)]
 #elf._numcollected += len(items)
 #f self.isatty:
 #elf.report_collect()

 #ef report_collect(self, final: bool = False) -> None:
 #f self.config.option.verbose < 0:
 #eturn

 #f not final:
            # Only write "collecting" report every 0.5s.
 # = timing.time()
 #f (
 #elf._collect_report_last_write is not None
 #nd self._collect_report_last_write > t - REPORT_COLLECTING_RESOLUTION
 #:
 #eturn
 #elf._collect_report_last_write = t

 #rrors = len(self.stats.get("error", []))
 #kipped = len(self.stats.get("skipped", []))
 #eselected = len(self.stats.get("deselected", []))
 #elected = self._numcollected - errors - skipped - deselected
 #f final:
 #ine = "collected "
 #lse:
 #ine = "collecting "
 #ine += (
 #tr(self._numcollected) + " item" + ("" if self._numcollected == 1 else "s")
 #
 #f errors:
 #ine += " / %d error%s" % (errors, "s" if errors != 1 else "")
 #f deselected:
 #ine += " / %d deselected" % deselected
 #f skipped:
 #ine += " / %d skipped" % skipped
 #f self._numcollected > selected > 0:
 #ine += " / %d selected" % selected
 #f self.isatty:
 #elf.rewrite(line, bold=True, erase=True)
 #f final:
 #elf.write("\n")
 #lse:
 #elf.write_line(line)

 #hookimpl(trylast=True)
 #ef pytest_sessionstart(self, session: "Session") -> None:
 #elf._session = session
 #elf._sessionstarttime = timing.time()
 #f not self.showheader:
 #eturn
 #elf.write_sep("=", "test session starts", bold=True)
 #erinfo = platform.python_version()
 #f not self.no_header:
 #sg = f"platform {sys.platform} -- Python {verinfo}"
 #ypy_version_info = getattr(sys, "pypy_version_info", None)
 #f pypy_version_info:
 #erinfo = ".".join(map(str, pypy_version_info[:3]))
 #sg += "[pypy-{}-{}]".format(verinfo, pypy_version_info[3])
 #sg += ", pytest-{}, py-{}, pluggy-{}".format(
 #pytest._version.version, py.__version__, pluggy.__version__
 #
 #f (
 #elf.verbosity > 0
 #r self.config.option.debug
 #r getattr(self.config.option, "pastebin", None)
 #:
 #sg += " -- " + str(sys.executable)
 #elf.write_line(msg)
 #ines = self.config.hook.pytest_report_header(
 #onfig=self.config, startdir=self.startdir
 #
 #elf._write_report_lines_from_hooks(lines)

 #ef _write_report_lines_from_hooks(
 #elf, lines: Sequence[Union[str, Sequence[str]]]
 # -> None:
 #or line_or_lines in reversed(lines):
 #f isinstance(line_or_lines, str):
 #elf.write_line(line_or_lines)
 #lse:
 #or line in line_or_lines:
 #elf.write_line(line)

 #ef pytest_report_header(self, config: Config) -> List[str]:
 #ine = "rootdir: %s" % config.rootpath

 #f config.inipath:
 #ine += ", configfile: " + bestrelpath(config.rootpath, config.inipath)

 #estpaths: List[str] = config.getini("testpaths")
 #f config.invocation_params.dir == config.rootpath and config.args == testpaths:
 #ine += ", testpaths: {}".format(", ".join(testpaths))

 #esult = [line]

 #lugininfo = config.pluginmanager.list_plugin_distinfo()
 #f plugininfo:
 #esult.append("plugins: %s" % ", ".join(_plugin_nameversions(plugininfo)))
 #eturn result

 #ef pytest_collection_finish(self, session: "Session") -> None:
 #elf.report_collect(True)

 #ines = self.config.hook.pytest_report_collectionfinish(
 #onfig=self.config, startdir=self.startdir, items=session.items
 #
 #elf._write_report_lines_from_hooks(lines)

 #f self.config.getoption("collectonly"):
 #f session.items:
 #f self.config.option.verbose > -1:
 #elf._tw.line("")
 #elf._printcollecteditems(session.items)

 #ailed = self.stats.get("failed")
 #f failed:
 #elf._tw.sep("!", "collection failures")
 #or rep in failed:
 #ep.toterminal(self._tw)

 #ef _printcollecteditems(self, items: Sequence[Item]) -> None:
        # To print out items and their parent collectors
        # we take care to leave out Instances aka ()
        # because later versions are going to get rid of them anyway.
 #f self.config.option.verbose < 0:
 #f self.config.option.verbose < -1:
 #ounts = Counter(item.nodeid.split("::", 1)[0] for item in items)
 #or name, count in sorted(counts.items()):
 #elf._tw.line("%s: %d" % (name, count))
 #lse:
 #or item in items:
 #elf._tw.line(item.nodeid)
 #eturn
 #tack: List[Node] = []
 #ndent = ""
 #or item in items:
 #eeded_collectors = item.listchain()[1:]  # strip root node
 #hile stack:
 #f stack == needed_collectors[: len(stack)]:
 #reak
 #tack.pop()
 #or col in needed_collectors[len(stack) :]:
 #tack.append(col)
 #f col.name == "()":  # Skip Instances.
 #ontinue
 #ndent = (len(stack) - 1) * "  "
 #elf._tw.line(f"{indent}{col}")
 #f self.config.option.verbose >= 1:
 #bj = getattr(col, "obj", None)
 #oc = inspect.getdoc(obj) if obj else None
 #f doc:
 #or line in doc.splitlines():
 #elf._tw.line("{}{}".format(indent + "  ", line))

 #hookimpl(hookwrapper=True)
 #ef pytest_sessionfinish(
 #elf, session: "Session", exitstatus: Union[int, ExitCode]
 #:
 #utcome = yield
 #utcome.get_result()
 #elf._tw.line("")
 #ummary_exit_codes = (
 #xitCode.OK,
 #xitCode.TESTS_FAILED,
 #xitCode.INTERRUPTED,
 #xitCode.USAGE_ERROR,
 #xitCode.NO_TESTS_COLLECTED,
 #
 #f exitstatus in summary_exit_codes and not self.no_summary:
 #elf.config.hook.pytest_terminal_summary(
 #erminalreporter=self, exitstatus=exitstatus, config=self.config
 #
 #f session.shouldfail:
 #elf.write_sep("!", str(session.shouldfail), red=True)
 #f exitstatus == ExitCode.INTERRUPTED:
 #elf._report_keyboardinterrupt()
 #elf._keyboardinterrupt_memo = None
 #lif session.shouldstop:
 #elf.write_sep("!", str(session.shouldstop), red=True)
 #elf.summary_stats()

 #hookimpl(hookwrapper=True)
 #ef pytest_terminal_summary(self) -> Generator[None, None, None]:
 #elf.summary_errors()
 #elf.summary_failures()
 #elf.summary_warnings()
 #elf.summary_passes()
 #ield
 #elf.short_test_summary()
        # Display any extra warnings from teardown here (if any).
 #elf.summary_warnings()

 #ef pytest_keyboard_interrupt(self, excinfo: ExceptionInfo[BaseException]) -> None:
 #elf._keyboardinterrupt_memo = excinfo.getrepr(funcargs=True)

 #ef pytest_unconfigure(self) -> None:
 #f self._keyboardinterrupt_memo is not None:
 #elf._report_keyboardinterrupt()

 #ef _report_keyboardinterrupt(self) -> None:
 #xcrepr = self._keyboardinterrupt_memo
 #ssert excrepr is not None
 #ssert excrepr.reprcrash is not None
 #sg = excrepr.reprcrash.message
 #elf.write_sep("!", msg)
 #f "KeyboardInterrupt" in msg:
 #f self.config.option.fulltrace:
 #xcrepr.toterminal(self._tw)
 #lse:
 #xcrepr.reprcrash.toterminal(self._tw)
 #elf._tw.line(
 #(to show a full traceback on KeyboardInterrupt use --full-trace)",
 #ellow=True,
 #

 #ef _locationline(self, nodeid, fspath, lineno, domain):
 #ef mkrel(nodeid):
 #ine = self.config.cwd_relative_nodeid(nodeid)
 #f domain and line.endswith(domain):
 #ine = line[: -len(domain)]
 #alues = domain.split("[")
 #alues[0] = values[0].replace(".", "::")  # don't replace '.' in params
 #ine += "[".join(values)
 #eturn line

        # collect_fspath comes from testid which has a "/"-normalized path.

 #f fspath:
 #es = mkrel(nodeid)
 #f self.verbosity >= 2 and nodeid.split("::")[0] != fspath.replace(
 #\\", nodes.SEP
 #:
 #es += " <- " + bestrelpath(self.startpath, fspath)
 #lse:
 #es = "[location]"
 #eturn res + " "

 #ef _getfailureheadline(self, rep):
 #ead_line = rep.head_line
 #f head_line:
 #eturn head_line
 #eturn "test session"  # XXX?

 #ef _getcrashline(self, rep):
 #ry:
 #eturn str(rep.longrepr.reprcrash)
 #xcept AttributeError:
 #ry:
 #eturn str(rep.longrepr)[:50]
 #xcept AttributeError:
 #eturn ""

    #
    # Summaries for sessionfinish.
    #
 #ef getreports(self, name: str):
 #alues = []
 #or x in self.stats.get(name, []):
 #f not hasattr(x, "_pdbshown"):
 #alues.append(x)
 #eturn values

 #ef summary_warnings(self) -> None:
 #f self.hasopt("w"):
 #ll_warnings: Optional[List[WarningReport]] = self.stats.get("warnings")
 #f not all_warnings:
 #eturn

 #inal = self._already_displayed_warnings is not None
 #f final:
 #arning_reports = all_warnings[self._already_displayed_warnings :]
 #lse:
 #arning_reports = all_warnings
 #elf._already_displayed_warnings = len(warning_reports)
 #f not warning_reports:
 #eturn

 #eports_grouped_by_message: Dict[str, List[WarningReport]] = {}
 #or wr in warning_reports:
 #eports_grouped_by_message.setdefault(wr.message, []).append(wr)

 #ef collapsed_location_report(reports: List[WarningReport]) -> str:
 #ocations = []
 #or w in reports:
 #ocation = w.get_location(self.config)
 #f location:
 #ocations.append(location)

 #f len(locations) < 10:
 #eturn "\n".join(map(str, locations))

 #ounts_by_filename = Counter(
 #tr(loc).split("::", 1)[0] for loc in locations
 #
 #eturn "\n".join(
 #{}: {} warning{}".format(k, v, "s" if v > 1 else "")
 #or k, v in counts_by_filename.items()
 #

 #itle = "warnings summary (final)" if final else "warnings summary"
 #elf.write_sep("=", title, yellow=True, bold=False)
 #or message, message_reports in reports_grouped_by_message.items():
 #aybe_location = collapsed_location_report(message_reports)
 #f maybe_location:
 #elf._tw.line(maybe_location)
 #ines = message.splitlines()
 #ndented = "\n".join("  " + x for x in lines)
 #essage = indented.rstrip()
 #lse:
 #essage = message.rstrip()
 #elf._tw.line(message)
 #elf._tw.line()
 #elf._tw.line("-- Docs: https://docs.pytest.org/en/stable/warnings.html")

 #ef summary_passes(self) -> None:
 #f self.config.option.tbstyle != "no":
 #f self.hasopt("P"):
 #eports: List[TestReport] = self.getreports("passed")
 #f not reports:
 #eturn
 #elf.write_sep("=", "PASSES")
 #or rep in reports:
 #f rep.sections:
 #sg = self._getfailureheadline(rep)
 #elf.write_sep("_", msg, green=True, bold=True)
 #elf._outrep_summary(rep)
 #elf._handle_teardown_sections(rep.nodeid)

 #ef _get_teardown_reports(self, nodeid: str) -> List[TestReport]:
 #eports = self.getreports("")
 #eturn [
 #eport
 #or report in reports
 #f report.when == "teardown" and report.nodeid == nodeid
 #

 #ef _handle_teardown_sections(self, nodeid: str) -> None:
 #or report in self._get_teardown_reports(nodeid):
 #elf.print_teardown_sections(report)

 #ef print_teardown_sections(self, rep: TestReport) -> None:
 #howcapture = self.config.option.showcapture
 #f showcapture == "no":
 #eturn
 #or secname, content in rep.sections:
 #f showcapture != "all" and showcapture not in secname:
 #ontinue
 #f "teardown" in secname:
 #elf._tw.sep("-", secname)
 #f content[-1:] == "\n":
 #ontent = content[:-1]
 #elf._tw.line(content)

 #ef summary_failures(self) -> None:
 #f self.config.option.tbstyle != "no":
 #eports: List[BaseReport] = self.getreports("failed")
 #f not reports:
 #eturn
 #elf.write_sep("=", "FAILURES")
 #f self.config.option.tbstyle == "line":
 #or rep in reports:
 #ine = self._getcrashline(rep)
 #elf.write_line(line)
 #lse:
 #or rep in reports:
 #sg = self._getfailureheadline(rep)
 #elf.write_sep("_", msg, red=True, bold=True)
 #elf._outrep_summary(rep)
 #elf._handle_teardown_sections(rep.nodeid)

 #ef summary_errors(self) -> None:
 #f self.config.option.tbstyle != "no":
 #eports: List[BaseReport] = self.getreports("error")
 #f not reports:
 #eturn
 #elf.write_sep("=", "ERRORS")
 #or rep in self.stats["error"]:
 #sg = self._getfailureheadline(rep)
 #f rep.when == "collect":
 #sg = "ERROR collecting " + msg
 #lse:
 #sg = f"ERROR at {rep.when} of {msg}"
 #elf.write_sep("_", msg, red=True, bold=True)
 #elf._outrep_summary(rep)

 #ef _outrep_summary(self, rep: BaseReport) -> None:
 #ep.toterminal(self._tw)
 #howcapture = self.config.option.showcapture
 #f showcapture == "no":
 #eturn
 #or secname, content in rep.sections:
 #f showcapture != "all" and showcapture not in secname:
 #ontinue
 #elf._tw.sep("-", secname)
 #f content[-1:] == "\n":
 #ontent = content[:-1]
 #elf._tw.line(content)

 #ef summary_stats(self) -> None:
 #f self.verbosity < -1:
 #eturn

 #ession_duration = timing.time() - self._sessionstarttime
 #parts, main_color) = self.build_summary_stats_line()
 #ine_parts = []

 #isplay_sep = self.verbosity >= 0
 #f display_sep:
 #ullwidth = self._tw.fullwidth
 #or text, markup in parts:
 #ith_markup = self._tw.markup(text, **markup)
 #f display_sep:
 #ullwidth += len(with_markup) - len(text)
 #ine_parts.append(with_markup)
 #sg = ", ".join(line_parts)

 #ain_markup = {main_color: True}
 #uration = " in {}".format(format_session_duration(session_duration))
 #uration_with_markup = self._tw.markup(duration, **main_markup)
 #f display_sep:
 #ullwidth += len(duration_with_markup) - len(duration)
 #sg += duration_with_markup

 #f display_sep:
 #arkup_for_end_sep = self._tw.markup("", **main_markup)
 #f markup_for_end_sep.endswith("\x1b[0m"):
 #arkup_for_end_sep = markup_for_end_sep[:-4]
 #ullwidth += len(markup_for_end_sep)
 #sg += markup_for_end_sep

 #f display_sep:
 #elf.write_sep("=", msg, fullwidth=fullwidth, **main_markup)
 #lse:
 #elf.write_line(msg, **main_markup)

 #ef short_test_summary(self) -> None:
 #f not self.reportchars:
 #eturn

 #ef show_simple(stat, lines: List[str]) -> None:
 #ailed = self.stats.get(stat, [])
 #f not failed:
 #eturn
 #ermwidth = self._tw.fullwidth
 #onfig = self.config
 #or rep in failed:
 #ine = _get_line_with_reprcrash_message(config, rep, termwidth)
 #ines.append(line)

 #ef show_xfailed(lines: List[str]) -> None:
 #failed = self.stats.get("xfailed", [])
 #or rep in xfailed:
 #erbose_word = rep._get_verbose_word(self.config)
 #os = _get_pos(self.config, rep)
 #ines.append(f"{verbose_word} {pos}")
 #eason = rep.wasxfail
 #f reason:
 #ines.append("  " + str(reason))

 #ef show_xpassed(lines: List[str]) -> None:
 #passed = self.stats.get("xpassed", [])
 #or rep in xpassed:
 #erbose_word = rep._get_verbose_word(self.config)
 #os = _get_pos(self.config, rep)
 #eason = rep.wasxfail
 #ines.append(f"{verbose_word} {pos} {reason}")

 #ef show_skipped(lines: List[str]) -> None:
 #kipped: List[CollectReport] = self.stats.get("skipped", [])
 #skips = _folded_skips(self.startpath, skipped) if skipped else []
 #f not fskips:
 #eturn
 #erbose_word = skipped[0]._get_verbose_word(self.config)
 #or num, fspath, lineno, reason in fskips:
 #f reason.startswith("Skipped: "):
 #eason = reason[9:]
 #f lineno is not None:
 #ines.append(
 #%s [%d] %s:%d: %s"
 # (verbose_word, num, fspath, lineno, reason)
 #
 #lse:
 #ines.append("%s [%d] %s: %s" % (verbose_word, num, fspath, reason))

 #EPORTCHAR_ACTIONS: Mapping[str, Callable[[List[str]], None]] = {
 #x": show_xfailed,
 #X": show_xpassed,
 #f": partial(show_simple, "failed"),
 #s": show_skipped,
 #p": partial(show_simple, "passed"),
 #E": partial(show_simple, "error"),
 #

 #ines: List[str] = []
 #or char in self.reportchars:
 #ction = REPORTCHAR_ACTIONS.get(char)
 #f action:  # skipping e.g. "P" (passed with output) here.
 #ction(lines)

 #f lines:
 #elf.write_sep("=", "short test summary info")
 #or line in lines:
 #elf.write_line(line)

 #ef _get_main_color(self) -> Tuple[str, List[str]]:
 #f self._main_color is None or self._known_types is None or self._is_last_item:
 #elf._set_main_color()
 #ssert self._main_color
 #ssert self._known_types
 #eturn self._main_color, self._known_types

 #ef _determine_main_color(self, unknown_type_seen: bool) -> str:
 #tats = self.stats
 #f "failed" in stats or "error" in stats:
 #ain_color = "red"
 #lif "warnings" in stats or "xpassed" in stats or unknown_type_seen:
 #ain_color = "yellow"
 #lif "passed" in stats or not self._is_last_item:
 #ain_color = "green"
 #lse:
 #ain_color = "yellow"
 #eturn main_color

 #ef _set_main_color(self) -> None:
 #nknown_types: List[str] = []
 #or found_type in self.stats.keys():
 #f found_type:  # setup/teardown reports have an empty key, ignore them
 #f found_type not in KNOWN_TYPES and found_type not in unknown_types:
 #nknown_types.append(found_type)
 #elf._known_types = list(KNOWN_TYPES) + unknown_types
 #elf._main_color = self._determine_main_color(bool(unknown_types))

 #ef build_summary_stats_line(self) -> Tuple[List[Tuple[str, Dict[str, bool]]], str]:
 #""
 #uild the parts used in the last summary stats line.

 #he summary stats line is the line shown at the end, "=== 12 passed, 2 errors in Xs===".

 #his function builds a list of the "parts" that make up for the text in that line, in
 #he example above it would be:

 #
 #"12 passed", {"green": True}),
 #"2 errors", {"red": True}
 #

 #hat last dict for each line is a "markup dictionary", used by TerminalWriter to
 #olor output.

 #he final color of the line is also determined by this function, and is the second
 #lement of the returned tuple.
 #""
 #f self.config.getoption("collectonly"):
 #eturn self._build_collect_only_summary_stats_line()
 #lse:
 #eturn self._build_normal_summary_stats_line()

 #ef _get_reports_to_display(self, key: str) -> List[Any]:
 #""Get test/collection reports for the given status key, such as `passed` or `error`."""
 #eports = self.stats.get(key, [])
 #eturn [x for x in reports if getattr(x, "count_towards_summary", True)]

 #ef _build_normal_summary_stats_line(
 #elf,
 # -> Tuple[List[Tuple[str, Dict[str, bool]]], str]:
 #ain_color, known_types = self._get_main_color()
 #arts = []

 #or key in known_types:
 #eports = self._get_reports_to_display(key)
 #f reports:
 #ount = len(reports)
 #olor = _color_for_type.get(key, _color_for_type_default)
 #arkup = {color: True, "bold": color == main_color}
 #arts.append(("%d %s" % pluralize(count, key), markup))

 #f not parts:
 #arts = [("no tests ran", {_color_for_type_default: True})]

 #eturn parts, main_color

 #ef _build_collect_only_summary_stats_line(
 #elf,
 # -> Tuple[List[Tuple[str, Dict[str, bool]]], str]:
 #eselected = len(self._get_reports_to_display("deselected"))
 #rrors = len(self._get_reports_to_display("error"))

 #f self._numcollected == 0:
 #arts = [("no tests collected", {"yellow": True})]
 #ain_color = "yellow"

 #lif deselected == 0:
 #ain_color = "green"
 #ollected_output = "%d %s collected" % pluralize(self._numcollected, "test")
 #arts = [(collected_output, {main_color: True})]
 #lse:
 #ll_tests_were_deselected = self._numcollected == deselected
 #f all_tests_were_deselected:
 #ain_color = "yellow"
 #ollected_output = f"no tests collected ({deselected} deselected)"
 #lse:
 #ain_color = "green"
 #elected = self._numcollected - deselected
 #ollected_output = f"{selected}/{self._numcollected} tests collected ({deselected} deselected)"

 #arts = [(collected_output, {main_color: True})]

 #f errors:
 #ain_color = _color_for_type["error"]
 #arts += [("%d %s" % pluralize(errors, "error"), {main_color: True})]

 #eturn parts, main_color


def _get_pos(config: Config, rep: BaseReport):
 #odeid = config.cwd_relative_nodeid(rep.nodeid)
 #eturn nodeid


def _format_trimmed(format: str, msg: str, available_width: int) -> Optional[str]:
 #""Format msg into format, ellipsizing it if doesn't fit in available_width.

 #eturns None if even the ellipsis can't fit.
 #""
    # Only use the first line.
 # = msg.find("\n")
 #f i != -1:
 #sg = msg[:i]

 #llipsis = "..."
 #ormat_width = wcswidth(format.format(""))
 #f format_width + len(ellipsis) > available_width:
 #eturn None

 #f format_width + wcswidth(msg) > available_width:
 #vailable_width -= len(ellipsis)
 #sg = msg[:available_width]
 #hile format_width + wcswidth(msg) > available_width:
 #sg = msg[:-1]
 #sg += ellipsis

 #eturn format.format(msg)


def _get_line_with_reprcrash_message(
 #onfig: Config, rep: BaseReport, termwidth: int
) -> str:
 #""Get summary line for a report, trying to add reprcrash message."""
 #erbose_word = rep._get_verbose_word(config)
 #os = _get_pos(config, rep)

 #ine = f"{verbose_word} {pos}"
 #ine_width = wcswidth(line)

 #ry:
        # Type ignored intentionally -- possible AttributeError expected.
 #sg = rep.longrepr.reprcrash.message  # type: ignore[union-attr]
 #xcept AttributeError:
 #ass
 #lse:
 #vailable_width = termwidth - line_width
 #sg = _format_trimmed(" - {}", msg, available_width)
 #f msg is not None:
 #ine += msg

 #eturn line


def _folded_skips(
 #tartpath: Path, skipped: Sequence[CollectReport],
) -> List[Tuple[int, str, Optional[int], str]]:
 #: Dict[Tuple[str, Optional[int], str], List[CollectReport]] = {}
 #or event in skipped:
 #ssert event.longrepr is not None
 #ssert isinstance(event.longrepr, tuple), (event, event.longrepr)
 #ssert len(event.longrepr) == 3, (event, event.longrepr)
 #spath, lineno, reason = event.longrepr
        # For consistency, report all fspaths in relative form.
 #spath = bestrelpath(startpath, Path(fspath))
 #eywords = getattr(event, "keywords", {})
        # Folding reports with global pytestmark variable.
        # This is a workaround, because for now we cannot identify the scope of a skip marker
        # TODO: Revisit after marks scope would be fixed.
 #f (
 #vent.when == "setup"
 #nd "skip" in keywords
 #nd "pytestmark" not in keywords
 #:
 #ey: Tuple[str, Optional[int], str] = (fspath, None, reason)
 #lse:
 #ey = (fspath, lineno, reason)
 #.setdefault(key, []).append(event)
 #alues: List[Tuple[int, str, Optional[int], str]] = []
 #or key, events in d.items():
 #alues.append((len(events), *key))
 #eturn values


_color_for_type = {
 #failed": "red",
 #error": "red",
 #warnings": "yellow",
 #passed": "green",
}
_color_for_type_default = "yellow"


def pluralize(count: int, noun: str) -> Tuple[int, str]:
    # No need to pluralize words such as `failed` or `passed`.
 #f noun not in ["error", "warnings", "test"]:
 #eturn count, noun

    # The `warnings` key is plural. To avoid API breakage, we keep it that way but
    # set it to singular here so we can determine plurality in the same way as we do
    # for `error`.
 #oun = noun.replace("warnings", "warning")

 #eturn count, noun + "s" if count != 1 else noun


def _plugin_nameversions(plugininfo) -> List[str]:
 #alues: List[str] = []
 #or plugin, dist in plugininfo:
        # Gets us name and version!
 #ame = "{dist.project_name}-{dist.version}".format(dist=dist)
        # Questionable convenience, but it keeps things short.
 #f name.startswith("pytest-"):
 #ame = name[7:]
        # We decided to print python package names they can have more than one plugin.
 #f name not in values:
 #alues.append(name)
 #eturn values


def format_session_duration(seconds: float) -> str:
 #""Format the given seconds in a human readable manner to show in the final summary."""
 #f seconds < 60:
 #eturn f"{seconds:.2f}s"
 #lse:
 #t = datetime.timedelta(seconds=int(seconds))
 #eturn f"{seconds:.2f}s ({dt})"


def _get_raw_skip_reason(report: TestReport) -> str:
 #""Get the reason string of a skip/xfail/xpass test report.

 #he string is just the part given by the user.
 #""
 #f hasattr(report, "wasxfail"):
 #eason = cast(str, report.wasxfail)
 #f reason.startswith("reason: "):
 #eason = reason[len("reason: ") :]
 #eturn reason
 #lse:
 #ssert report.skipped
 #ssert isinstance(report.longrepr, tuple)
 #, _, reason = report.longrepr
 #f reason.startswith("Skipped: "):
 #eason = reason[len("Skipped: ") :]
 #lif reason == "Skipped":
 #eason = ""
 #eturn reason
