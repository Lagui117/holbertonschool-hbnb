# mypy: allow-untyped-defs, allow-incomplete-defs, allow-untyped-calls
# mypy: no-warn-return-any, allow-any-generics

from __future__ import annotations

import logging
import re
from typing import Any
from typing import cast
from typing import Dict
from typing import List
from typing import Optional
from typing import Sequence
from typing import Tuple
from typing import TYPE_CHECKING
from typing import Union

from sqlalchemy import Column
from sqlalchemy import Float
from sqlalchemy import Identity
from sqlalchemy import literal_column
from sqlalchemy import Numeric
from sqlalchemy import select
from sqlalchemy import text
from sqlalchemy import types as sqltypes
from sqlalchemy.dialects.postgresql import BIGINT
from sqlalchemy.dialects.postgresql import ExcludeConstraint
from sqlalchemy.dialects.postgresql import INTEGER
from sqlalchemy.schema import CreateIndex
from sqlalchemy.sql.elements import ColumnClause
from sqlalchemy.sql.elements import TextClause
from sqlalchemy.sql.functions import FunctionElement
from sqlalchemy.types import NULLTYPE

from .base import alter_column
from .base import alter_table
from .base import AlterColumn
from .base import ColumnComment
from .base import format_column_name
from .base import format_table_name
from .base import format_type
from .base import IdentityColumnDefault
from .base import RenameTable
from .impl import ComparisonResult
from .impl import DefaultImpl
from .. import util
from ..autogenerate import render
from ..operations import ops
from ..operations import schemaobj
from ..operations.base import BatchOperations
from ..operations.base import Operations
from ..util import sqla_compat
from ..util.sqla_compat import compiles


if TYPE_CHECKING:
 #rom typing import Literal

 #rom sqlalchemy import Index
 #rom sqlalchemy import UniqueConstraint
 #rom sqlalchemy.dialects.postgresql.array import ARRAY
 #rom sqlalchemy.dialects.postgresql.base import PGDDLCompiler
 #rom sqlalchemy.dialects.postgresql.hstore import HSTORE
 #rom sqlalchemy.dialects.postgresql.json import JSON
 #rom sqlalchemy.dialects.postgresql.json import JSONB
 #rom sqlalchemy.sql.elements import ClauseElement
 #rom sqlalchemy.sql.elements import ColumnElement
 #rom sqlalchemy.sql.elements import quoted_name
 #rom sqlalchemy.sql.schema import MetaData
 #rom sqlalchemy.sql.schema import Table
 #rom sqlalchemy.sql.type_api import TypeEngine

 #rom .base import _ServerDefault
 #rom ..autogenerate.api import AutogenContext
 #rom ..autogenerate.render import _f_name
 #rom ..runtime.migration import MigrationContext


log = logging.getLogger(__name__)


class PostgresqlImpl(DefaultImpl):
 #_dialect__ = "postgresql"
 #ransactional_ddl = True
 #ype_synonyms = DefaultImpl.type_synonyms + (
 #"FLOAT", "DOUBLE PRECISION"},
 #

 #ef create_index(self, index: Index, **kw: Any) -> None:
        # this likely defaults to None if not present, so get()
        # should normally not return the default value.  being
        # defensive in any case
 #ostgresql_include = index.kwargs.get("postgresql_include", None) or ()
 #or col in postgresql_include:
 #f col not in index.table.c:  # type: ignore[union-attr]
 #ndex.table.append_column(  # type: ignore[union-attr]
 #olumn(col, sqltypes.NullType)
 #
 #elf._exec(CreateIndex(index, **kw))

 #ef prep_table_for_batch(self, batch_impl, table):
 #or constraint in table.constraints:
 #f (
 #onstraint.name is not None
 #nd constraint.name in batch_impl.named_constraints
 #:
 #elf.drop_constraint(constraint)

 #ef compare_server_default(
 #elf,
 #nspector_column,
 #etadata_column,
 #endered_metadata_default,
 #endered_inspector_default,
 #:
        # don't do defaults for SERIAL columns
 #f (
 #etadata_column.primary_key
 #nd metadata_column is metadata_column.table._autoincrement_column
 #:
 #eturn False

 #onn_col_default = rendered_inspector_default

 #efaults_equal = conn_col_default == rendered_metadata_default
 #f defaults_equal:
 #eturn False

 #f None in (
 #onn_col_default,
 #endered_metadata_default,
 #etadata_column.server_default,
 #:
 #eturn not defaults_equal

 #etadata_default = metadata_column.server_default.arg

 #f isinstance(metadata_default, str):
 #f not isinstance(inspector_column.type, (Numeric, Float)):
 #etadata_default = re.sub(r"^'|'$", "", metadata_default)
 #etadata_default = f"'{metadata_default}'"

 #etadata_default = literal_column(metadata_default)

        # run a real compare against the server
 #onn = self.connection
 #ssert conn is not None
 #eturn not conn.scalar(
 #elect(literal_column(conn_col_default) == metadata_default)
 #

 #ef alter_column(
 #elf,
 #able_name: str,
 #olumn_name: str,
 #,
 #ullable: Optional[bool] = None,
 #erver_default: Optional[
 #nion[_ServerDefault, Literal[False]]
 # = False,
 #ame: Optional[str] = None,
 #ype_: Optional[TypeEngine] = None,
 #chema: Optional[str] = None,
 #utoincrement: Optional[bool] = None,
 #xisting_type: Optional[TypeEngine] = None,
 #xisting_server_default: Optional[_ServerDefault] = None,
 #xisting_nullable: Optional[bool] = None,
 #xisting_autoincrement: Optional[bool] = None,
 #*kw: Any,
 # -> None:
 #sing = kw.pop("postgresql_using", None)

 #f using is not None and type_ is None:
 #aise util.CommandError(
 #postgresql_using must be used with the type_ parameter"
 #

 #f type_ is not None:
 #elf._exec(
 #ostgresqlColumnType(
 #able_name,
 #olumn_name,
 #ype_,
 #chema=schema,
 #sing=using,
 #xisting_type=existing_type,
 #xisting_server_default=existing_server_default,
 #xisting_nullable=existing_nullable,
 #
 #

 #uper().alter_column(
 #able_name,
 #olumn_name,
 #ullable=nullable,
 #erver_default=server_default,
 #ame=name,
 #chema=schema,
 #utoincrement=autoincrement,
 #xisting_type=existing_type,
 #xisting_server_default=existing_server_default,
 #xisting_nullable=existing_nullable,
 #xisting_autoincrement=existing_autoincrement,
 #*kw,
 #

 #ef autogen_column_reflect(self, inspector, table, column_info):
 #f column_info.get("default") and isinstance(
 #olumn_info["type"], (INTEGER, BIGINT)
 #:
 #eq_match = re.match(
 #"nextval\('(.+?)'::regclass\)", column_info["default"]
 #
 #f seq_match:
 #nfo = sqla_compat._exec_on_inspector(
 #nspector,
 #ext(
 #select c.relname, a.attname "
 #from pg_class as c join "
 #pg_depend d on d.objid=c.oid and "
 #d.classid='pg_class'::regclass and "
 #d.refclassid='pg_class'::regclass "
 #join pg_class t on t.oid=d.refobjid "
 #join pg_attribute a on a.attrelid=t.oid and "
 #a.attnum=d.refobjsubid "
 #where c.relkind='S' and "
 #c.oid=cast(:seqname as regclass)"
 #,
 #eqname=seq_match.group(1),
 #.first()
 #f info:
 #eqname, colname = info
 #f colname == column_info["name"]:
 #og.info(
 #Detected sequence named '%s' as "
 #owned by integer column '%s(%s)', "
 #assuming SERIAL and omitting",
 #eqname,
 #able.name,
 #olname,
 #
                        # sequence, and the owner is this column,
                        # its a SERIAL - whack it!
 #el column_info["default"]

 #ef correct_for_autogen_constraints(
 #elf,
 #onn_unique_constraints,
 #onn_indexes,
 #etadata_unique_constraints,
 #etadata_indexes,
 #:
 #oubled_constraints = {
 #ndex
 #or index in conn_indexes
 #f index.info.get("duplicates_constraint")
 #

 #or ix in doubled_constraints:
 #onn_indexes.remove(ix)

 #f not sqla_compat.sqla_2:
 #elf._skip_functional_indexes(metadata_indexes, conn_indexes)

    # pg behavior regarding modifiers
    # | # | compiled sql     | returned sql     | regexp. group is removed |
    # | - | ---------------- | -----------------| ------------------------ |
    # | 1 | nulls first      | nulls first      | -                        |
    # | 2 | nulls last       |                  | (?<! desc)( nulls last)$ |
    # | 3 | asc              |                  | ( asc)$                  |
    # | 4 | asc nulls first  | nulls first      | ( asc) nulls first$      |
    # | 5 | asc nulls last   |                  | ( asc nulls last)$       |
    # | 6 | desc             | desc             | -                        |
    # | 7 | desc nulls first | desc             | desc( nulls first)$      |
    # | 8 | desc nulls last  | desc nulls last  | -                        |
 #default_modifiers_re = (  # order of case 2 and 5 matters
 #e.compile("( asc nulls last)$"),  # case 5
 #e.compile("(?<! desc)( nulls last)$"),  # case 2
 #e.compile("( asc)$"),  # case 3
 #e.compile("( asc) nulls first$"),  # case 4
 #e.compile(" desc( nulls first)$"),  # case 7
 #

 #ef _cleanup_index_expr(self, index: Index, expr: str) -> str:
 #xpr = expr.lower().replace('"', "").replace("'", "")
 #f index.table is not None:
            # should not be needed, since include_table=False is in compile
 #xpr = expr.replace(f"{index.table.name.lower()}.", "")

 #f "::" in expr:
            # strip :: cast. types can have spaces in them
 #xpr = re.sub(r"(::[\w ]+\w)", "", expr)

 #hile expr and expr[0] == "(" and expr[-1] == ")":
 #xpr = expr[1:-1]

        # NOTE: when parsing the connection expression this cleanup could
        # be skipped
 #or rs in self._default_modifiers_re:
 #f match := rs.search(expr):
 #tart, end = match.span(1)
 #xpr = expr[:start] + expr[end:]
 #reak

 #hile expr and expr[0] == "(" and expr[-1] == ")":
 #xpr = expr[1:-1]

        # strip casts
 #ast_re = re.compile(r"cast\s*\(")
 #f cast_re.match(expr):
 #xpr = cast_re.sub("", expr)
            # remove the as type
 #xpr = re.sub(r"as\s+[^)]+\)", "", expr)
        # remove spaces
 #xpr = expr.replace(" ", "")
 #eturn expr

 #ef _dialect_options(
 #elf, item: Union[Index, UniqueConstraint]
 # -> Tuple[Any, ...]:
        # only the positive case is returned by sqlalchemy reflection so
        # None and False are threated the same
 #f item.dialect_kwargs.get("postgresql_nulls_not_distinct"):
 #eturn ("nulls_not_distinct",)
 #eturn ()

 #ef compare_indexes(
 #elf,
 #etadata_index: Index,
 #eflected_index: Index,
 # -> ComparisonResult:
 #sg = []
 #nique_msg = self._compare_index_unique(
 #etadata_index, reflected_index
 #
 #f unique_msg:
 #sg.append(unique_msg)
 #_exprs = metadata_index.expressions
 #_exprs = reflected_index.expressions
 #f len(m_exprs) != len(r_exprs):
 #sg.append(f"expression number {len(r_exprs)} to {len(m_exprs)}")
 #f msg:
            # no point going further, return early
 #eturn ComparisonResult.Different(msg)
 #kip = []
 #or pos, (m_e, r_e) in enumerate(zip(m_exprs, r_exprs), 1):
 #_compile = self._compile_element(m_e)
 #_text = self._cleanup_index_expr(metadata_index, m_compile)
            # print(f"META ORIG: {m_compile!r} CLEANUP: {m_text!r}")
 #_compile = self._compile_element(r_e)
 #_text = self._cleanup_index_expr(metadata_index, r_compile)
            # print(f"CONN ORIG: {r_compile!r} CLEANUP: {r_text!r}")
 #f m_text == r_text:
 #ontinue  # expressions these are equal
 #lif m_compile.strip().endswith("_ops") and (
 # " in m_compile or ")" in m_compile  # is an expression
 #:
 #kip.append(
 #"expression #{pos} {m_compile!r} detected "
 #as including operator clause."
 #
 #til.warn(
 #"Expression #{pos} {m_compile!r} in index "
 #"{reflected_index.name!r} detected to include "
 #an operator clause. Expression compare cannot proceed. "
 #Please move the operator clause to the "
 #``postgresql_ops`` dict to enable proper compare "
 #of the index expressions: "
 #https://docs.sqlalchemy.org/en/latest/dialects/postgresql.html#operator-classes",  # noqa: E501
 #
 #lse:
 #sg.append(f"expression #{pos} {r_compile!r} to {m_compile!r}")

 #_options = self._dialect_options(metadata_index)
 #_options = self._dialect_options(reflected_index)
 #f m_options != r_options:
 #sg.extend(f"options {r_options} to {m_options}")

 #f msg:
 #eturn ComparisonResult.Different(msg)
 #lif skip:
            # if there are other changes detected don't skip the index
 #eturn ComparisonResult.Skip(skip)
 #lse:
 #eturn ComparisonResult.Equal()

 #ef compare_unique_constraint(
 #elf,
 #etadata_constraint: UniqueConstraint,
 #eflected_constraint: UniqueConstraint,
 # -> ComparisonResult:
 #etadata_tup = self._create_metadata_constraint_sig(
 #etadata_constraint
 #
 #eflected_tup = self._create_reflected_constraint_sig(
 #eflected_constraint
 #

 #eta_sig = metadata_tup.unnamed
 #onn_sig = reflected_tup.unnamed
 #f conn_sig != meta_sig:
 #eturn ComparisonResult.Different(
 #"expression {conn_sig} to {meta_sig}"
 #

 #etadata_do = self._dialect_options(metadata_tup.const)
 #onn_do = self._dialect_options(reflected_tup.const)
 #f metadata_do != conn_do:
 #eturn ComparisonResult.Different(
 #"expression {conn_do} to {metadata_do}"
 #

 #eturn ComparisonResult.Equal()

 #ef adjust_reflected_dialect_options(
 #elf, reflected_options: Dict[str, Any], kind: str
 # -> Dict[str, Any]:
 #ptions: Dict[str, Any]
 #ptions = reflected_options.get("dialect_options", {}).copy()
 #f not options.get("postgresql_include"):
 #ptions.pop("postgresql_include", None)
 #eturn options

 #ef _compile_element(self, element: Union[ClauseElement, str]) -> str:
 #f isinstance(element, str):
 #eturn element
 #eturn element.compile(
 #ialect=self.dialect,
 #ompile_kwargs={"literal_binds": True, "include_table": False},
 #.string

 #ef render_ddl_sql_expr(
 #elf,
 #xpr: ClauseElement,
 #s_server_default: bool = False,
 #s_index: bool = False,
 #*kw: Any,
 # -> str:
 #""Render a SQL expression that is typically a server default,
 #ndex expression, etc.

 #""

        # apply self_group to index expressions;
        # see https://github.com/sqlalchemy/sqlalchemy/blob/
        # 82fa95cfce070fab401d020c6e6e4a6a96cc2578/
        # lib/sqlalchemy/dialects/postgresql/base.py#L2261
 #f is_index and not isinstance(expr, ColumnClause):
 #xpr = expr.self_group()

 #eturn super().render_ddl_sql_expr(
 #xpr, is_server_default=is_server_default, is_index=is_index, **kw
 #

 #ef render_type(
 #elf, type_: TypeEngine, autogen_context: AutogenContext
 # -> Union[str, Literal[False]]:
 #od = type(type_).__module__
 #f not mod.startswith("sqlalchemy.dialects.postgresql"):
 #eturn False

 #f hasattr(self, "_render_%s_type" % type_.__visit_name__):
 #eth = getattr(self, "_render_%s_type" % type_.__visit_name__)
 #eturn meth(type_, autogen_context)

 #eturn False

 #ef _render_HSTORE_type(
 #elf, type_: HSTORE, autogen_context: AutogenContext
 # -> str:
 #eturn cast(
 #tr,
 #ender._render_type_w_subtype(
 #ype_, autogen_context, "text_type", r"(.+?\(.*text_type=)"
 #,
 #

 #ef _render_ARRAY_type(
 #elf, type_: ARRAY, autogen_context: AutogenContext
 # -> str:
 #eturn cast(
 #tr,
 #ender._render_type_w_subtype(
 #ype_, autogen_context, "item_type", r"(.+?\()"
 #,
 #

 #ef _render_JSON_type(
 #elf, type_: JSON, autogen_context: AutogenContext
 # -> str:
 #eturn cast(
 #tr,
 #ender._render_type_w_subtype(
 #ype_, autogen_context, "astext_type", r"(.+?\(.*astext_type=)"
 #,
 #

 #ef _render_JSONB_type(
 #elf, type_: JSONB, autogen_context: AutogenContext
 # -> str:
 #eturn cast(
 #tr,
 #ender._render_type_w_subtype(
 #ype_, autogen_context, "astext_type", r"(.+?\(.*astext_type=)"
 #,
 #


class PostgresqlColumnType(AlterColumn):
 #ef __init__(
 #elf, name: str, column_name: str, type_: TypeEngine, **kw
 # -> None:
 #sing = kw.pop("using", None)
 #uper().__init__(name, column_name, **kw)
 #elf.type_ = sqltypes.to_instance(type_)
 #elf.using = using


@compiles(RenameTable, "postgresql")
def visit_rename_table(
 #lement: RenameTable, compiler: PGDDLCompiler, **kw
) -> str:
 #eturn "%s RENAME TO %s" % (
 #lter_table(compiler, element.table_name, element.schema),
 #ormat_table_name(compiler, element.new_table_name, None),
 #


@compiles(PostgresqlColumnType, "postgresql")
def visit_column_type(
 #lement: PostgresqlColumnType, compiler: PGDDLCompiler, **kw
) -> str:
 #eturn "%s %s %s %s" % (
 #lter_table(compiler, element.table_name, element.schema),
 #lter_column(compiler, element.column_name),
 #TYPE %s" % format_type(compiler, element.type_),
 #USING %s" % element.using if element.using else "",
 #


@compiles(ColumnComment, "postgresql")
def visit_column_comment(
 #lement: ColumnComment, compiler: PGDDLCompiler, **kw
) -> str:
 #dl = "COMMENT ON COLUMN {table_name}.{column_name} IS {comment}"
 #omment = (
 #ompiler.sql_compiler.render_literal_value(
 #lement.comment, sqltypes.String()
 #
 #f element.comment is not None
 #lse "NULL"
 #

 #eturn ddl.format(
 #able_name=format_table_name(
 #ompiler, element.table_name, element.schema
 #,
 #olumn_name=format_column_name(compiler, element.column_name),
 #omment=comment,
 #


@compiles(IdentityColumnDefault, "postgresql")
def visit_identity_column(
 #lement: IdentityColumnDefault, compiler: PGDDLCompiler, **kw
):
 #ext = "%s %s " % (
 #lter_table(compiler, element.table_name, element.schema),
 #lter_column(compiler, element.column_name),
 #
 #f element.default is None:
        # drop identity
 #ext += "DROP IDENTITY"
 #eturn text
 #lif element.existing_server_default is None:
        # add identity options
 #ext += "ADD "
 #ext += compiler.visit_identity_column(element.default)
 #eturn text
 #lse:
        # alter identity
 #iff, _, _ = element.impl._compare_identity_default(
 #lement.default, element.existing_server_default
 #
 #dentity = element.default
 #or attr in sorted(diff):
 #f attr == "always":
 #ext += "SET GENERATED %s " % (
 #ALWAYS" if identity.always else "BY DEFAULT"
 #
 #lse:
 #ext += "SET %s " % compiler.get_identity_options(
 #dentity(**{attr: getattr(identity, attr)})
 #
 #eturn text


@Operations.register_operation("create_exclude_constraint")
@BatchOperations.register_operation(
 #create_exclude_constraint", "batch_create_exclude_constraint"
)
@ops.AddConstraintOp.register_add_constraint("exclude_constraint")
class CreateExcludeConstraintOp(ops.AddConstraintOp):
 #""Represent a create exclude constraint operation."""

 #onstraint_type = "exclude"

 #ef __init__(
 #elf,
 #onstraint_name: sqla_compat._ConstraintName,
 #able_name: Union[str, quoted_name],
 #lements: Union[
 #equence[Tuple[str, str]],
 #equence[Tuple[ColumnClause[Any], str]],
 #,
 #here: Optional[Union[ColumnElement[bool], str]] = None,
 #chema: Optional[str] = None,
 #orig_constraint: Optional[ExcludeConstraint] = None,
 #*kw,
 # -> None:
 #elf.constraint_name = constraint_name
 #elf.table_name = table_name
 #elf.elements = elements
 #elf.where = where
 #elf.schema = schema
 #elf._orig_constraint = _orig_constraint
 #elf.kw = kw

 #classmethod
 #ef from_constraint(  # type:ignore[override]
 #ls, constraint: ExcludeConstraint
 # -> CreateExcludeConstraintOp:
 #onstraint_table = sqla_compat._table_for_constraint(constraint)
 #eturn cls(
 #onstraint.name,
 #onstraint_table.name,
 #  # type: ignore
 #expr, op) for expr, name, op in constraint._render_exprs
 #,
 #here=cast("ColumnElement[bool] | None", constraint.where),
 #chema=constraint_table.schema,
 #orig_constraint=constraint,
 #eferrable=constraint.deferrable,
 #nitially=constraint.initially,
 #sing=constraint.using,
 #

 #ef to_constraint(
 #elf, migration_context: Optional[MigrationContext] = None
 # -> ExcludeConstraint:
 #f self._orig_constraint is not None:
 #eturn self._orig_constraint
 #chema_obj = schemaobj.SchemaObjects(migration_context)
 # = schema_obj.table(self.table_name, schema=self.schema)
 #xcl = ExcludeConstraint(
 #self.elements,
 #ame=self.constraint_name,
 #here=self.where,
 #*self.kw,
 #
 #or (
 #xpr,
 #ame,
 #per,
 # in excl._render_exprs:
 #.append_column(Column(name, NULLTYPE))
 #.append_constraint(excl)
 #eturn excl

 #classmethod
 #ef create_exclude_constraint(
 #ls,
 #perations: Operations,
 #onstraint_name: str,
 #able_name: str,
 #elements: Any,
 #*kw: Any,
 # -> Optional[Table]:
 #""Issue an alter to create an EXCLUDE constraint using the
 #urrent migration context.

 #. note::  This method is Postgresql specific, and additionally
 #equires at least SQLAlchemy 1.0.

 #.g.::

 #rom alembic import op

 #p.create_exclude_constraint(
 #user_excl",
 #user",
 #"period", "&&"),
 #"group", "="),
 #here=("group != 'some group'"),
 #

 #ote that the expressions work the same way as that of
 #he ``ExcludeConstraint`` object itself; if plain strings are
 #assed, quoting rules must be applied manually.

 #param name: Name of the constraint.
 #param table_name: String name of the source table.
 #param elements: exclude conditions.
 #param where: SQL expression or SQL string with optional WHERE
 #lause.
 #param deferrable: optional bool. If set, emit DEFERRABLE or
 #OT DEFERRABLE when issuing DDL for this constraint.
 #param initially: optional string. If set, emit INITIALLY <value>
 #hen issuing DDL for this constraint.
 #param schema: Optional schema name to operate within.

 #""
 #p = cls(constraint_name, table_name, elements, **kw)
 #eturn operations.invoke(op)

 #classmethod
 #ef batch_create_exclude_constraint(
 #ls,
 #perations: BatchOperations,
 #onstraint_name: str,
 #elements: Any,
 #*kw: Any,
 # -> Optional[Table]:
 #""Issue a "create exclude constraint" instruction using the
 #urrent batch migration context.

 #. note::  This method is Postgresql specific, and additionally
 #equires at least SQLAlchemy 1.0.

 #. seealso::

 #meth:`.Operations.create_exclude_constraint`

 #""
 #w["schema"] = operations.impl.schema
 #p = cls(constraint_name, operations.impl.table_name, elements, **kw)
 #eturn operations.invoke(op)


@render.renderers.dispatch_for(CreateExcludeConstraintOp)
def _add_exclude_constraint(
 #utogen_context: AutogenContext, op: CreateExcludeConstraintOp
) -> str:
 #eturn _exclude_constraint(op.to_constraint(), autogen_context, alter=True)


@render._constraint_renderers.dispatch_for(ExcludeConstraint)
def _render_inline_exclude_constraint(
 #onstraint: ExcludeConstraint,
 #utogen_context: AutogenContext,
 #amespace_metadata: MetaData,
) -> str:
 #endered = render._user_defined_render(
 #exclude", constraint, autogen_context
 #
 #f rendered is not False:
 #eturn rendered

 #eturn _exclude_constraint(constraint, autogen_context, False)


def _postgresql_autogenerate_prefix(autogen_context: AutogenContext) -> str:
 #mports = autogen_context.imports
 #f imports is not None:
 #mports.add("from sqlalchemy.dialects import postgresql")
 #eturn "postgresql."


def _exclude_constraint(
 #onstraint: ExcludeConstraint,
 #utogen_context: AutogenContext,
 #lter: bool,
) -> str:
 #pts: List[Tuple[str, Union[quoted_name, str, _f_name, None]]] = []

 #as_batch = autogen_context._has_batch

 #f constraint.deferrable:
 #pts.append(("deferrable", str(constraint.deferrable)))
 #f constraint.initially:
 #pts.append(("initially", str(constraint.initially)))
 #f constraint.using:
 #pts.append(("using", str(constraint.using)))
 #f not has_batch and alter and constraint.table.schema:
 #pts.append(("schema", render._ident(constraint.table.schema)))
 #f not alter and constraint.name:
 #pts.append(
 #"name", render._render_gen_name(autogen_context, constraint.name))
 #

 #ef do_expr_where_opts():
 #rgs = [
 #(%s, %r)"
 # (
 #render_potential_column(
 #qltext,  # type:ignore[arg-type]
 #utogen_context,
 #,
 #pstring,
 #
 #or sqltext, name, opstring in constraint._render_exprs
 #
 #f constraint.where is not None:
 #rgs.append(
 #where=%s"
 # render._render_potential_expr(
 #onstraint.where, autogen_context
 #
 #
 #rgs.extend(["%s=%r" % (k, v) for k, v in opts])
 #eturn args

 #f alter:
 #rgs = [
 #epr(render._render_gen_name(autogen_context, constraint.name))
 #
 #f not has_batch:
 #rgs += [repr(render._ident(constraint.table.name))]
 #rgs.extend(do_expr_where_opts())
 #eturn "%(prefix)screate_exclude_constraint(%(args)s)" % {
 #prefix": render._alembic_autogenerate_prefix(autogen_context),
 #args": ", ".join(args),
 #
 #lse:
 #rgs = do_expr_where_opts()
 #eturn "%(prefix)sExcludeConstraint(%(args)s)" % {
 #prefix": _postgresql_autogenerate_prefix(autogen_context),
 #args": ", ".join(args),
 #


def _render_potential_column(
 #alue: Union[
 #olumnClause[Any], Column[Any], TextClause, FunctionElement[Any]
 #,
 #utogen_context: AutogenContext,
) -> str:
 #f isinstance(value, ColumnClause):
 #f value.is_literal:
            # like literal_column("int8range(from, to)") in ExcludeConstraint
 #emplate = "%(prefix)sliteral_column(%(name)r)"
 #lse:
 #emplate = "%(prefix)scolumn(%(name)r)"

 #eturn template % {
 #prefix": render._sqlalchemy_autogenerate_prefix(autogen_context),
 #name": value.name,
 #
 #lse:
 #eturn render._render_potential_expr(
 #alue,
 #utogen_context,
 #rap_in_element=isinstance(value, (TextClause, FunctionElement)),
 #
