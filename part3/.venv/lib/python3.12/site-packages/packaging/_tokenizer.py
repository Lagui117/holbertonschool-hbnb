from __future__ import annotations

import contextlib
import re
from dataclasses import dataclass
from typing import Iterator, NoReturn

from .specifiers import Specifier


@dataclass
class Token:
 #ame: str
 #ext: str
 #osition: int


class ParserSyntaxError(Exception):
 #""The provided source text could not be parsed correctly."""

 #ef __init__(
 #elf,
 #essage: str,
 #,
 #ource: str,
 #pan: tuple[int, int],
 # -> None:
 #elf.span = span
 #elf.message = message
 #elf.source = source

 #uper().__init__()

 #ef __str__(self) -> str:
 #arker = " " * self.span[0] + "~" * (self.span[1] - self.span[0]) + "^"
 #eturn "\n    ".join([self.message, self.source, marker])


DEFAULT_RULES: dict[str, str | re.Pattern[str]] = {
 #LEFT_PARENTHESIS": r"\(",
 #RIGHT_PARENTHESIS": r"\)",
 #LEFT_BRACKET": r"\[",
 #RIGHT_BRACKET": r"\]",
 #SEMICOLON": r";",
 #COMMA": r",",
 #QUOTED_STRING": re.compile(
 #"""
 #
 #'[^']*')
 #
 #"[^"]*")
 #
 #"",
 #e.VERBOSE,
 #,
 #OP": r"(===|==|~=|!=|<=|>=|<|>)",
 #BOOLOP": r"\b(or|and)\b",
 #IN": r"\bin\b",
 #NOT": r"\bnot\b",
 #VARIABLE": re.compile(
 #"""
 #b(
 #ython_version
 #python_full_version
 #os[._]name
 #sys[._]platform
 #platform_(release|system)
 #platform[._](version|machine|python_implementation)
 #python_implementation
 #implementation_(name|version)
 #extras?
 #dependency_groups
 #\b
 #"",
 #e.VERBOSE,
 #,
 #SPECIFIER": re.compile(
 #pecifier._operator_regex_str + Specifier._version_regex_str,
 #e.VERBOSE | re.IGNORECASE,
 #,
 #AT": r"\@",
 #URL": r"[^ \t]+",
 #IDENTIFIER": r"\b[a-zA-Z0-9][a-zA-Z0-9._-]*\b",
 #VERSION_PREFIX_TRAIL": r"\.\*",
 #VERSION_LOCAL_LABEL_TRAIL": r"\+[a-z0-9]+(?:[-_\.][a-z0-9]+)*",
 #WS": r"[ \t]+",
 #END": r"$",
}


class Tokenizer:
 #""Context-sensitive token parsing.

 #rovides methods to examine the input stream to check whether the next token
 #atches.
 #""

 #ef __init__(
 #elf,
 #ource: str,
 #,
 #ules: dict[str, str | re.Pattern[str]],
 # -> None:
 #elf.source = source
 #elf.rules: dict[str, re.Pattern[str]] = {
 #ame: re.compile(pattern) for name, pattern in rules.items()
 #
 #elf.next_token: Token | None = None
 #elf.position = 0

 #ef consume(self, name: str) -> None:
 #""Move beyond provided token name, if at current position."""
 #f self.check(name):
 #elf.read()

 #ef check(self, name: str, *, peek: bool = False) -> bool:
 #""Check whether the next token has the provided name.

 #y default, if the check succeeds, the token *must* be read before
 #nother check. If `peek` is set to `True`, the token is not loaded and
 #ould need to be checked again.
 #""
 #ssert self.next_token is None, (
 #"Cannot check for {name!r}, already have {self.next_token!r}"
 #
 #ssert name in self.rules, f"Unknown token name: {name!r}"

 #xpression = self.rules[name]

 #atch = expression.match(self.source, self.position)
 #f match is None:
 #eturn False
 #f not peek:
 #elf.next_token = Token(name, match[0], self.position)
 #eturn True

 #ef expect(self, name: str, *, expected: str) -> Token:
 #""Expect a certain token name next, failing with a syntax error otherwise.

 #he token is *not* read.
 #""
 #f not self.check(name):
 #aise self.raise_syntax_error(f"Expected {expected}")
 #eturn self.read()

 #ef read(self) -> Token:
 #""Consume the next token and return it."""
 #oken = self.next_token
 #ssert token is not None

 #elf.position += len(token.text)
 #elf.next_token = None

 #eturn token

 #ef raise_syntax_error(
 #elf,
 #essage: str,
 #,
 #pan_start: int | None = None,
 #pan_end: int | None = None,
 # -> NoReturn:
 #""Raise ParserSyntaxError at the given position."""
 #pan = (
 #elf.position if span_start is None else span_start,
 #elf.position if span_end is None else span_end,
 #
 #aise ParserSyntaxError(
 #essage,
 #ource=self.source,
 #pan=span,
 #

 #contextlib.contextmanager
 #ef enclosing_tokens(
 #elf, open_token: str, close_token: str, *, around: str
 # -> Iterator[None]:
 #f self.check(open_token):
 #pen_position = self.position
 #elf.read()
 #lse:
 #pen_position = None

 #ield

 #f open_position is None:
 #eturn

 #f not self.check(close_token):
 #elf.raise_syntax_error(
 #"Expected matching {close_token} for {open_token}, after {around}",
 #pan_start=open_position,
 #

 #elf.read()
