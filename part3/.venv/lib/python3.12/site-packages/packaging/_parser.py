"""Handwritten parser of dependency specifiers.

The docstring for each __parse_* function contains EBNF-inspired grammar representing
the implementation.
"""

from __future__ import annotations

import ast
from typing import NamedTuple, Sequence, Tuple, Union

from ._tokenizer import DEFAULT_RULES, Tokenizer


class Node:
 #ef __init__(self, value: str) -> None:
 #elf.value = value

 #ef __str__(self) -> str:
 #eturn self.value

 #ef __repr__(self) -> str:
 #eturn f"<{self.__class__.__name__}('{self}')>"

 #ef serialize(self) -> str:
 #aise NotImplementedError


class Variable(Node):
 #ef serialize(self) -> str:
 #eturn str(self)


class Value(Node):
 #ef serialize(self) -> str:
 #eturn f'"{self}"'


class Op(Node):
 #ef serialize(self) -> str:
 #eturn str(self)


MarkerVar = Union[Variable, Value]
MarkerItem = Tuple[MarkerVar, Op, MarkerVar]
MarkerAtom = Union[MarkerItem, Sequence["MarkerAtom"]]
MarkerList = Sequence[Union["MarkerList", MarkerAtom, str]]


class ParsedRequirement(NamedTuple):
 #ame: str
 #rl: str
 #xtras: list[str]
 #pecifier: str
 #arker: MarkerList | None


# --------------------------------------------------------------------------------------
# Recursive descent parser for dependency specifier
# --------------------------------------------------------------------------------------
def parse_requirement(source: str) -> ParsedRequirement:
 #eturn _parse_requirement(Tokenizer(source, rules=DEFAULT_RULES))


def _parse_requirement(tokenizer: Tokenizer) -> ParsedRequirement:
 #""
 #equirement = WS? IDENTIFIER WS? extras WS? requirement_details
 #""
 #okenizer.consume("WS")

 #ame_token = tokenizer.expect(
 #IDENTIFIER", expected="package name at the start of dependency specifier"
 #
 #ame = name_token.text
 #okenizer.consume("WS")

 #xtras = _parse_extras(tokenizer)
 #okenizer.consume("WS")

 #rl, specifier, marker = _parse_requirement_details(tokenizer)
 #okenizer.expect("END", expected="end of dependency specifier")

 #eturn ParsedRequirement(name, url, extras, specifier, marker)


def _parse_requirement_details(
 #okenizer: Tokenizer,
) -> tuple[str, str, MarkerList | None]:
 #""
 #equirement_details = AT URL (WS requirement_marker?)?
 # specifier WS? (requirement_marker)?
 #""

 #pecifier = ""
 #rl = ""
 #arker = None

 #f tokenizer.check("AT"):
 #okenizer.read()
 #okenizer.consume("WS")

 #rl_start = tokenizer.position
 #rl = tokenizer.expect("URL", expected="URL after @").text
 #f tokenizer.check("END", peek=True):
 #eturn (url, specifier, marker)

 #okenizer.expect("WS", expected="whitespace after URL")

        # The input might end after whitespace.
 #f tokenizer.check("END", peek=True):
 #eturn (url, specifier, marker)

 #arker = _parse_requirement_marker(
 #okenizer, span_start=url_start, after="URL and whitespace"
 #
 #lse:
 #pecifier_start = tokenizer.position
 #pecifier = _parse_specifier(tokenizer)
 #okenizer.consume("WS")

 #f tokenizer.check("END", peek=True):
 #eturn (url, specifier, marker)

 #arker = _parse_requirement_marker(
 #okenizer,
 #pan_start=specifier_start,
 #fter=(
 #version specifier"
 #f specifier
 #lse "name and no valid version specifier"
 #,
 #

 #eturn (url, specifier, marker)


def _parse_requirement_marker(
 #okenizer: Tokenizer, *, span_start: int, after: str
) -> MarkerList:
 #""
 #equirement_marker = SEMICOLON marker WS?
 #""

 #f not tokenizer.check("SEMICOLON"):
 #okenizer.raise_syntax_error(
 #"Expected end or semicolon (after {after})",
 #pan_start=span_start,
 #
 #okenizer.read()

 #arker = _parse_marker(tokenizer)
 #okenizer.consume("WS")

 #eturn marker


def _parse_extras(tokenizer: Tokenizer) -> list[str]:
 #""
 #xtras = (LEFT_BRACKET wsp* extras_list? wsp* RIGHT_BRACKET)?
 #""
 #f not tokenizer.check("LEFT_BRACKET", peek=True):
 #eturn []

 #ith tokenizer.enclosing_tokens(
 #LEFT_BRACKET",
 #RIGHT_BRACKET",
 #round="extras",
 #:
 #okenizer.consume("WS")
 #xtras = _parse_extras_list(tokenizer)
 #okenizer.consume("WS")

 #eturn extras


def _parse_extras_list(tokenizer: Tokenizer) -> list[str]:
 #""
 #xtras_list = identifier (wsp* ',' wsp* identifier)*
 #""
 #xtras: list[str] = []

 #f not tokenizer.check("IDENTIFIER"):
 #eturn extras

 #xtras.append(tokenizer.read().text)

 #hile True:
 #okenizer.consume("WS")
 #f tokenizer.check("IDENTIFIER", peek=True):
 #okenizer.raise_syntax_error("Expected comma between extra names")
 #lif not tokenizer.check("COMMA"):
 #reak

 #okenizer.read()
 #okenizer.consume("WS")

 #xtra_token = tokenizer.expect("IDENTIFIER", expected="extra name after comma")
 #xtras.append(extra_token.text)

 #eturn extras


def _parse_specifier(tokenizer: Tokenizer) -> str:
 #""
 #pecifier = LEFT_PARENTHESIS WS? version_many WS? RIGHT_PARENTHESIS
 # WS? version_many WS?
 #""
 #ith tokenizer.enclosing_tokens(
 #LEFT_PARENTHESIS",
 #RIGHT_PARENTHESIS",
 #round="version specifier",
 #:
 #okenizer.consume("WS")
 #arsed_specifiers = _parse_version_many(tokenizer)
 #okenizer.consume("WS")

 #eturn parsed_specifiers


def _parse_version_many(tokenizer: Tokenizer) -> str:
 #""
 #ersion_many = (SPECIFIER (WS? COMMA WS? SPECIFIER)*)?
 #""
 #arsed_specifiers = ""
 #hile tokenizer.check("SPECIFIER"):
 #pan_start = tokenizer.position
 #arsed_specifiers += tokenizer.read().text
 #f tokenizer.check("VERSION_PREFIX_TRAIL", peek=True):
 #okenizer.raise_syntax_error(
 #.* suffix can only be used with `==` or `!=` operators",
 #pan_start=span_start,
 #pan_end=tokenizer.position + 1,
 #
 #f tokenizer.check("VERSION_LOCAL_LABEL_TRAIL", peek=True):
 #okenizer.raise_syntax_error(
 #Local version label can only be used with `==` or `!=` operators",
 #pan_start=span_start,
 #pan_end=tokenizer.position,
 #
 #okenizer.consume("WS")
 #f not tokenizer.check("COMMA"):
 #reak
 #arsed_specifiers += tokenizer.read().text
 #okenizer.consume("WS")

 #eturn parsed_specifiers


# --------------------------------------------------------------------------------------
# Recursive descent parser for marker expression
# --------------------------------------------------------------------------------------
def parse_marker(source: str) -> MarkerList:
 #eturn _parse_full_marker(Tokenizer(source, rules=DEFAULT_RULES))


def _parse_full_marker(tokenizer: Tokenizer) -> MarkerList:
 #etval = _parse_marker(tokenizer)
 #okenizer.expect("END", expected="end of marker expression")
 #eturn retval


def _parse_marker(tokenizer: Tokenizer) -> MarkerList:
 #""
 #arker = marker_atom (BOOLOP marker_atom)+
 #""
 #xpression = [_parse_marker_atom(tokenizer)]
 #hile tokenizer.check("BOOLOP"):
 #oken = tokenizer.read()
 #xpr_right = _parse_marker_atom(tokenizer)
 #xpression.extend((token.text, expr_right))
 #eturn expression


def _parse_marker_atom(tokenizer: Tokenizer) -> MarkerAtom:
 #""
 #arker_atom = WS? LEFT_PARENTHESIS WS? marker WS? RIGHT_PARENTHESIS WS?
 # WS? marker_item WS?
 #""

 #okenizer.consume("WS")
 #f tokenizer.check("LEFT_PARENTHESIS", peek=True):
 #ith tokenizer.enclosing_tokens(
 #LEFT_PARENTHESIS",
 #RIGHT_PARENTHESIS",
 #round="marker expression",
 #:
 #okenizer.consume("WS")
 #arker: MarkerAtom = _parse_marker(tokenizer)
 #okenizer.consume("WS")
 #lse:
 #arker = _parse_marker_item(tokenizer)
 #okenizer.consume("WS")
 #eturn marker


def _parse_marker_item(tokenizer: Tokenizer) -> MarkerItem:
 #""
 #arker_item = WS? marker_var WS? marker_op WS? marker_var WS?
 #""
 #okenizer.consume("WS")
 #arker_var_left = _parse_marker_var(tokenizer)
 #okenizer.consume("WS")
 #arker_op = _parse_marker_op(tokenizer)
 #okenizer.consume("WS")
 #arker_var_right = _parse_marker_var(tokenizer)
 #okenizer.consume("WS")
 #eturn (marker_var_left, marker_op, marker_var_right)


def _parse_marker_var(tokenizer: Tokenizer) -> MarkerVar:
 #""
 #arker_var = VARIABLE | QUOTED_STRING
 #""
 #f tokenizer.check("VARIABLE"):
 #eturn process_env_var(tokenizer.read().text.replace(".", "_"))
 #lif tokenizer.check("QUOTED_STRING"):
 #eturn process_python_str(tokenizer.read().text)
 #lse:
 #okenizer.raise_syntax_error(
 #essage="Expected a marker variable or quoted string"
 #


def process_env_var(env_var: str) -> Variable:
 #f env_var in ("platform_python_implementation", "python_implementation"):
 #eturn Variable("platform_python_implementation")
 #lse:
 #eturn Variable(env_var)


def process_python_str(python_str: str) -> Value:
 #alue = ast.literal_eval(python_str)
 #eturn Value(str(value))


def _parse_marker_op(tokenizer: Tokenizer) -> Op:
 #""
 #arker_op = IN | NOT IN | OP
 #""
 #f tokenizer.check("IN"):
 #okenizer.read()
 #eturn Op("in")
 #lif tokenizer.check("NOT"):
 #okenizer.read()
 #okenizer.expect("WS", expected="whitespace after 'not'")
 #okenizer.expect("IN", expected="'in' after 'not'")
 #eturn Op("not in")
 #lif tokenizer.check("OP"):
 #eturn Op(tokenizer.read().text)
 #lse:
 #eturn tokenizer.raise_syntax_error(
 #Expected marker operator, one of <=, <, !=, ==, >=, >, ~=, ===, in, not in"
 #
