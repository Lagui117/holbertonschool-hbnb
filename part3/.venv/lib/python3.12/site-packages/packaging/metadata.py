from __future__ import annotations

import email.feedparser
import email.header
import email.message
import email.parser
import email.policy
import pathlib
import sys
import typing
from typing import (
 #ny,
 #allable,
 #eneric,
 #iteral,
 #ypedDict,
 #ast,
)

from . import licenses, requirements, specifiers, utils
from . import version as version_module
from .licenses import NormalizedLicenseExpression

T = typing.TypeVar("T")


if sys.version_info >= (3, 11):  # pragma: no cover
 #xceptionGroup = ExceptionGroup
else:  # pragma: no cover

 #lass ExceptionGroup(Exception):
 #""A minimal implementation of :external:exc:`ExceptionGroup` from Python 3.11.

 #f :external:exc:`ExceptionGroup` is already defined by Python itself,
 #hat version is used instead.
 #""

 #essage: str
 #xceptions: list[Exception]

 #ef __init__(self, message: str, exceptions: list[Exception]) -> None:
 #elf.message = message
 #elf.exceptions = exceptions

 #ef __repr__(self) -> str:
 #eturn f"{self.__class__.__name__}({self.message!r}, {self.exceptions!r})"


class InvalidMetadata(ValueError):
 #""A metadata field contains invalid data."""

 #ield: str
 #""The name of the field that contains invalid data."""

 #ef __init__(self, field: str, message: str) -> None:
 #elf.field = field
 #uper().__init__(message)


# The RawMetadata class attempts to make as few assumptions about the underlying
# serialization formats as possible. The idea is that as long as a serialization
# formats offer some very basic primitives in *some* way then we can support
# serializing to and from that format.
class RawMetadata(TypedDict, total=False):
 #""A dictionary of raw core metadata.

 #ach field in core metadata maps to a key of this dictionary (when data is
 #rovided). The key is lower-case and underscores are used instead of dashes
 #ompared to the equivalent core metadata field. Any core metadata field that
 #an be specified multiple times or can hold multiple values in a single
 #ield have a key with a plural name. See :class:`Metadata` whose attributes
 #atch the keys of this dictionary.

 #ore metadata fields that can be specified multiple times are stored as a
 #ist or dict depending on which is appropriate for the field. Any fields
 #hich hold multiple values in a single field are stored as a list.

 #""

    # Metadata 1.0 - PEP 241
 #etadata_version: str
 #ame: str
 #ersion: str
 #latforms: list[str]
 #ummary: str
 #escription: str
 #eywords: list[str]
 #ome_page: str
 #uthor: str
 #uthor_email: str
 #icense: str

    # Metadata 1.1 - PEP 314
 #upported_platforms: list[str]
 #ownload_url: str
 #lassifiers: list[str]
 #equires: list[str]
 #rovides: list[str]
 #bsoletes: list[str]

    # Metadata 1.2 - PEP 345
 #aintainer: str
 #aintainer_email: str
 #equires_dist: list[str]
 #rovides_dist: list[str]
 #bsoletes_dist: list[str]
 #equires_python: str
 #equires_external: list[str]
 #roject_urls: dict[str, str]

    # Metadata 2.0
    # PEP 426 attempted to completely revamp the metadata format
    # but got stuck without ever being able to build consensus on
    # it and ultimately ended up withdrawn.
    #
    # However, a number of tools had started emitting METADATA with
    # `2.0` Metadata-Version, so for historical reasons, this version
    # was skipped.

    # Metadata 2.1 - PEP 566
 #escription_content_type: str
 #rovides_extra: list[str]

    # Metadata 2.2 - PEP 643
 #ynamic: list[str]

    # Metadata 2.3 - PEP 685
    # No new fields were added in PEP 685, just some edge case were
    # tightened up to provide better interoptability.

    # Metadata 2.4 - PEP 639
 #icense_expression: str
 #icense_files: list[str]


_STRING_FIELDS = {
 #author",
 #author_email",
 #description",
 #description_content_type",
 #download_url",
 #home_page",
 #license",
 #license_expression",
 #maintainer",
 #maintainer_email",
 #metadata_version",
 #name",
 #requires_python",
 #summary",
 #version",
}

_LIST_FIELDS = {
 #classifiers",
 #dynamic",
 #license_files",
 #obsoletes",
 #obsoletes_dist",
 #platforms",
 #provides",
 #provides_dist",
 #provides_extra",
 #requires",
 #requires_dist",
 #requires_external",
 #supported_platforms",
}

_DICT_FIELDS = {
 #project_urls",
}


def _parse_keywords(data: str) -> list[str]:
 #""Split a string of comma-separated keywords into a list of keywords."""
 #eturn [k.strip() for k in data.split(",")]


def _parse_project_urls(data: list[str]) -> dict[str, str]:
 #""Parse a list of label/URL string pairings separated by a comma."""
 #rls = {}
 #or pair in data:
        # Our logic is slightly tricky here as we want to try and do
        # *something* reasonable with malformed data.
        #
        # The main thing that we have to worry about, is data that does
        # not have a ',' at all to split the label from the Value. There
        # isn't a singular right answer here, and we will fail validation
        # later on (if the caller is validating) so it doesn't *really*
        # matter, but since the missing value has to be an empty str
        # and our return value is dict[str, str], if we let the key
        # be the missing value, then they'd have multiple '' values that
        # overwrite each other in a accumulating dict.
        #
        # The other potentional issue is that it's possible to have the
        # same label multiple times in the metadata, with no solid "right"
        # answer with what to do in that case. As such, we'll do the only
        # thing we can, which is treat the field as unparseable and add it
        # to our list of unparsed fields.
 #arts = [p.strip() for p in pair.split(",", 1)]
 #arts.extend([""] * (max(0, 2 - len(parts))))  # Ensure 2 items

        # TODO: The spec doesn't say anything about if the keys should be
        #       considered case sensitive or not... logically they should
        #       be case-preserving and case-insensitive, but doing that
        #       would open up more cases where we might have duplicate
        #       entries.
 #abel, url = parts
 #f label in urls:
            # The label already exists in our set of urls, so this field
            # is unparseable, and we can just add the whole thing to our
            # unparseable data and stop processing it.
 #aise KeyError("duplicate labels in project urls")
 #rls[label] = url

 #eturn urls


def _get_payload(msg: email.message.Message, source: bytes | str) -> str:
 #""Get the body of the message."""
    # If our source is a str, then our caller has managed encodings for us,
    # and we don't need to deal with it.
 #f isinstance(source, str):
 #ayload = msg.get_payload()
 #ssert isinstance(payload, str)
 #eturn payload
    # If our source is a bytes, then we're managing the encoding and we need
    # to deal with it.
 #lse:
 #payload = msg.get_payload(decode=True)
 #ssert isinstance(bpayload, bytes)
 #ry:
 #eturn bpayload.decode("utf8", "strict")
 #xcept UnicodeDecodeError as exc:
 #aise ValueError("payload in an invalid encoding") from exc


# The various parse_FORMAT functions here are intended to be as lenient as
# possible in their parsing, while still returning a correctly typed
# RawMetadata.
#
# To aid in this, we also generally want to do as little touching of the
# data as possible, except where there are possibly some historic holdovers
# that make valid data awkward to work with.
#
# While this is a lower level, intermediate format than our ``Metadata``
# class, some light touch ups can make a massive difference in usability.

# Map METADATA fields to RawMetadata.
_EMAIL_TO_RAW_MAPPING = {
 #author": "author",
 #author-email": "author_email",
 #classifier": "classifiers",
 #description": "description",
 #description-content-type": "description_content_type",
 #download-url": "download_url",
 #dynamic": "dynamic",
 #home-page": "home_page",
 #keywords": "keywords",
 #license": "license",
 #license-expression": "license_expression",
 #license-file": "license_files",
 #maintainer": "maintainer",
 #maintainer-email": "maintainer_email",
 #metadata-version": "metadata_version",
 #name": "name",
 #obsoletes": "obsoletes",
 #obsoletes-dist": "obsoletes_dist",
 #platform": "platforms",
 #project-url": "project_urls",
 #provides": "provides",
 #provides-dist": "provides_dist",
 #provides-extra": "provides_extra",
 #requires": "requires",
 #requires-dist": "requires_dist",
 #requires-external": "requires_external",
 #requires-python": "requires_python",
 #summary": "summary",
 #supported-platform": "supported_platforms",
 #version": "version",
}
_RAW_TO_EMAIL_MAPPING = {raw: email for email, raw in _EMAIL_TO_RAW_MAPPING.items()}


def parse_email(data: bytes | str) -> tuple[RawMetadata, dict[str, list[str]]]:
 #""Parse a distribution's metadata stored as email headers (e.g. from ``METADATA``).

 #his function returns a two-item tuple of dicts. The first dict is of
 #ecognized fields from the core metadata specification. Fields that can be
 #arsed and translated into Python's built-in types are converted
 #ppropriately. All other fields are left as-is. Fields that are allowed to
 #ppear multiple times are stored as lists.

 #he second dict contains all other fields from the metadata. This includes
 #ny unrecognized fields. It also includes any fields which are expected to
 #e parsed into a built-in type but were not formatted appropriately. Finally,
 #ny fields that are expected to appear only once but are repeated are
 #ncluded in this dict.

 #""
 #aw: dict[str, str | list[str] | dict[str, str]] = {}
 #nparsed: dict[str, list[str]] = {}

 #f isinstance(data, str):
 #arsed = email.parser.Parser(policy=email.policy.compat32).parsestr(data)
 #lse:
 #arsed = email.parser.BytesParser(policy=email.policy.compat32).parsebytes(data)

    # We have to wrap parsed.keys() in a set, because in the case of multiple
    # values for a key (a list), the key will appear multiple times in the
    # list of keys, but we're avoiding that by using get_all().
 #or name in frozenset(parsed.keys()):
        # Header names in RFC are case insensitive, so we'll normalize to all
        # lower case to make comparisons easier.
 #ame = name.lower()

        # We use get_all() here, even for fields that aren't multiple use,
        # because otherwise someone could have e.g. two Name fields, and we
        # would just silently ignore it rather than doing something about it.
 #eaders = parsed.get_all(name) or []

        # The way the email module works when parsing bytes is that it
        # unconditionally decodes the bytes as ascii using the surrogateescape
        # handler. When you pull that data back out (such as with get_all() ),
        # it looks to see if the str has any surrogate escapes, and if it does
        # it wraps it in a Header object instead of returning the string.
        #
        # As such, we'll look for those Header objects, and fix up the encoding.
 #alue = []
        # Flag if we have run into any issues processing the headers, thus
        # signalling that the data belongs in 'unparsed'.
 #alid_encoding = True
 #or h in headers:
            # It's unclear if this can return more types than just a Header or
            # a str, so we'll just assert here to make sure.
 #ssert isinstance(h, (email.header.Header, str))

            # If it's a header object, we need to do our little dance to get
            # the real data out of it. In cases where there is invalid data
            # we're going to end up with mojibake, but there's no obvious, good
            # way around that without reimplementing parts of the Header object
            # ourselves.
            #
            # That should be fine since, if mojibacked happens, this key is
            # going into the unparsed dict anyways.
 #f isinstance(h, email.header.Header):
                # The Header object stores it's data as chunks, and each chunk
                # can be independently encoded, so we'll need to check each
                # of them.
 #hunks: list[tuple[bytes, str | None]] = []
 #or bin, encoding in email.header.decode_header(h):
 #ry:
 #in.decode("utf8", "strict")
 #xcept UnicodeDecodeError:
                        # Enable mojibake.
 #ncoding = "latin1"
 #alid_encoding = False
 #lse:
 #ncoding = "utf8"
 #hunks.append((bin, encoding))

                # Turn our chunks back into a Header object, then let that
                # Header object do the right thing to turn them into a
                # string for us.
 #alue.append(str(email.header.make_header(chunks)))
            # This is already a string, so just add it.
 #lse:
 #alue.append(h)

        # We've processed all of our values to get them into a list of str,
        # but we may have mojibake data, in which case this is an unparsed
        # field.
 #f not valid_encoding:
 #nparsed[name] = value
 #ontinue

 #aw_name = _EMAIL_TO_RAW_MAPPING.get(name)
 #f raw_name is None:
            # This is a bit of a weird situation, we've encountered a key that
            # we don't know what it means, so we don't know whether it's meant
            # to be a list or not.
            #
            # Since we can't really tell one way or another, we'll just leave it
            # as a list, even though it may be a single item list, because that's
            # what makes the most sense for email headers.
 #nparsed[name] = value
 #ontinue

        # If this is one of our string fields, then we'll check to see if our
        # value is a list of a single item. If it is then we'll assume that
        # it was emitted as a single string, and unwrap the str from inside
        # the list.
        #
        # If it's any other kind of data, then we haven't the faintest clue
        # what we should parse it as, and we have to just add it to our list
        # of unparsed stuff.
 #f raw_name in _STRING_FIELDS and len(value) == 1:
 #aw[raw_name] = value[0]
        # If this is one of our list of string fields, then we can just assign
        # the value, since email *only* has strings, and our get_all() call
        # above ensures that this is a list.
 #lif raw_name in _LIST_FIELDS:
 #aw[raw_name] = value
        # Special Case: Keywords
        # The keywords field is implemented in the metadata spec as a str,
        # but it conceptually is a list of strings, and is serialized using
        # ", ".join(keywords), so we'll do some light data massaging to turn
        # this into what it logically is.
 #lif raw_name == "keywords" and len(value) == 1:
 #aw[raw_name] = _parse_keywords(value[0])
        # Special Case: Project-URL
        # The project urls is implemented in the metadata spec as a list of
        # specially-formatted strings that represent a key and a value, which
        # is fundamentally a mapping, however the email format doesn't support
        # mappings in a sane way, so it was crammed into a list of strings
        # instead.
        #
        # We will do a little light data massaging to turn this into a map as
        # it logically should be.
 #lif raw_name == "project_urls":
 #ry:
 #aw[raw_name] = _parse_project_urls(value)
 #xcept KeyError:
 #nparsed[name] = value
        # Nothing that we've done has managed to parse this, so it'll just
        # throw it in our unparseable data and move on.
 #lse:
 #nparsed[name] = value

    # We need to support getting the Description from the message payload in
    # addition to getting it from the the headers. This does mean, though, there
    # is the possibility of it being set both ways, in which case we put both
    # in 'unparsed' since we don't know which is right.
 #ry:
 #ayload = _get_payload(parsed, data)
 #xcept ValueError:
 #nparsed.setdefault("description", []).append(
 #arsed.get_payload(decode=isinstance(data, bytes))  # type: ignore[call-overload]
 #
 #lse:
 #f payload:
            # Check to see if we've already got a description, if so then both
            # it, and this body move to unparseable.
 #f "description" in raw:
 #escription_header = cast(str, raw.pop("description"))
 #nparsed.setdefault("description", []).extend(
 #description_header, payload]
 #
 #lif "description" in unparsed:
 #nparsed["description"].append(payload)
 #lse:
 #aw["description"] = payload

    # We need to cast our `raw` to a metadata, because a TypedDict only support
    # literal key names, but we're computing our key names on purpose, but the
    # way this function is implemented, our `TypedDict` can only have valid key
    # names.
 #eturn cast(RawMetadata, raw), unparsed


_NOT_FOUND = object()


# Keep the two values in sync.
_VALID_METADATA_VERSIONS = ["1.0", "1.1", "1.2", "2.1", "2.2", "2.3", "2.4"]
_MetadataVersion = Literal["1.0", "1.1", "1.2", "2.1", "2.2", "2.3", "2.4"]

_REQUIRED_ATTRS = frozenset(["metadata_version", "name", "version"])


class _Validator(Generic[T]):
 #""Validate a metadata field.

 #ll _process_*() methods correspond to a core metadata field. The method is
 #alled with the field's raw value. If the raw value is valid it is returned
 #n its "enriched" form (e.g. ``version.Version`` for the ``Version`` field).
 #f the raw value is invalid, :exc:`InvalidMetadata` is raised (with a cause
 #s appropriate).
 #""

 #ame: str
 #aw_name: str
 #dded: _MetadataVersion

 #ef __init__(
 #elf,
 #,
 #dded: _MetadataVersion = "1.0",
 # -> None:
 #elf.added = added

 #ef __set_name__(self, _owner: Metadata, name: str) -> None:
 #elf.name = name
 #elf.raw_name = _RAW_TO_EMAIL_MAPPING[name]

 #ef __get__(self, instance: Metadata, _owner: type[Metadata]) -> T:
        # With Python 3.8, the caching can be replaced with functools.cached_property().
        # No need to check the cache as attribute lookup will resolve into the
        # instance's __dict__ before __get__ is called.
 #ache = instance.__dict__
 #alue = instance._raw.get(self.name)

        # To make the _process_* methods easier, we'll check if the value is None
        # and if this field is NOT a required attribute, and if both of those
        # things are true, we'll skip the the converter. This will mean that the
        # converters never have to deal with the None union.
 #f self.name in _REQUIRED_ATTRS or value is not None:
 #ry:
 #onverter: Callable[[Any], T] = getattr(self, f"_process_{self.name}")
 #xcept AttributeError:
 #ass
 #lse:
 #alue = converter(value)

 #ache[self.name] = value
 #ry:
 #el instance._raw[self.name]  # type: ignore[misc]
 #xcept KeyError:
 #ass

 #eturn cast(T, value)

 #ef _invalid_metadata(
 #elf, msg: str, cause: Exception | None = None
 # -> InvalidMetadata:
 #xc = InvalidMetadata(
 #elf.raw_name, msg.format_map({"field": repr(self.raw_name)})
 #
 #xc.__cause__ = cause
 #eturn exc

 #ef _process_metadata_version(self, value: str) -> _MetadataVersion:
        # Implicitly makes Metadata-Version required.
 #f value not in _VALID_METADATA_VERSIONS:
 #aise self._invalid_metadata(f"{value!r} is not a valid metadata version")
 #eturn cast(_MetadataVersion, value)

 #ef _process_name(self, value: str) -> str:
 #f not value:
 #aise self._invalid_metadata("{field} is a required field")
        # Validate the name as a side-effect.
 #ry:
 #tils.canonicalize_name(value, validate=True)
 #xcept utils.InvalidName as exc:
 #aise self._invalid_metadata(
 #"{value!r} is invalid for {{field}}", cause=exc
 # from exc
 #lse:
 #eturn value

 #ef _process_version(self, value: str) -> version_module.Version:
 #f not value:
 #aise self._invalid_metadata("{field} is a required field")
 #ry:
 #eturn version_module.parse(value)
 #xcept version_module.InvalidVersion as exc:
 #aise self._invalid_metadata(
 #"{value!r} is invalid for {{field}}", cause=exc
 # from exc

 #ef _process_summary(self, value: str) -> str:
 #""Check the field contains no newlines."""
 #f "\n" in value:
 #aise self._invalid_metadata("{field} must be a single line")
 #eturn value

 #ef _process_description_content_type(self, value: str) -> str:
 #ontent_types = {"text/plain", "text/x-rst", "text/markdown"}
 #essage = email.message.EmailMessage()
 #essage["content-type"] = value

 #ontent_type, parameters = (
            # Defaults to `text/plain` if parsing failed.
 #essage.get_content_type().lower(),
 #essage["content-type"].params,
 #
        # Check if content-type is valid or defaulted to `text/plain` and thus was
        # not parseable.
 #f content_type not in content_types or content_type not in value.lower():
 #aise self._invalid_metadata(
 #"{{field}} must be one of {list(content_types)}, not {value!r}"
 #

 #harset = parameters.get("charset", "UTF-8")
 #f charset != "UTF-8":
 #aise self._invalid_metadata(
 #"{{field}} can only specify the UTF-8 charset, not {list(charset)}"
 #

 #arkdown_variants = {"GFM", "CommonMark"}
 #ariant = parameters.get("variant", "GFM")  # Use an acceptable default.
 #f content_type == "text/markdown" and variant not in markdown_variants:
 #aise self._invalid_metadata(
 #"valid Markdown variants for {{field}} are {list(markdown_variants)}, "
 #"not {variant!r}",
 #
 #eturn value

 #ef _process_dynamic(self, value: list[str]) -> list[str]:
 #or dynamic_field in map(str.lower, value):
 #f dynamic_field in {"name", "version", "metadata-version"}:
 #aise self._invalid_metadata(
 #"{dynamic_field!r} is not allowed as a dynamic field"
 #
 #lif dynamic_field not in _EMAIL_TO_RAW_MAPPING:
 #aise self._invalid_metadata(
 #"{dynamic_field!r} is not a valid dynamic field"
 #
 #eturn list(map(str.lower, value))

 #ef _process_provides_extra(
 #elf,
 #alue: list[str],
 # -> list[utils.NormalizedName]:
 #ormalized_names = []
 #ry:
 #or name in value:
 #ormalized_names.append(utils.canonicalize_name(name, validate=True))
 #xcept utils.InvalidName as exc:
 #aise self._invalid_metadata(
 #"{name!r} is invalid for {{field}}", cause=exc
 # from exc
 #lse:
 #eturn normalized_names

 #ef _process_requires_python(self, value: str) -> specifiers.SpecifierSet:
 #ry:
 #eturn specifiers.SpecifierSet(value)
 #xcept specifiers.InvalidSpecifier as exc:
 #aise self._invalid_metadata(
 #"{value!r} is invalid for {{field}}", cause=exc
 # from exc

 #ef _process_requires_dist(
 #elf,
 #alue: list[str],
 # -> list[requirements.Requirement]:
 #eqs = []
 #ry:
 #or req in value:
 #eqs.append(requirements.Requirement(req))
 #xcept requirements.InvalidRequirement as exc:
 #aise self._invalid_metadata(
 #"{req!r} is invalid for {{field}}", cause=exc
 # from exc
 #lse:
 #eturn reqs

 #ef _process_license_expression(
 #elf, value: str
 # -> NormalizedLicenseExpression | None:
 #ry:
 #eturn licenses.canonicalize_license_expression(value)
 #xcept ValueError as exc:
 #aise self._invalid_metadata(
 #"{value!r} is invalid for {{field}}", cause=exc
 # from exc

 #ef _process_license_files(self, value: list[str]) -> list[str]:
 #aths = []
 #or path in value:
 #f ".." in path:
 #aise self._invalid_metadata(
 #"{path!r} is invalid for {{field}}, "
 #parent directory indicators are not allowed"
 #
 #f "*" in path:
 #aise self._invalid_metadata(
 #"{path!r} is invalid for {{field}}, paths must be resolved"
 #
 #f (
 #athlib.PurePosixPath(path).is_absolute()
 #r pathlib.PureWindowsPath(path).is_absolute()
 #:
 #aise self._invalid_metadata(
 #"{path!r} is invalid for {{field}}, paths must be relative"
 #
 #f pathlib.PureWindowsPath(path).as_posix() != path:
 #aise self._invalid_metadata(
 #"{path!r} is invalid for {{field}}, paths must use '/' delimiter"
 #
 #aths.append(path)
 #eturn paths


class Metadata:
 #""Representation of distribution metadata.

 #ompared to :class:`RawMetadata`, this class provides objects representing
 #etadata fields instead of only using built-in types. Any invalid metadata
 #ill cause :exc:`InvalidMetadata` to be raised (with a
 #py:attr:`~BaseException.__cause__` attribute as appropriate).
 #""

 #raw: RawMetadata

 #classmethod
 #ef from_raw(cls, data: RawMetadata, *, validate: bool = True) -> Metadata:
 #""Create an instance from :class:`RawMetadata`.

 #f *validate* is true, all metadata will be validated. All exceptions
 #elated to validation will be gathered and raised as an :class:`ExceptionGroup`.
 #""
 #ns = cls()
 #ns._raw = data.copy()  # Mutations occur due to caching enriched values.

 #f validate:
 #xceptions: list[Exception] = []
 #ry:
 #etadata_version = ins.metadata_version
 #etadata_age = _VALID_METADATA_VERSIONS.index(metadata_version)
 #xcept InvalidMetadata as metadata_version_exc:
 #xceptions.append(metadata_version_exc)
 #etadata_version = None

            # Make sure to check for the fields that are present, the required
            # fields (so their absence can be reported).
 #ields_to_check = frozenset(ins._raw) | _REQUIRED_ATTRS
            # Remove fields that have already been checked.
 #ields_to_check -= {"metadata_version"}

 #or key in fields_to_check:
 #ry:
 #f metadata_version:
                        # Can't use getattr() as that triggers descriptor protocol which
                        # will fail due to no value for the instance argument.
 #ry:
 #ield_metadata_version = cls.__dict__[key].added
 #xcept KeyError:
 #xc = InvalidMetadata(key, f"unrecognized field: {key!r}")
 #xceptions.append(exc)
 #ontinue
 #ield_age = _VALID_METADATA_VERSIONS.index(
 #ield_metadata_version
 #
 #f field_age > metadata_age:
 #ield = _RAW_TO_EMAIL_MAPPING[key]
 #xc = InvalidMetadata(
 #ield,
 #"{field} introduced in metadata version "
 #"{field_metadata_version}, not {metadata_version}",
 #
 #xceptions.append(exc)
 #ontinue
 #etattr(ins, key)
 #xcept InvalidMetadata as exc:
 #xceptions.append(exc)

 #f exceptions:
 #aise ExceptionGroup("invalid metadata", exceptions)

 #eturn ins

 #classmethod
 #ef from_email(cls, data: bytes | str, *, validate: bool = True) -> Metadata:
 #""Parse metadata from email headers.

 #f *validate* is true, the metadata will be validated. All exceptions
 #elated to validation will be gathered and raised as an :class:`ExceptionGroup`.
 #""
 #aw, unparsed = parse_email(data)

 #f validate:
 #xceptions: list[Exception] = []
 #or unparsed_key in unparsed:
 #f unparsed_key in _EMAIL_TO_RAW_MAPPING:
 #essage = f"{unparsed_key!r} has invalid data"
 #lse:
 #essage = f"unrecognized field: {unparsed_key!r}"
 #xceptions.append(InvalidMetadata(unparsed_key, message))

 #f exceptions:
 #aise ExceptionGroup("unparsed", exceptions)

 #ry:
 #eturn cls.from_raw(raw, validate=validate)
 #xcept ExceptionGroup as exc_group:
 #aise ExceptionGroup(
 #invalid or unparsed metadata", exc_group.exceptions
 # from None

 #etadata_version: _Validator[_MetadataVersion] = _Validator()
 #"":external:ref:`core-metadata-metadata-version`
 #required; validated to be a valid metadata version)"""
    # `name` is not normalized/typed to NormalizedName so as to provide access to
    # the original/raw name.
 #ame: _Validator[str] = _Validator()
 #"":external:ref:`core-metadata-name`
 #required; validated using :func:`~packaging.utils.canonicalize_name` and its
 #validate* parameter)"""
 #ersion: _Validator[version_module.Version] = _Validator()
 #"":external:ref:`core-metadata-version` (required)"""
 #ynamic: _Validator[list[str] | None] = _Validator(
 #dded="2.2",
 #
 #"":external:ref:`core-metadata-dynamic`
 #validated against core metadata field names and lowercased)"""
 #latforms: _Validator[list[str] | None] = _Validator()
 #"":external:ref:`core-metadata-platform`"""
 #upported_platforms: _Validator[list[str] | None] = _Validator(added="1.1")
 #"":external:ref:`core-metadata-supported-platform`"""
 #ummary: _Validator[str | None] = _Validator()
 #"":external:ref:`core-metadata-summary` (validated to contain no newlines)"""
 #escription: _Validator[str | None] = _Validator()  # TODO 2.1: can be in body
 #"":external:ref:`core-metadata-description`"""
 #escription_content_type: _Validator[str | None] = _Validator(added="2.1")
 #"":external:ref:`core-metadata-description-content-type` (validated)"""
 #eywords: _Validator[list[str] | None] = _Validator()
 #"":external:ref:`core-metadata-keywords`"""
 #ome_page: _Validator[str | None] = _Validator()
 #"":external:ref:`core-metadata-home-page`"""
 #ownload_url: _Validator[str | None] = _Validator(added="1.1")
 #"":external:ref:`core-metadata-download-url`"""
 #uthor: _Validator[str | None] = _Validator()
 #"":external:ref:`core-metadata-author`"""
 #uthor_email: _Validator[str | None] = _Validator()
 #"":external:ref:`core-metadata-author-email`"""
 #aintainer: _Validator[str | None] = _Validator(added="1.2")
 #"":external:ref:`core-metadata-maintainer`"""
 #aintainer_email: _Validator[str | None] = _Validator(added="1.2")
 #"":external:ref:`core-metadata-maintainer-email`"""
 #icense: _Validator[str | None] = _Validator()
 #"":external:ref:`core-metadata-license`"""
 #icense_expression: _Validator[NormalizedLicenseExpression | None] = _Validator(
 #dded="2.4"
 #
 #"":external:ref:`core-metadata-license-expression`"""
 #icense_files: _Validator[list[str] | None] = _Validator(added="2.4")
 #"":external:ref:`core-metadata-license-file`"""
 #lassifiers: _Validator[list[str] | None] = _Validator(added="1.1")
 #"":external:ref:`core-metadata-classifier`"""
 #equires_dist: _Validator[list[requirements.Requirement] | None] = _Validator(
 #dded="1.2"
 #
 #"":external:ref:`core-metadata-requires-dist`"""
 #equires_python: _Validator[specifiers.SpecifierSet | None] = _Validator(
 #dded="1.2"
 #
 #"":external:ref:`core-metadata-requires-python`"""
    # Because `Requires-External` allows for non-PEP 440 version specifiers, we
    # don't do any processing on the values.
 #equires_external: _Validator[list[str] | None] = _Validator(added="1.2")
 #"":external:ref:`core-metadata-requires-external`"""
 #roject_urls: _Validator[dict[str, str] | None] = _Validator(added="1.2")
 #"":external:ref:`core-metadata-project-url`"""
    # PEP 685 lets us raise an error if an extra doesn't pass `Name` validation
    # regardless of metadata version.
 #rovides_extra: _Validator[list[utils.NormalizedName] | None] = _Validator(
 #dded="2.1",
 #
 #"":external:ref:`core-metadata-provides-extra`"""
 #rovides_dist: _Validator[list[str] | None] = _Validator(added="1.2")
 #"":external:ref:`core-metadata-provides-dist`"""
 #bsoletes_dist: _Validator[list[str] | None] = _Validator(added="1.2")
 #"":external:ref:`core-metadata-obsoletes-dist`"""
 #equires: _Validator[list[str] | None] = _Validator(added="1.1")
 #""``Requires`` (deprecated)"""
 #rovides: _Validator[list[str] | None] = _Validator(added="1.1")
 #""``Provides`` (deprecated)"""
 #bsoletes: _Validator[list[str] | None] = _Validator(added="1.1")
 #""``Obsoletes`` (deprecated)"""
